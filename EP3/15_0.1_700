Pesos Camada de Entrada: 
[[-6.12861050e-03  6.30693264e-02  1.50574791e-01 -1.51784329e-01
  -8.71238477e-02 -1.11012600e-01  8.18688244e-02 -1.51356736e-02
  -1.34739076e-01 -6.53920937e-02  6.76493553e-02  2.76981744e-02
   1.51241950e-01  6.64569371e-03  1.23757286e-01]
 [-1.74600165e-01  4.72101688e-02 -5.36236873e-02 -1.80869168e-01
  -1.16391965e-02  7.86508026e-02  2.23042890e-02 -1.41750791e-01
   2.01297216e-01 -1.39124649e-01 -2.56900199e-02 -1.53286750e-01
  -1.77154089e-01 -6.69208787e-02 -1.12482011e-01]
 [-7.34416610e-02 -1.30274518e-02  1.86821841e-01  6.74625891e-02
  -3.63536154e-02  1.40340168e-01  6.11101409e-02  1.06292405e-01
  -1.42252167e-01  1.52998301e-01  5.22915430e-02  1.19004401e-01
  -2.66862261e-02 -1.50722801e-01 -7.12787563e-02]
 [-1.99602600e-01  3.71837976e-03  1.06073155e-01 -6.41086339e-02
  -1.59236323e-01  1.61622909e-01 -1.66859988e-01  9.99606336e-02
  -2.29830571e-02 -1.41274132e-01 -2.00775840e-01  1.92998381e-01
  -7.25996124e-02  1.66771664e-01 -4.72575906e-02]
 [ 1.65316431e-02 -9.52864576e-03  3.50790965e-02  3.14810645e-02
   1.80881415e-02 -1.10876333e-01  7.71866187e-02 -1.23646802e-02
  -1.71837513e-01  1.10258729e-01 -8.28825746e-02  5.10577057e-02
   6.02916874e-02  6.42549931e-02  8.94993706e-02]
 [-8.47381528e-02 -1.23715959e-02 -1.39344761e-01 -1.29621651e-01
   1.51319283e-02 -1.02929264e-01 -1.42459109e-01  1.41567770e-01
   2.58918706e-02  1.80076053e-01  8.42136682e-02  1.98933028e-01
   1.47075464e-01  1.04787772e-01 -1.87921795e-01]
 [ 8.52343991e-02  1.96590790e-01  1.49378740e-02  1.69498151e-02
   7.68523418e-02 -1.83631265e-01 -2.49167201e-02 -1.05503142e-01
  -1.87347609e-01  1.23590986e-01  4.42969950e-02  3.71122567e-03
  -4.44373621e-02 -6.67845821e-02  1.78641471e-01]
 [-5.56057378e-02 -1.19586314e-01 -2.55484688e-02  1.00634064e-01
  -3.77797591e-02  7.10463446e-02  1.40565865e-01 -3.15421424e-02
   1.58982153e-01  1.36295174e-01 -3.71149120e-02  9.10837863e-02
  -1.39851841e-01 -1.01407661e-01 -1.44895774e-03]
 [ 1.18294108e-01  1.81177550e-01  1.56938735e-01  2.40010189e-02
  -5.75365319e-02 -9.37324299e-03 -1.78119333e-01  1.00436257e-01
  -6.56178418e-02 -1.59297433e-01 -1.99599443e-01  1.34740877e-01
  -4.70887486e-02 -1.98049307e-01 -1.14491067e-01]
 [-7.86550839e-02  1.49092634e-01  1.75156731e-01  1.47312157e-01
  -1.44134529e-01 -1.89042152e-01  1.53835805e-01 -9.20469636e-02
  -1.31351548e-01  4.97321902e-03  1.76704534e-01  4.34425807e-02
  -1.12586318e-01  1.66168433e-01 -1.13780074e-01]
 [ 6.83961387e-02  1.13075054e-01 -6.15829142e-02 -3.40889547e-02
  -1.02096959e-01  6.88613072e-03 -7.62734003e-02  9.58486398e-02
   1.40154757e-01 -1.25502157e-01  7.25879459e-02  1.50601225e-01
   1.59391990e-01 -7.54045838e-02  1.15971207e-01]
 [-4.87641156e-02  1.62168734e-01 -5.37174919e-02  1.72813697e-01
   1.07366974e-02  1.81608559e-01 -1.69691297e-01 -1.08589824e-01
  -1.95847985e-03 -5.62773111e-02 -2.76024023e-02  1.35112085e-01
  -5.38610820e-02 -8.90582487e-02  6.47890117e-02]
 [-3.55697645e-02 -1.77694726e-01  7.44920352e-02  1.81260870e-01
  -1.86812299e-01  5.15843394e-02  4.04690903e-02  5.59762729e-02
  -4.22537381e-03 -1.87077866e-01  5.07411345e-02  5.79807574e-02
  -4.17935525e-03  8.93527331e-02 -1.90096125e-01]
 [-1.80579363e-01  7.00111285e-02  9.84973984e-02  1.36928633e-01
   1.02075627e-01  2.90450058e-02 -2.43298214e-02 -1.53417022e-02
  -5.30489655e-02  1.11638556e-01  3.77856471e-02 -4.63929950e-03
  -7.31056922e-02 -1.68236270e-01  7.20587052e-02]
 [-3.53017161e-02 -1.84772672e-01 -1.25053890e-01 -1.15990929e-01
  -1.47922987e-01 -1.28276154e-01  9.30586167e-02 -1.12289923e-01
  -8.48112876e-03 -9.25753590e-02 -1.15990263e-01  1.52910250e-01
   7.93273668e-02 -1.32199498e-01  2.68551028e-02]
 [ 1.27312614e-02 -3.07815930e-02  1.74041469e-01 -4.92531218e-02
   1.11033451e-01 -1.33567659e-01  6.13106893e-02 -2.70019670e-02
  -1.75753240e-01 -7.82989694e-02 -9.59152398e-02 -4.52552594e-03
   9.91370605e-02 -4.76272972e-02  8.82027131e-02]
 [-4.43073729e-02  1.56012575e-01  1.72050919e-01 -8.35668643e-02
   4.92560084e-02 -1.22491936e-01  1.66760527e-01  1.61439406e-01
   1.15822022e-01 -1.10227483e-01 -1.43828427e-01 -1.68134980e-01
   3.56869379e-02 -7.02284351e-02  1.14714148e-01]
 [-3.22300960e-03 -1.86103214e-01 -1.08734224e-01 -4.93857767e-03
   1.93629523e-01  7.75515119e-02 -8.96270546e-02 -1.13965332e-01
   3.61562988e-03  1.95992331e-01 -1.58407090e-01 -1.21004977e-01
   5.44455225e-02 -1.47447369e-01  1.34175456e-01]
 [ 1.27790430e-01  1.17366718e-01 -8.34718908e-02 -5.31736114e-02
   1.43606304e-01  6.93976855e-02  1.55272886e-01 -1.96180621e-01
   1.12829969e-01  7.48261442e-02  1.17706431e-01  1.83514551e-01
  -1.68756429e-01  2.80484240e-02 -1.55850788e-01]
 [ 1.66068611e-01  8.45740232e-02 -4.62059285e-02  3.35698334e-02
  -1.61956245e-02 -5.85042987e-02 -3.61106995e-02 -1.55831978e-01
   1.15507037e-01  1.11433265e-01 -2.55140521e-03 -1.63973579e-01
  -1.22229469e-01  3.05952075e-02  7.48842428e-02]
 [-1.64370622e-01 -1.36324103e-01 -5.19853366e-02 -6.13330907e-02
  -7.40388228e-02  7.10224367e-02  1.82894260e-01 -2.60978697e-03
  -9.35037005e-03  7.11615875e-03 -2.94903035e-02 -4.49190393e-02
   1.46471723e-01  9.46926136e-02 -1.11257780e-01]
 [ 1.61665353e-01  1.17130300e-01  1.99062293e-01  1.43413014e-01
   1.37594087e-01 -9.43925929e-02  4.13754050e-02  1.06678091e-01
  -1.01697903e-01  9.66663383e-02  1.01911879e-01  1.39018286e-01
  -6.97121255e-02 -1.29893572e-01 -1.65023762e-01]
 [-1.00967332e-01  4.64078252e-02 -1.33958793e-01 -1.65053446e-01
   6.96820657e-02  1.31716271e-01 -1.30653063e-01 -1.30767873e-02
   1.29790457e-01 -4.13308701e-02 -1.70778451e-01  1.78483389e-01
  -2.68330902e-02  1.68371930e-01  8.73299745e-02]
 [ 1.69220192e-01 -1.31397600e-02  6.31428560e-02 -7.82319547e-02
  -1.44267416e-01 -1.65922983e-04  4.03306694e-02 -1.08589440e-02
  -5.84984767e-02 -1.39320897e-01 -1.58713992e-01  5.21648500e-02
   5.14121565e-02  1.62111986e-01 -7.13875596e-02]
 [-1.66269072e-02  1.29100978e-01  3.04657696e-02 -1.62073917e-01
  -1.88749105e-01  1.50392151e-01  8.63663501e-02  8.46387734e-02
   6.66906162e-02  1.71092827e-01  3.33344335e-02  6.97167528e-02
   7.18448465e-02 -1.52826534e-02  5.12243249e-02]
 [-2.55428302e-02 -9.55818049e-02 -3.14825233e-02  3.42524529e-02
   9.83195710e-02 -1.91122850e-01  2.95488369e-02  6.24175919e-02
  -2.80467143e-02 -8.27095844e-02  4.87261221e-02  5.23555687e-02
   6.38979536e-03 -1.36585630e-01 -4.42940279e-03]
 [ 1.05031811e-01 -1.60887253e-02  6.58525949e-02 -1.00288341e-02
  -1.64311912e-01  1.84177434e-01 -1.16969458e-01  2.36866526e-02
  -1.21891017e-01  1.90633812e-01 -4.32421224e-02 -5.18115958e-02
   3.68692466e-02 -5.49232772e-02  9.33588204e-02]
 [-1.11031415e-01 -2.08753729e-02  5.94278959e-02  1.50868000e-01
   1.50196229e-01  5.40238185e-02 -1.08759620e-01  1.11893612e-01
   8.27629186e-02 -2.01213032e-02  1.42299098e-01  8.99100855e-02
  -1.66989399e-01 -1.99975023e-02 -1.61275675e-01]
 [ 1.77145599e-01  1.07532016e-01  5.76951053e-02 -8.24512953e-02
  -2.57583064e-02  1.92063821e-01  1.18618415e-01  1.17458771e-01
   1.42696768e-01 -4.57029666e-03 -4.66731401e-02  1.72891083e-01
   8.60061943e-02  5.05935019e-03 -4.56123573e-02]
 [ 1.08868013e-01 -1.97110533e-01  1.89471217e-01  1.82052776e-01
   2.54662050e-02  5.00749134e-02 -3.52100045e-02  1.24033689e-01
   1.86543212e-01  1.35985470e-01  5.39538096e-02 -1.33609206e-01
   7.61598033e-02 -1.45991845e-01  1.17942521e-01]
 [ 8.65561504e-02  9.44178798e-02  1.52616124e-01  1.57599428e-01
  -1.15033493e-01  5.98156257e-02 -1.89286741e-01  8.25009388e-02
   8.82708329e-02 -1.36476716e-01  1.06073965e-01  1.95968783e-01
   1.16747278e-01 -3.14010940e-02  3.13602152e-02]
 [ 1.04160152e-01 -1.89148320e-01 -1.84694109e-01  1.49592699e-01
   2.46049932e-02 -1.35847902e-01 -8.68749020e-02  1.05428073e-01
  -1.16321683e-01 -1.82317947e-01  1.62703101e-01  7.63939539e-02
   3.61732246e-03  1.28715151e-01  7.04521051e-02]
 [-1.69628129e-01  6.46909112e-02 -8.55664609e-02 -5.01541164e-02
   1.76382373e-01 -1.79998233e-01  1.28154896e-01 -1.72724838e-01
  -1.30225688e-01 -7.44516023e-02  5.70974130e-02  9.56699184e-03
   7.78933411e-02  1.24891605e-01  1.50705982e-01]
 [-1.46803243e-01  2.69310545e-02 -1.76533751e-02  8.88533987e-02
   1.33358752e-03  1.41176076e-01  1.32953689e-01  1.81115948e-01
   7.52212520e-02 -8.68372053e-02  1.36269757e-02  1.97601161e-01
  -1.09810422e-01  1.19909053e-02 -9.52306395e-02]]
Bias Camada de Entrada: 
[-0.04562352  0.12780618 -0.17944888  0.19409061  0.18816095  0.17940245
  0.09927736 -0.05786187  0.0755768  -0.07072474 -0.01315265 -0.03380245
  0.02101263 -0.10881435 -0.05098175]
Pesos Camada Escondida: 
[[ 0.16597131]
 [ 0.10125249]
 [-0.13962091]
 [-0.10763022]
 [-0.311577  ]
 [ 0.33649237]
 [-0.29963413]
 [ 0.27546186]
 [-0.1313318 ]
 [-0.21791325]
 [ 0.33336022]
 [-0.1920715 ]
 [ 0.11773203]
 [ 0.33202853]
 [ 0.13431612]]
Bias Camada Escondida: 
[-0.27232595]
Iteration 1, loss = 0.70937355
Iteration 2, loss = 0.66145185
Iteration 3, loss = 0.65472004
Iteration 4, loss = 0.65309081
Iteration 5, loss = 0.64842062
Iteration 6, loss = 0.63708808
Iteration 7, loss = 0.62646439
Iteration 8, loss = 0.62009970
Iteration 9, loss = 0.61253703
Iteration 10, loss = 0.60354895
Iteration 11, loss = 0.59761503
Iteration 12, loss = 0.58972101
Iteration 13, loss = 0.58075337
Iteration 14, loss = 0.56972742
Iteration 15, loss = 0.55781693
Iteration 16, loss = 0.55013505
Iteration 17, loss = 0.53942840
Iteration 18, loss = 0.52625018
Iteration 19, loss = 0.51407381
Iteration 20, loss = 0.50363849
Iteration 21, loss = 0.49119019
Iteration 22, loss = 0.48125471
Iteration 23, loss = 0.46909455
Iteration 24, loss = 0.45798583
Iteration 25, loss = 0.44760087
Iteration 26, loss = 0.43789494
Iteration 27, loss = 0.42878577
Iteration 28, loss = 0.41976482
Iteration 29, loss = 0.41162676
Iteration 30, loss = 0.40366502
Iteration 31, loss = 0.39558868
Iteration 32, loss = 0.38865141
Iteration 33, loss = 0.38222735
Iteration 34, loss = 0.37533802
Iteration 35, loss = 0.36892498
Iteration 36, loss = 0.36331298
Iteration 37, loss = 0.35767767
Iteration 38, loss = 0.35284515
Iteration 39, loss = 0.34858092
Iteration 40, loss = 0.34559863
Iteration 41, loss = 0.33893053
Iteration 42, loss = 0.33429061
Iteration 43, loss = 0.33327559
Iteration 44, loss = 0.32885638
Iteration 45, loss = 0.32438401
Iteration 46, loss = 0.31992145
Iteration 47, loss = 0.31612978
Iteration 48, loss = 0.31310989
Iteration 49, loss = 0.31192025
Iteration 50, loss = 0.30855544
Iteration 51, loss = 0.30487815
Iteration 52, loss = 0.30215588
Iteration 53, loss = 0.29944593
Iteration 54, loss = 0.29773917
Iteration 55, loss = 0.29389008
Iteration 56, loss = 0.29163986
Iteration 57, loss = 0.28989333
Iteration 58, loss = 0.28768278
Iteration 59, loss = 0.28571757
Iteration 60, loss = 0.28295085
Iteration 61, loss = 0.28161455
Iteration 62, loss = 0.28076949
Iteration 63, loss = 0.27859880
Iteration 64, loss = 0.27615462
Iteration 65, loss = 0.27401604
Iteration 66, loss = 0.27256392
Iteration 67, loss = 0.27093492
Iteration 68, loss = 0.27030977
Iteration 69, loss = 0.27006750
Iteration 70, loss = 0.26779883
Iteration 71, loss = 0.26353890
Iteration 72, loss = 0.26324938
Iteration 73, loss = 0.26273461
Iteration 74, loss = 0.25921033
Iteration 75, loss = 0.25684610
Iteration 76, loss = 0.25648779
Iteration 77, loss = 0.25435022
Iteration 78, loss = 0.25214118
Iteration 79, loss = 0.25085785
Iteration 80, loss = 0.24923425
Iteration 81, loss = 0.24805559
Iteration 82, loss = 0.24633596
Iteration 83, loss = 0.24541171
Iteration 84, loss = 0.24336849
Iteration 85, loss = 0.24217580
Iteration 86, loss = 0.24076829
Iteration 87, loss = 0.23932958
Iteration 88, loss = 0.23698879
Iteration 89, loss = 0.23830808
Iteration 90, loss = 0.23766953
Iteration 91, loss = 0.23470065
Iteration 92, loss = 0.23165771
Iteration 93, loss = 0.23318668
Iteration 94, loss = 0.22980593
Iteration 95, loss = 0.22788751
Iteration 96, loss = 0.22755068
Iteration 97, loss = 0.22936875
Iteration 98, loss = 0.22437610
Iteration 99, loss = 0.22215511
Iteration 100, loss = 0.22152465
Iteration 101, loss = 0.21981076
Iteration 102, loss = 0.21802878
Iteration 103, loss = 0.21648852
Iteration 104, loss = 0.21510992
Iteration 105, loss = 0.21374217
Iteration 106, loss = 0.21373166
Iteration 107, loss = 0.21182742
Iteration 108, loss = 0.20998195
Iteration 109, loss = 0.20829214
Iteration 110, loss = 0.20834572
Iteration 111, loss = 0.21064512
Iteration 112, loss = 0.21045385
Iteration 113, loss = 0.20848405
Iteration 114, loss = 0.20368546
Iteration 115, loss = 0.20121350
Iteration 116, loss = 0.19949506
Iteration 117, loss = 0.19772707
Iteration 118, loss = 0.19845757
Iteration 119, loss = 0.19523562
Iteration 120, loss = 0.19423541
Iteration 121, loss = 0.19560495
Iteration 122, loss = 0.19480724
Iteration 123, loss = 0.19155216
Iteration 124, loss = 0.18899110
Iteration 125, loss = 0.18891234
Iteration 126, loss = 0.18699726
Iteration 127, loss = 0.18623426
Iteration 128, loss = 0.18432880
Iteration 129, loss = 0.18266700
Iteration 130, loss = 0.18155209
Iteration 131, loss = 0.17978269
Iteration 132, loss = 0.17833281
Iteration 133, loss = 0.17699881
Iteration 134, loss = 0.17567068
Iteration 135, loss = 0.17490183
Iteration 136, loss = 0.17507747
Iteration 137, loss = 0.17266397
Iteration 138, loss = 0.17152773
Iteration 139, loss = 0.17011039
Iteration 140, loss = 0.16854704
Iteration 141, loss = 0.16799990
Iteration 142, loss = 0.16785067
Iteration 143, loss = 0.16712427
Iteration 144, loss = 0.16597818
Iteration 145, loss = 0.16388869
Iteration 146, loss = 0.16303294
Iteration 147, loss = 0.16368781
Iteration 148, loss = 0.16053464
Iteration 149, loss = 0.15940836
Iteration 150, loss = 0.15880678
Iteration 151, loss = 0.15874693
Iteration 152, loss = 0.15946478
Iteration 153, loss = 0.15596068
Iteration 154, loss = 0.15563170
Iteration 155, loss = 0.15448390
Iteration 156, loss = 0.15347027
Iteration 157, loss = 0.15257103
Iteration 158, loss = 0.15194767
Iteration 159, loss = 0.15077539
Iteration 160, loss = 0.14920607
Iteration 161, loss = 0.14866287
Iteration 162, loss = 0.14775531
Iteration 163, loss = 0.14738998
Iteration 164, loss = 0.14684817
Iteration 165, loss = 0.14580261
Iteration 166, loss = 0.14418133
Iteration 167, loss = 0.14347853
Iteration 168, loss = 0.14378370
Iteration 169, loss = 0.14510806
Iteration 170, loss = 0.14396828
Iteration 171, loss = 0.14126417
Iteration 172, loss = 0.13948132
Iteration 173, loss = 0.13888288
Iteration 174, loss = 0.13797448
Iteration 175, loss = 0.13736372
Iteration 176, loss = 0.13701098
Iteration 177, loss = 0.13619861
Iteration 178, loss = 0.13548414
Iteration 179, loss = 0.13469649
Iteration 180, loss = 0.13535679
Iteration 181, loss = 0.13351772
Iteration 182, loss = 0.13283119
Iteration 183, loss = 0.13202678
Iteration 184, loss = 0.13121755
Iteration 185, loss = 0.13047997
Iteration 186, loss = 0.13123792
Iteration 187, loss = 0.13103729
Iteration 188, loss = 0.12885838
Iteration 189, loss = 0.12822863
Iteration 190, loss = 0.12701425
Iteration 191, loss = 0.12651873
Iteration 192, loss = 0.12630888
Iteration 193, loss = 0.12547344
Iteration 194, loss = 0.12550275
Iteration 195, loss = 0.12438260
Iteration 196, loss = 0.12403397
Iteration 197, loss = 0.12305120
Iteration 198, loss = 0.12297177
Iteration 199, loss = 0.12249167
Iteration 200, loss = 0.12149457
Iteration 201, loss = 0.12084737
Iteration 202, loss = 0.12056253
Iteration 203, loss = 0.11995229
Iteration 204, loss = 0.11893718
Iteration 205, loss = 0.11802249
Iteration 206, loss = 0.11733964
Iteration 207, loss = 0.11695792
Iteration 208, loss = 0.11678690
Iteration 209, loss = 0.11586629
Iteration 210, loss = 0.11547282
Iteration 211, loss = 0.11452625
Iteration 212, loss = 0.11470262
Iteration 213, loss = 0.11448903
Iteration 214, loss = 0.11344267
Iteration 215, loss = 0.11271178
Iteration 216, loss = 0.11192761
Iteration 217, loss = 0.11141094
Iteration 218, loss = 0.11063733
Iteration 219, loss = 0.11056383
Iteration 220, loss = 0.11097892
Iteration 221, loss = 0.11008213
Iteration 222, loss = 0.10883605
Iteration 223, loss = 0.10862478
Iteration 224, loss = 0.10765732
Iteration 225, loss = 0.10749616
Iteration 226, loss = 0.10714174
Iteration 227, loss = 0.10658777
Iteration 228, loss = 0.10596095
Iteration 229, loss = 0.10525474
Iteration 230, loss = 0.10467447
Iteration 231, loss = 0.10444077
Iteration 232, loss = 0.10413970
Iteration 233, loss = 0.10365278
Iteration 234, loss = 0.10312873
Iteration 235, loss = 0.10335686
Iteration 236, loss = 0.10241702
Iteration 237, loss = 0.10193951
Iteration 238, loss = 0.10150563
Iteration 239, loss = 0.10084785
Iteration 240, loss = 0.10023909
Iteration 241, loss = 0.09995862
Iteration 242, loss = 0.09935416
Iteration 243, loss = 0.09880634
Iteration 244, loss = 0.09832227
Iteration 245, loss = 0.09792606
Iteration 246, loss = 0.09739194
Iteration 247, loss = 0.09744619
Iteration 248, loss = 0.09699563
Iteration 249, loss = 0.09597400
Iteration 250, loss = 0.09665152
Iteration 251, loss = 0.09805729
Iteration 252, loss = 0.09869594
Iteration 253, loss = 0.09680274
Iteration 254, loss = 0.09489472
Iteration 255, loss = 0.09425889
Iteration 256, loss = 0.09385543
Iteration 257, loss = 0.09425234
Iteration 258, loss = 0.09314770
Iteration 259, loss = 0.09247426
Iteration 260, loss = 0.09218480
Iteration 261, loss = 0.09166942
Iteration 262, loss = 0.09125799
Iteration 263, loss = 0.09111021
Iteration 264, loss = 0.09058247
Iteration 265, loss = 0.09008298
Iteration 266, loss = 0.08979286
Iteration 267, loss = 0.08920044
Iteration 268, loss = 0.08896440
Iteration 269, loss = 0.08847647
Iteration 270, loss = 0.08807728
Iteration 271, loss = 0.08761937
Iteration 272, loss = 0.08752353
Iteration 273, loss = 0.08735995
Iteration 274, loss = 0.08734813
Iteration 275, loss = 0.08693606
Iteration 276, loss = 0.08707301
Iteration 277, loss = 0.08602833
Iteration 278, loss = 0.08592276
Iteration 279, loss = 0.08573951
Iteration 280, loss = 0.08563049
Iteration 281, loss = 0.08571226
Iteration 282, loss = 0.08442073
Iteration 283, loss = 0.08411218
Iteration 284, loss = 0.08413056
Iteration 285, loss = 0.08406309
Iteration 286, loss = 0.08391041
Iteration 287, loss = 0.08309362
Iteration 288, loss = 0.08234640
Iteration 289, loss = 0.08176141
Iteration 290, loss = 0.08199663
Iteration 291, loss = 0.08143214
Iteration 292, loss = 0.08099308
Iteration 293, loss = 0.08043167
Iteration 294, loss = 0.08032212
Iteration 295, loss = 0.07961264
Iteration 296, loss = 0.07933531
Iteration 297, loss = 0.07966316
Iteration 298, loss = 0.07933241
Iteration 299, loss = 0.07894433
Iteration 300, loss = 0.07869465
Iteration 301, loss = 0.07824572
Iteration 302, loss = 0.07794503
Iteration 303, loss = 0.07746039
Iteration 304, loss = 0.07691670
Iteration 305, loss = 0.07660565
Iteration 306, loss = 0.07643633
Iteration 307, loss = 0.07661424
Iteration 308, loss = 0.07698137
Iteration 309, loss = 0.07683126
Iteration 310, loss = 0.07559290
Iteration 311, loss = 0.07483474
Iteration 312, loss = 0.07459748
Iteration 313, loss = 0.07520494
Iteration 314, loss = 0.07553642
Iteration 315, loss = 0.07532592
Iteration 316, loss = 0.07482744
Iteration 317, loss = 0.07412664
Iteration 318, loss = 0.07345862
Iteration 319, loss = 0.07317359
Iteration 320, loss = 0.07261222
Iteration 321, loss = 0.07216850
Iteration 322, loss = 0.07198583
Iteration 323, loss = 0.07144857
Iteration 324, loss = 0.07124787
Iteration 325, loss = 0.07090525
Iteration 326, loss = 0.07061339
Iteration 327, loss = 0.07050331
Iteration 328, loss = 0.07048857
Iteration 329, loss = 0.07054034
Iteration 330, loss = 0.07055456
Iteration 331, loss = 0.07042748
Iteration 332, loss = 0.06978785
Iteration 333, loss = 0.06910899
Iteration 334, loss = 0.06857160
Iteration 335, loss = 0.06823607
Iteration 336, loss = 0.06817650
Iteration 337, loss = 0.06772420
Iteration 338, loss = 0.06751550
Iteration 339, loss = 0.06741407
Iteration 340, loss = 0.06709110
Iteration 341, loss = 0.06696576
Iteration 342, loss = 0.06647588
Iteration 343, loss = 0.06685164
Iteration 344, loss = 0.06635048
Iteration 345, loss = 0.06592165
Iteration 346, loss = 0.06576767
Iteration 347, loss = 0.06537120
Iteration 348, loss = 0.06520077
Iteration 349, loss = 0.06490996
Iteration 350, loss = 0.06459999
Iteration 351, loss = 0.06439584
Iteration 352, loss = 0.06442229
Iteration 353, loss = 0.06435703
Iteration 354, loss = 0.06394330
Iteration 355, loss = 0.06384229
Iteration 356, loss = 0.06369329
Iteration 357, loss = 0.06336245
Iteration 358, loss = 0.06305468
Iteration 359, loss = 0.06265937
Iteration 360, loss = 0.06232124
Iteration 361, loss = 0.06207371
Iteration 362, loss = 0.06221200
Iteration 363, loss = 0.06168825
Iteration 364, loss = 0.06145890
Iteration 365, loss = 0.06142247
Iteration 366, loss = 0.06131999
Iteration 367, loss = 0.06119723
Iteration 368, loss = 0.06088809
Iteration 369, loss = 0.06062747
Iteration 370, loss = 0.06030925
Iteration 371, loss = 0.06018719
Iteration 372, loss = 0.05956194
Iteration 373, loss = 0.05902041
Iteration 374, loss = 0.05902602
Iteration 375, loss = 0.05935248
Iteration 376, loss = 0.05874053
Iteration 377, loss = 0.05824916
Iteration 378, loss = 0.05786733
Iteration 379, loss = 0.05805384
Iteration 380, loss = 0.05900410
Iteration 381, loss = 0.05895040
Iteration 382, loss = 0.05832938
Iteration 383, loss = 0.05761563
Iteration 384, loss = 0.05688413
Iteration 385, loss = 0.05658570
Iteration 386, loss = 0.05620447
Iteration 387, loss = 0.05595026
Iteration 388, loss = 0.05589106
Iteration 389, loss = 0.05594624
Iteration 390, loss = 0.05561803
Iteration 391, loss = 0.05520745
Iteration 392, loss = 0.05493615
Iteration 393, loss = 0.05474574
Iteration 394, loss = 0.05470191
Iteration 395, loss = 0.05460504
Iteration 396, loss = 0.05424014
Iteration 397, loss = 0.05419815
Iteration 398, loss = 0.05392289
Iteration 399, loss = 0.05390446
Iteration 400, loss = 0.05351266
Iteration 401, loss = 0.05312442
Iteration 402, loss = 0.05327922
Iteration 403, loss = 0.05283535
Iteration 404, loss = 0.05330316
Iteration 405, loss = 0.05313455
Iteration 406, loss = 0.05282220
Iteration 407, loss = 0.05242397
Iteration 408, loss = 0.05207438
Iteration 409, loss = 0.05169268
Iteration 410, loss = 0.05143404
Iteration 411, loss = 0.05140320
Iteration 412, loss = 0.05118563
Iteration 413, loss = 0.05123433
Iteration 414, loss = 0.05125229
Iteration 415, loss = 0.05109510
Iteration 416, loss = 0.05063352
Iteration 417, loss = 0.05036007
Iteration 418, loss = 0.05009732
Iteration 419, loss = 0.04977387
Iteration 420, loss = 0.04973191
Iteration 421, loss = 0.04974959
Iteration 422, loss = 0.04939557
Iteration 423, loss = 0.04926280
Iteration 424, loss = 0.04954748
Iteration 425, loss = 0.04941689
Iteration 426, loss = 0.04924048
Iteration 427, loss = 0.04900965
Iteration 428, loss = 0.04855874
Iteration 429, loss = 0.04815767
Iteration 430, loss = 0.04797258
Iteration 431, loss = 0.04770830
Iteration 432, loss = 0.04759946
Iteration 433, loss = 0.04755946
Iteration 434, loss = 0.04731313
Iteration 435, loss = 0.04725717
Iteration 436, loss = 0.04715108
Iteration 437, loss = 0.04696251
Iteration 438, loss = 0.04664438
Iteration 439, loss = 0.04650412
Iteration 440, loss = 0.04658435
Iteration 441, loss = 0.04673953
Iteration 442, loss = 0.04684601
Iteration 443, loss = 0.04668775
Iteration 444, loss = 0.04618769
Iteration 445, loss = 0.04606354
Iteration 446, loss = 0.04604269
Iteration 447, loss = 0.04543536
Iteration 448, loss = 0.04543563
Iteration 449, loss = 0.04497914
Iteration 450, loss = 0.04483656
Iteration 451, loss = 0.04464106
Iteration 452, loss = 0.04447347
Iteration 453, loss = 0.04485465
Iteration 454, loss = 0.04428796
Iteration 455, loss = 0.04410713
Iteration 456, loss = 0.04392435
Iteration 457, loss = 0.04381207
Iteration 458, loss = 0.04365307
Iteration 459, loss = 0.04348273
Iteration 460, loss = 0.04330680
Iteration 461, loss = 0.04333502
Iteration 462, loss = 0.04319176
Iteration 463, loss = 0.04301312
Iteration 464, loss = 0.04277210
Iteration 465, loss = 0.04361218
Iteration 466, loss = 0.04279254
Iteration 467, loss = 0.04251517
Iteration 468, loss = 0.04233941
Iteration 469, loss = 0.04220071
Iteration 470, loss = 0.04209680
Iteration 471, loss = 0.04199276
Iteration 472, loss = 0.04197401
Iteration 473, loss = 0.04158464
Iteration 474, loss = 0.04150301
Iteration 475, loss = 0.04169551
Iteration 476, loss = 0.04225854
Iteration 477, loss = 0.04247291
Iteration 478, loss = 0.04186680
Iteration 479, loss = 0.04102097
Iteration 480, loss = 0.04060166
Iteration 481, loss = 0.04031739
Iteration 482, loss = 0.04046027
Iteration 483, loss = 0.04049728
Iteration 484, loss = 0.04053463
Iteration 485, loss = 0.04048971
Iteration 486, loss = 0.04021071
Iteration 487, loss = 0.04003249
Iteration 488, loss = 0.03983501
Iteration 489, loss = 0.03961754
Iteration 490, loss = 0.03961567
Iteration 491, loss = 0.03929636
Iteration 492, loss = 0.03921253
Iteration 493, loss = 0.03914767
Iteration 494, loss = 0.03899753
Iteration 495, loss = 0.03893314
Iteration 496, loss = 0.03861303
Iteration 497, loss = 0.03850080
Iteration 498, loss = 0.03893712
Iteration 499, loss = 0.03818934
Iteration 500, loss = 0.03808824
Iteration 501, loss = 0.03781888
Iteration 502, loss = 0.03790576
Iteration 503, loss = 0.03789097
Iteration 504, loss = 0.03785142
Iteration 505, loss = 0.03775018
Iteration 506, loss = 0.03727638
Iteration 507, loss = 0.03762078
Iteration 508, loss = 0.03772788
Iteration 509, loss = 0.03768049
Iteration 510, loss = 0.03713287
Iteration 511, loss = 0.03686778
Iteration 512, loss = 0.03652814
Iteration 513, loss = 0.03645245
Iteration 514, loss = 0.03649791
Iteration 515, loss = 0.03622593
Iteration 516, loss = 0.03612777
Iteration 517, loss = 0.03604137
Iteration 518, loss = 0.03600950
Iteration 519, loss = 0.03595470
Iteration 520, loss = 0.03574342
Iteration 521, loss = 0.03567634
Iteration 522, loss = 0.03550991
Iteration 523, loss = 0.03542997
Iteration 524, loss = 0.03552049
Iteration 525, loss = 0.03522854
Iteration 526, loss = 0.03502291
Iteration 527, loss = 0.03488024
Iteration 528, loss = 0.03479982
Iteration 529, loss = 0.03472134
Iteration 530, loss = 0.03460551
Iteration 531, loss = 0.03449100
Iteration 532, loss = 0.03440805
Iteration 533, loss = 0.03431801
Iteration 534, loss = 0.03426156
Iteration 535, loss = 0.03407905
Iteration 536, loss = 0.03395994
Iteration 537, loss = 0.03399201
Iteration 538, loss = 0.03389923
Iteration 539, loss = 0.03385743
Iteration 540, loss = 0.03374640
Iteration 541, loss = 0.03372141
Iteration 542, loss = 0.03385868
Iteration 543, loss = 0.03335614
Iteration 544, loss = 0.03335876
Iteration 545, loss = 0.03291590
Iteration 546, loss = 0.03313049
Iteration 547, loss = 0.03300090
Iteration 548, loss = 0.03295018
Iteration 549, loss = 0.03277074
Iteration 550, loss = 0.03254939
Iteration 551, loss = 0.03239482
Iteration 552, loss = 0.03231946
Iteration 553, loss = 0.03214645
Iteration 554, loss = 0.03206889
Iteration 555, loss = 0.03191394
Iteration 556, loss = 0.03185554
Iteration 557, loss = 0.03196951
Iteration 558, loss = 0.03221009
Iteration 559, loss = 0.03194885
Iteration 560, loss = 0.03153379
Iteration 561, loss = 0.03181776
Iteration 562, loss = 0.03130957
Iteration 563, loss = 0.03113412
Iteration 564, loss = 0.03104077
Iteration 565, loss = 0.03101447
Iteration 566, loss = 0.03083705
Iteration 567, loss = 0.03070739
Iteration 568, loss = 0.03072365
Iteration 569, loss = 0.03054912
Iteration 570, loss = 0.03050194
Iteration 571, loss = 0.03066944
Iteration 572, loss = 0.03081129
Iteration 573, loss = 0.03084375
Iteration 574, loss = 0.03079219
Iteration 575, loss = 0.03065201
Iteration 576, loss = 0.03065022
Iteration 577, loss = 0.03069076
Iteration 578, loss = 0.03073460
Iteration 579, loss = 0.03031657
Iteration 580, loss = 0.02981752
Iteration 581, loss = 0.02963692
Iteration 582, loss = 0.02969429
Iteration 583, loss = 0.02967000
Iteration 584, loss = 0.02944819
Iteration 585, loss = 0.02929511
Iteration 586, loss = 0.02930090
Iteration 587, loss = 0.02913394
Iteration 588, loss = 0.02896846
Iteration 589, loss = 0.02891153
Iteration 590, loss = 0.02877051
Iteration 591, loss = 0.02878660
Iteration 592, loss = 0.02875868
Iteration 593, loss = 0.02855167
Iteration 594, loss = 0.02851582
Iteration 595, loss = 0.02837425
Iteration 596, loss = 0.02840954
Iteration 597, loss = 0.02823217
Iteration 598, loss = 0.02803447
Iteration 599, loss = 0.02826835
Iteration 600, loss = 0.02799198
Iteration 601, loss = 0.02786425
Iteration 602, loss = 0.02781389
Iteration 603, loss = 0.02781590
Iteration 604, loss = 0.02764567
Iteration 605, loss = 0.02758185
Iteration 606, loss = 0.02744586
Iteration 607, loss = 0.02739262
Iteration 608, loss = 0.02739420
Iteration 609, loss = 0.02751855
Iteration 610, loss = 0.02755075
Iteration 611, loss = 0.02756527
Iteration 612, loss = 0.02753013
Iteration 613, loss = 0.02764528
Iteration 614, loss = 0.02775614
Iteration 615, loss = 0.02753531
Iteration 616, loss = 0.02733266
Iteration 617, loss = 0.02709031
Iteration 618, loss = 0.02696356
Iteration 619, loss = 0.02681775
Iteration 620, loss = 0.02665796
Iteration 621, loss = 0.02646655
Iteration 622, loss = 0.02634913
Iteration 623, loss = 0.02636849
Iteration 624, loss = 0.02662511
Iteration 625, loss = 0.02696176
Iteration 626, loss = 0.02740941
Iteration 627, loss = 0.02661610
Iteration 628, loss = 0.02610321
Iteration 629, loss = 0.02582877
Iteration 630, loss = 0.02579824
Iteration 631, loss = 0.02611138
Iteration 632, loss = 0.02568017
Iteration 633, loss = 0.02565619
Iteration 634, loss = 0.02561317
Iteration 635, loss = 0.02539270
Iteration 636, loss = 0.02530548
Iteration 637, loss = 0.02527831
Iteration 638, loss = 0.02524257
Iteration 639, loss = 0.02519248
Iteration 640, loss = 0.02511542
Iteration 641, loss = 0.02505629
Iteration 642, loss = 0.02503021
Iteration 643, loss = 0.02485434
Iteration 644, loss = 0.02486683
Iteration 645, loss = 0.02475574
Iteration 646, loss = 0.02465611
Iteration 647, loss = 0.02460916
Iteration 648, loss = 0.02455471
Iteration 649, loss = 0.02485581
Iteration 650, loss = 0.02482140
Iteration 651, loss = 0.02460779
Iteration 652, loss = 0.02449920
Iteration 653, loss = 0.02440556
Iteration 654, loss = 0.02435932
Iteration 655, loss = 0.02427287
Iteration 656, loss = 0.02419680
Iteration 657, loss = 0.02414701
Iteration 658, loss = 0.02413432
Iteration 659, loss = 0.02421231
Iteration 660, loss = 0.02434646
Iteration 661, loss = 0.02422899
Iteration 662, loss = 0.02402777
Iteration 663, loss = 0.02375558
Iteration 664, loss = 0.02367147
Iteration 665, loss = 0.02350090
Iteration 666, loss = 0.02340780
Iteration 667, loss = 0.02333650
Iteration 668, loss = 0.02331866
Iteration 669, loss = 0.02323315
Iteration 670, loss = 0.02309877
Iteration 671, loss = 0.02320474
Iteration 672, loss = 0.02301896
Iteration 673, loss = 0.02299823
Iteration 674, loss = 0.02292295
Iteration 675, loss = 0.02285447
Iteration 676, loss = 0.02284004
Iteration 677, loss = 0.02277855
Iteration 678, loss = 0.02273073
Iteration 679, loss = 0.02275671
Iteration 680, loss = 0.02278786
Iteration 681, loss = 0.02275724
Iteration 682, loss = 0.02266957
Iteration 683, loss = 0.02252120
Iteration 684, loss = 0.02260814
Iteration 685, loss = 0.02257987
Iteration 686, loss = 0.02268883
Iteration 687, loss = 0.02285953
Iteration 688, loss = 0.02303628
Iteration 689, loss = 0.02317936
Iteration 690, loss = 0.02324726
Iteration 691, loss = 0.02280466
Iteration 692, loss = 0.02239903
Iteration 693, loss = 0.02209636
Iteration 694, loss = 0.02202186
Iteration 695, loss = 0.02190014
Iteration 696, loss = 0.02183902
Iteration 697, loss = 0.02179794
Iteration 698, loss = 0.02174086
Iteration 699, loss = 0.02167271
Iteration 700, loss = 0.02159407
PARAMETROS DE INICIALIZACAO DA REDE
NUMERO DE NEURONIOS 
Camada de Entrada: 34
Camada Escondida: 15
Camada de Saida: 1

--- PARAMETROS DE CONFIGURACAO DA REDE ---
Numero de Epocas: 700
Taxa de Aprendizado: adaptive
Taxa de Aprendizado Inicial: 0.1
METRICAS

RESULTADOS:

[0 0 1 0 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1 1 1 1 1 1 1
 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 1 1 1
 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 1 0 1 1 0 0 1 1 0 1 1 1 0 1 1 0]
ACURACIA: 0.9433962264150944

