Pesos Camada de Entrada: 
[[ 0.21321548  0.0599206  -0.14947345]
 [-0.00560593  0.17017448  0.21631079]
 [ 0.08271458 -0.19441307  0.18086863]
 [ 0.12809016 -0.11857734 -0.01235851]
 [-0.09867009  0.01825506  0.07869866]
 [ 0.09052991 -0.12158015  0.06085706]
 [ 0.17440488  0.22894674  0.03953589]
 [-0.0701613  -0.01420811 -0.20400959]
 [ 0.12183092  0.00579724 -0.10105994]
 [ 0.13702192 -0.0458116   0.14979781]
 [ 0.10454629  0.13259074 -0.11878256]
 [ 0.09262091  0.13270863  0.10070319]
 [-0.1465426   0.22603622  0.20030975]
 [ 0.03103385 -0.0793381   0.09552146]
 [ 0.01202716  0.19160795 -0.00320179]
 [ 0.16534659  0.11386687 -0.06198567]
 [-0.20790595  0.22414596  0.13307618]
 [-0.08062564 -0.18821567  0.09817526]
 [-0.05412287 -0.23187329  0.07693162]
 [ 0.00853929 -0.11040024  0.01360117]
 [ 0.10680049 -0.14741663  0.00810267]
 [ 0.03172393  0.00179479  0.1410682 ]
 [-0.22309168 -0.21582721 -0.02453087]
 [ 0.16747902  0.06922031 -0.0053628 ]
 [-0.11009557 -0.03806399 -0.10374491]
 [-0.19815753 -0.16301937  0.22568284]
 [ 0.08454453  0.22006042 -0.13430577]
 [ 0.11130088  0.08178024 -0.16081452]
 [ 0.03291095  0.13455467 -0.21924465]
 [-0.21255434  0.22810985 -0.20272024]
 [-0.21348712  0.10108557  0.17428086]
 [ 0.08868627 -0.12973251  0.1511431 ]
 [-0.14302335  0.18728983 -0.00285402]
 [-0.07978042 -0.17473785 -0.07284431]]
Bias Camada de Entrada: 
[-0.00153667 -0.16975331 -0.11446765]
Pesos Camada Escondida: 
[[ 0.26257767]
 [ 0.35229732]
 [-0.68981724]]
Bias Camada Escondida: 
[-0.17187216]
Iteration 1, loss = 0.71803989
Iteration 2, loss = 0.68309465
Iteration 3, loss = 0.65205765
Iteration 4, loss = 0.63711451
Iteration 5, loss = 0.63552418
Iteration 6, loss = 0.63640575
Iteration 7, loss = 0.63165030
Iteration 8, loss = 0.62493365
Iteration 9, loss = 0.61721854
Iteration 10, loss = 0.60979160
Iteration 11, loss = 0.60243110
Iteration 12, loss = 0.59646668
Iteration 13, loss = 0.58887287
Iteration 14, loss = 0.58201338
Iteration 15, loss = 0.57487864
Iteration 16, loss = 0.56712377
Iteration 17, loss = 0.56025357
Iteration 18, loss = 0.55325333
Iteration 19, loss = 0.54370950
Iteration 20, loss = 0.53411673
Iteration 21, loss = 0.52460755
Iteration 22, loss = 0.51569637
Iteration 23, loss = 0.50618509
Iteration 24, loss = 0.49656515
Iteration 25, loss = 0.48673350
Iteration 26, loss = 0.47710306
Iteration 27, loss = 0.46799943
Iteration 28, loss = 0.45853755
Iteration 29, loss = 0.45019330
Iteration 30, loss = 0.44209936
Iteration 31, loss = 0.43323521
Iteration 32, loss = 0.42412548
Iteration 33, loss = 0.41659917
Iteration 34, loss = 0.41037018
Iteration 35, loss = 0.40427271
Iteration 36, loss = 0.39690382
Iteration 37, loss = 0.38968088
Iteration 38, loss = 0.38302302
Iteration 39, loss = 0.37724503
Iteration 40, loss = 0.37232828
Iteration 41, loss = 0.36726618
Iteration 42, loss = 0.36235582
Iteration 43, loss = 0.35770476
Iteration 44, loss = 0.35316886
Iteration 45, loss = 0.34917456
Iteration 46, loss = 0.34536219
Iteration 47, loss = 0.34198524
Iteration 48, loss = 0.33840712
Iteration 49, loss = 0.33485429
Iteration 50, loss = 0.33199346
Iteration 51, loss = 0.32857218
Iteration 52, loss = 0.32540022
Iteration 53, loss = 0.32296788
Iteration 54, loss = 0.31996725
Iteration 55, loss = 0.31718535
Iteration 56, loss = 0.31497984
Iteration 57, loss = 0.31220993
Iteration 58, loss = 0.30998567
Iteration 59, loss = 0.30791761
Iteration 60, loss = 0.30550714
Iteration 61, loss = 0.30323686
Iteration 62, loss = 0.30148843
Iteration 63, loss = 0.30031620
Iteration 64, loss = 0.29999482
Iteration 65, loss = 0.29576375
Iteration 66, loss = 0.29313975
Iteration 67, loss = 0.29186016
Iteration 68, loss = 0.28998158
Iteration 69, loss = 0.28800378
Iteration 70, loss = 0.28612677
Iteration 71, loss = 0.28471148
Iteration 72, loss = 0.28249210
Iteration 73, loss = 0.28099544
Iteration 74, loss = 0.27931821
Iteration 75, loss = 0.27761162
Iteration 76, loss = 0.27639510
Iteration 77, loss = 0.27464890
Iteration 78, loss = 0.27330794
Iteration 79, loss = 0.27179828
Iteration 80, loss = 0.27053192
Iteration 81, loss = 0.26928374
Iteration 82, loss = 0.26790772
Iteration 83, loss = 0.26666746
Iteration 84, loss = 0.26557219
Iteration 85, loss = 0.26410941
Iteration 86, loss = 0.26216256
Iteration 87, loss = 0.26014461
Iteration 88, loss = 0.25838719
Iteration 89, loss = 0.25664704
Iteration 90, loss = 0.25613359
Iteration 91, loss = 0.25538794
Iteration 92, loss = 0.25349795
Iteration 93, loss = 0.25130593
Iteration 94, loss = 0.24855026
Iteration 95, loss = 0.24674521
Iteration 96, loss = 0.24542129
Iteration 97, loss = 0.24390891
Iteration 98, loss = 0.24204086
Iteration 99, loss = 0.24070237
Iteration 100, loss = 0.23981839
Iteration 101, loss = 0.23907595
Iteration 102, loss = 0.23763034
Iteration 103, loss = 0.23581686
Iteration 104, loss = 0.23364313
Iteration 105, loss = 0.23166461
Iteration 106, loss = 0.22974690
Iteration 107, loss = 0.22805569
Iteration 108, loss = 0.22631478
Iteration 109, loss = 0.22442482
Iteration 110, loss = 0.22276640
Iteration 111, loss = 0.22126276
Iteration 112, loss = 0.21961425
Iteration 113, loss = 0.21816192
Iteration 114, loss = 0.21665528
Iteration 115, loss = 0.21546196
Iteration 116, loss = 0.21430716
Iteration 117, loss = 0.21383951
Iteration 118, loss = 0.21188682
Iteration 119, loss = 0.21027454
Iteration 120, loss = 0.20890557
Iteration 121, loss = 0.20753783
Iteration 122, loss = 0.20627078
Iteration 123, loss = 0.20500653
Iteration 124, loss = 0.20404654
Iteration 125, loss = 0.20303703
Iteration 126, loss = 0.20212734
Iteration 127, loss = 0.20115481
Iteration 128, loss = 0.19992810
Iteration 129, loss = 0.19854077
Iteration 130, loss = 0.19751371
Iteration 131, loss = 0.19653422
Iteration 132, loss = 0.19560704
Iteration 133, loss = 0.19461669
Iteration 134, loss = 0.19453062
Iteration 135, loss = 0.19325180
Iteration 136, loss = 0.19220265
Iteration 137, loss = 0.19084232
Iteration 138, loss = 0.18972034
Iteration 139, loss = 0.18873208
Iteration 140, loss = 0.18784588
Iteration 141, loss = 0.18717147
Iteration 142, loss = 0.18653812
Iteration 143, loss = 0.18624391
Iteration 144, loss = 0.18526475
Iteration 145, loss = 0.18374790
Iteration 146, loss = 0.18258961
Iteration 147, loss = 0.18150992
Iteration 148, loss = 0.18062084
Iteration 149, loss = 0.17988324
Iteration 150, loss = 0.17901220
Iteration 151, loss = 0.17823326
Iteration 152, loss = 0.17750188
Iteration 153, loss = 0.17665035
Iteration 154, loss = 0.17579347
Iteration 155, loss = 0.17492234
Iteration 156, loss = 0.17425868
Iteration 157, loss = 0.17327966
Iteration 158, loss = 0.17239776
Iteration 159, loss = 0.17183580
Iteration 160, loss = 0.17149199
Iteration 161, loss = 0.17119053
Iteration 162, loss = 0.17071460
Iteration 163, loss = 0.16931578
Iteration 164, loss = 0.16818677
Iteration 165, loss = 0.16746277
Iteration 166, loss = 0.16667790
Iteration 167, loss = 0.16607350
Iteration 168, loss = 0.16593003
Iteration 169, loss = 0.16538339
Iteration 170, loss = 0.16457808
Iteration 171, loss = 0.16359681
Iteration 172, loss = 0.16267884
Iteration 173, loss = 0.16205285
Iteration 174, loss = 0.16186965
Iteration 175, loss = 0.16176455
Iteration 176, loss = 0.16139531
Iteration 177, loss = 0.16038932
Iteration 178, loss = 0.15917312
Iteration 179, loss = 0.15789927
Iteration 180, loss = 0.15700279
Iteration 181, loss = 0.15643559
Iteration 182, loss = 0.15585433
Iteration 183, loss = 0.15554946
Iteration 184, loss = 0.15453776
Iteration 185, loss = 0.15376077
Iteration 186, loss = 0.15307256
Iteration 187, loss = 0.15245170
Iteration 188, loss = 0.15166126
Iteration 189, loss = 0.15095204
Iteration 190, loss = 0.15051312
Iteration 191, loss = 0.14971156
Iteration 192, loss = 0.14897135
Iteration 193, loss = 0.14834990
Iteration 194, loss = 0.14777967
Iteration 195, loss = 0.14746421
Iteration 196, loss = 0.14653822
Iteration 197, loss = 0.14603702
Iteration 198, loss = 0.14556904
Iteration 199, loss = 0.14504880
Iteration 200, loss = 0.14445290
Iteration 201, loss = 0.14384484
Iteration 202, loss = 0.14334104
Iteration 203, loss = 0.14279464
Iteration 204, loss = 0.14235575
Iteration 205, loss = 0.14153528
Iteration 206, loss = 0.14072685
Iteration 207, loss = 0.14048793
Iteration 208, loss = 0.13955542
Iteration 209, loss = 0.13902876
Iteration 210, loss = 0.13848111
Iteration 211, loss = 0.13802654
Iteration 212, loss = 0.13746910
Iteration 213, loss = 0.13692497
Iteration 214, loss = 0.13633860
Iteration 215, loss = 0.13574984
Iteration 216, loss = 0.13515847
Iteration 217, loss = 0.13463751
Iteration 218, loss = 0.13413629
Iteration 219, loss = 0.13379589
Iteration 220, loss = 0.13336787
Iteration 221, loss = 0.13252987
Iteration 222, loss = 0.13225700
Iteration 223, loss = 0.13168439
Iteration 224, loss = 0.13125880
Iteration 225, loss = 0.13078519
Iteration 226, loss = 0.13024449
Iteration 227, loss = 0.12988284
Iteration 228, loss = 0.12935011
Iteration 229, loss = 0.12905209
Iteration 230, loss = 0.12876568
Iteration 231, loss = 0.12840159
Iteration 232, loss = 0.12778545
Iteration 233, loss = 0.12738998
Iteration 234, loss = 0.12657105
Iteration 235, loss = 0.12597660
Iteration 236, loss = 0.12522351
Iteration 237, loss = 0.12469652
Iteration 238, loss = 0.12422106
Iteration 239, loss = 0.12360726
Iteration 240, loss = 0.12305823
Iteration 241, loss = 0.12265470
Iteration 242, loss = 0.12213024
Iteration 243, loss = 0.12162254
Iteration 244, loss = 0.12115783
Iteration 245, loss = 0.12071950
Iteration 246, loss = 0.12025888
Iteration 247, loss = 0.11995555
Iteration 248, loss = 0.11946024
Iteration 249, loss = 0.11903541
Iteration 250, loss = 0.11862721
Iteration 251, loss = 0.11812468
Iteration 252, loss = 0.11785102
Iteration 253, loss = 0.11766631
Iteration 254, loss = 0.11702317
Iteration 255, loss = 0.11655815
Iteration 256, loss = 0.11603656
Iteration 257, loss = 0.11557809
Iteration 258, loss = 0.11518707
Iteration 259, loss = 0.11484274
Iteration 260, loss = 0.11439140
Iteration 261, loss = 0.11390360
Iteration 262, loss = 0.11343198
Iteration 263, loss = 0.11300196
Iteration 264, loss = 0.11249373
Iteration 265, loss = 0.11197890
Iteration 266, loss = 0.11153675
Iteration 267, loss = 0.11108201
Iteration 268, loss = 0.11065501
Iteration 269, loss = 0.11023654
Iteration 270, loss = 0.11008473
Iteration 271, loss = 0.10952631
Iteration 272, loss = 0.10907917
Iteration 273, loss = 0.10878126
Iteration 274, loss = 0.10835787
Iteration 275, loss = 0.10789311
Iteration 276, loss = 0.10740258
Iteration 277, loss = 0.10702504
Iteration 278, loss = 0.10653358
Iteration 279, loss = 0.10616100
Iteration 280, loss = 0.10568543
Iteration 281, loss = 0.10520265
Iteration 282, loss = 0.10526782
Iteration 283, loss = 0.10444763
Iteration 284, loss = 0.10402168
Iteration 285, loss = 0.10371782
Iteration 286, loss = 0.10327676
Iteration 287, loss = 0.10286757
Iteration 288, loss = 0.10249239
Iteration 289, loss = 0.10218540
Iteration 290, loss = 0.10172758
Iteration 291, loss = 0.10141101
Iteration 292, loss = 0.10116847
Iteration 293, loss = 0.10109383
Iteration 294, loss = 0.10072912
Iteration 295, loss = 0.10003265
Iteration 296, loss = 0.09942014
Iteration 297, loss = 0.09898087
Iteration 298, loss = 0.09897021
Iteration 299, loss = 0.09872003
Iteration 300, loss = 0.09843845
Iteration 301, loss = 0.09808334
Iteration 302, loss = 0.09771888
Iteration 303, loss = 0.09721602
Iteration 304, loss = 0.09663975
Iteration 305, loss = 0.09620012
Iteration 306, loss = 0.09564270
Iteration 307, loss = 0.09528884
Iteration 308, loss = 0.09495185
Iteration 309, loss = 0.09450381
Iteration 310, loss = 0.09411279
Iteration 311, loss = 0.09388488
Iteration 312, loss = 0.09335272
Iteration 313, loss = 0.09301580
Iteration 314, loss = 0.09273762
Iteration 315, loss = 0.09245025
Iteration 316, loss = 0.09220231
Iteration 317, loss = 0.09186463
Iteration 318, loss = 0.09139072
Iteration 319, loss = 0.09087881
Iteration 320, loss = 0.09043376
Iteration 321, loss = 0.09002711
Iteration 322, loss = 0.08971353
Iteration 323, loss = 0.08925179
Iteration 324, loss = 0.08880759
Iteration 325, loss = 0.08838638
Iteration 326, loss = 0.08813832
Iteration 327, loss = 0.08790321
Iteration 328, loss = 0.08764658
Iteration 329, loss = 0.08773807
Iteration 330, loss = 0.08695627
Iteration 331, loss = 0.08637693
Iteration 332, loss = 0.08623685
Iteration 333, loss = 0.08559060
Iteration 334, loss = 0.08516607
Iteration 335, loss = 0.08483932
Iteration 336, loss = 0.08446090
Iteration 337, loss = 0.08425741
Iteration 338, loss = 0.08387173
Iteration 339, loss = 0.08345581
Iteration 340, loss = 0.08308148
Iteration 341, loss = 0.08274265
Iteration 342, loss = 0.08236396
Iteration 343, loss = 0.08206727
Iteration 344, loss = 0.08164969
Iteration 345, loss = 0.08129257
Iteration 346, loss = 0.08099549
Iteration 347, loss = 0.08074111
Iteration 348, loss = 0.08036029
Iteration 349, loss = 0.08003121
Iteration 350, loss = 0.07969746
Iteration 351, loss = 0.07943255
Iteration 352, loss = 0.07893318
Iteration 353, loss = 0.07863686
Iteration 354, loss = 0.07877019
Iteration 355, loss = 0.07862186
Iteration 356, loss = 0.07828649
Iteration 357, loss = 0.07768333
Iteration 358, loss = 0.07715662
Iteration 359, loss = 0.07713300
Iteration 360, loss = 0.07648907
Iteration 361, loss = 0.07646037
Iteration 362, loss = 0.07605431
Iteration 363, loss = 0.07535452
Iteration 364, loss = 0.07488996
Iteration 365, loss = 0.07456302
Iteration 366, loss = 0.07462855
Iteration 367, loss = 0.07424960
Iteration 368, loss = 0.07401122
Iteration 369, loss = 0.07372682
Iteration 370, loss = 0.07353438
Iteration 371, loss = 0.07339313
Iteration 372, loss = 0.07328946
Iteration 373, loss = 0.07267062
Iteration 374, loss = 0.07212634
Iteration 375, loss = 0.07169575
Iteration 376, loss = 0.07176325
Iteration 377, loss = 0.07197666
Iteration 378, loss = 0.07130337
Iteration 379, loss = 0.07091173
Iteration 380, loss = 0.07053927
Iteration 381, loss = 0.07041009
Iteration 382, loss = 0.07005088
Iteration 383, loss = 0.06972762
Iteration 384, loss = 0.06948968
Iteration 385, loss = 0.06926948
Iteration 386, loss = 0.06894684
Iteration 387, loss = 0.06864111
Iteration 388, loss = 0.06831421
Iteration 389, loss = 0.06831425
Iteration 390, loss = 0.06790227
Iteration 391, loss = 0.06758838
Iteration 392, loss = 0.06736219
Iteration 393, loss = 0.06753887
Iteration 394, loss = 0.06714860
Iteration 395, loss = 0.06683153
Iteration 396, loss = 0.06655140
Iteration 397, loss = 0.06626007
Iteration 398, loss = 0.06600543
Iteration 399, loss = 0.06580427
Iteration 400, loss = 0.06561985
Iteration 401, loss = 0.06542022
Iteration 402, loss = 0.06513825
Iteration 403, loss = 0.06490579
Iteration 404, loss = 0.06472898
Iteration 405, loss = 0.06462922
Iteration 406, loss = 0.06457667
Iteration 407, loss = 0.06438176
Iteration 408, loss = 0.06399303
Iteration 409, loss = 0.06363144
Iteration 410, loss = 0.06331799
Iteration 411, loss = 0.06308892
Iteration 412, loss = 0.06282863
Iteration 413, loss = 0.06266143
Iteration 414, loss = 0.06255767
Iteration 415, loss = 0.06213360
Iteration 416, loss = 0.06184847
Iteration 417, loss = 0.06169126
Iteration 418, loss = 0.06146604
Iteration 419, loss = 0.06141914
Iteration 420, loss = 0.06098555
Iteration 421, loss = 0.06079541
Iteration 422, loss = 0.06067832
Iteration 423, loss = 0.06053140
Iteration 424, loss = 0.06027467
Iteration 425, loss = 0.06007335
Iteration 426, loss = 0.05984694
Iteration 427, loss = 0.05967596
Iteration 428, loss = 0.05951123
Iteration 429, loss = 0.05933914
Iteration 430, loss = 0.05919527
Iteration 431, loss = 0.05890929
Iteration 432, loss = 0.05877495
Iteration 433, loss = 0.05859547
Iteration 434, loss = 0.05844135
Iteration 435, loss = 0.05826968
Iteration 436, loss = 0.05819706
Iteration 437, loss = 0.05809477
Iteration 438, loss = 0.05794016
Iteration 439, loss = 0.05795747
Iteration 440, loss = 0.05755096
Iteration 441, loss = 0.05732491
Iteration 442, loss = 0.05698497
Iteration 443, loss = 0.05666412
Iteration 444, loss = 0.05667692
Iteration 445, loss = 0.05666106
Iteration 446, loss = 0.05663716
Iteration 447, loss = 0.05632805
Iteration 448, loss = 0.05599535
Iteration 449, loss = 0.05575703
Iteration 450, loss = 0.05552558
Iteration 451, loss = 0.05533614
Iteration 452, loss = 0.05527881
Iteration 453, loss = 0.05506725
Iteration 454, loss = 0.05493277
Iteration 455, loss = 0.05495500
Iteration 456, loss = 0.05448433
Iteration 457, loss = 0.05484429
Iteration 458, loss = 0.05450122
Iteration 459, loss = 0.05437065
Iteration 460, loss = 0.05429622
Iteration 461, loss = 0.05417750
Iteration 462, loss = 0.05405002
Iteration 463, loss = 0.05385350
Iteration 464, loss = 0.05351628
Iteration 465, loss = 0.05315415
Iteration 466, loss = 0.05294759
Iteration 467, loss = 0.05287764
Iteration 468, loss = 0.05288390
Iteration 469, loss = 0.05284532
Iteration 470, loss = 0.05273571
Iteration 471, loss = 0.05256437
Iteration 472, loss = 0.05242041
Iteration 473, loss = 0.05218911
Iteration 474, loss = 0.05205779
Iteration 475, loss = 0.05189376
Iteration 476, loss = 0.05181271
Iteration 477, loss = 0.05167002
Iteration 478, loss = 0.05154980
Iteration 479, loss = 0.05138876
Iteration 480, loss = 0.05134751
Iteration 481, loss = 0.05129202
Iteration 482, loss = 0.05101163
Iteration 483, loss = 0.05079032
Iteration 484, loss = 0.05046718
Iteration 485, loss = 0.05022005
Iteration 486, loss = 0.05027904
Iteration 487, loss = 0.05020143
Iteration 488, loss = 0.05011333
Iteration 489, loss = 0.05000076
Iteration 490, loss = 0.04979849
Iteration 491, loss = 0.04959341
Iteration 492, loss = 0.04938283
Iteration 493, loss = 0.04922505
Iteration 494, loss = 0.04902822
Iteration 495, loss = 0.04897431
Iteration 496, loss = 0.04930394
Iteration 497, loss = 0.04908673
Iteration 498, loss = 0.04891133
Iteration 499, loss = 0.04868796
Iteration 500, loss = 0.04839974
Iteration 501, loss = 0.04815912
Iteration 502, loss = 0.04813470
Iteration 503, loss = 0.04795204
Iteration 504, loss = 0.04783048
Iteration 505, loss = 0.04781165
Iteration 506, loss = 0.04776807
Iteration 507, loss = 0.04770882
Iteration 508, loss = 0.04759015
Iteration 509, loss = 0.04743355
Iteration 510, loss = 0.04723183
Iteration 511, loss = 0.04711478
Iteration 512, loss = 0.04698548
Iteration 513, loss = 0.04676126
Iteration 514, loss = 0.04671899
Iteration 515, loss = 0.04648283
Iteration 516, loss = 0.04639309
Iteration 517, loss = 0.04640654
Iteration 518, loss = 0.04619957
Iteration 519, loss = 0.04604594
Iteration 520, loss = 0.04582834
Iteration 521, loss = 0.04575287
Iteration 522, loss = 0.04562696
Iteration 523, loss = 0.04555072
Iteration 524, loss = 0.04541970
Iteration 525, loss = 0.04535311
Iteration 526, loss = 0.04544448
Iteration 527, loss = 0.04542061
Iteration 528, loss = 0.04542841
Iteration 529, loss = 0.04524605
Iteration 530, loss = 0.04506281
Iteration 531, loss = 0.04489688
Iteration 532, loss = 0.04446207
Iteration 533, loss = 0.04443341
Iteration 534, loss = 0.04435389
Iteration 535, loss = 0.04419568
Iteration 536, loss = 0.04397237
Iteration 537, loss = 0.04397166
Iteration 538, loss = 0.04377758
Iteration 539, loss = 0.04372506
Iteration 540, loss = 0.04357102
Iteration 541, loss = 0.04345644
Iteration 542, loss = 0.04342247
Iteration 543, loss = 0.04323706
Iteration 544, loss = 0.04319328
Iteration 545, loss = 0.04312256
Iteration 546, loss = 0.04299819
Iteration 547, loss = 0.04289192
Iteration 548, loss = 0.04277028
Iteration 549, loss = 0.04259530
Iteration 550, loss = 0.04244505
Iteration 551, loss = 0.04230457
Iteration 552, loss = 0.04219698
Iteration 553, loss = 0.04210021
Iteration 554, loss = 0.04200810
Iteration 555, loss = 0.04186237
Iteration 556, loss = 0.04179746
Iteration 557, loss = 0.04181512
Iteration 558, loss = 0.04192065
Iteration 559, loss = 0.04185812
Iteration 560, loss = 0.04158419
Iteration 561, loss = 0.04133569
Iteration 562, loss = 0.04115974
Iteration 563, loss = 0.04153319
Iteration 564, loss = 0.04113618
Iteration 565, loss = 0.04095207
Iteration 566, loss = 0.04076086
Iteration 567, loss = 0.04057682
Iteration 568, loss = 0.04053772
Iteration 569, loss = 0.04047055
Iteration 570, loss = 0.04028706
Iteration 571, loss = 0.04023112
Iteration 572, loss = 0.04004557
Iteration 573, loss = 0.03997898
Iteration 574, loss = 0.03992193
Iteration 575, loss = 0.03998194
Iteration 576, loss = 0.03983536
Iteration 577, loss = 0.03973293
Iteration 578, loss = 0.03953207
Iteration 579, loss = 0.03941697
Iteration 580, loss = 0.03940706
Iteration 581, loss = 0.03930810
Iteration 582, loss = 0.03929466
Iteration 583, loss = 0.03913935
Iteration 584, loss = 0.03895306
Iteration 585, loss = 0.03894210
Iteration 586, loss = 0.03889827
Iteration 587, loss = 0.03894579
Iteration 588, loss = 0.03899264
Iteration 589, loss = 0.03902015
Iteration 590, loss = 0.03889368
Iteration 591, loss = 0.03866210
Iteration 592, loss = 0.03836043
Iteration 593, loss = 0.03823757
Iteration 594, loss = 0.03813321
Iteration 595, loss = 0.03800518
Iteration 596, loss = 0.03793934
Iteration 597, loss = 0.03783659
Iteration 598, loss = 0.03774664
Iteration 599, loss = 0.03765541
Iteration 600, loss = 0.03762899
Iteration 601, loss = 0.03749356
Iteration 602, loss = 0.03736928
Iteration 603, loss = 0.03725187
Iteration 604, loss = 0.03720378
Iteration 605, loss = 0.03707709
Iteration 606, loss = 0.03707316
Iteration 607, loss = 0.03695963
Iteration 608, loss = 0.03688626
Iteration 609, loss = 0.03691717
Iteration 610, loss = 0.03697519
Iteration 611, loss = 0.03678183
Iteration 612, loss = 0.03658436
Iteration 613, loss = 0.03640316
Iteration 614, loss = 0.03634864
Iteration 615, loss = 0.03641227
Iteration 616, loss = 0.03651277
Iteration 617, loss = 0.03632575
Iteration 618, loss = 0.03613547
Iteration 619, loss = 0.03599334
Iteration 620, loss = 0.03593280
Iteration 621, loss = 0.03575562
Iteration 622, loss = 0.03566613
Iteration 623, loss = 0.03558464
Iteration 624, loss = 0.03551382
Iteration 625, loss = 0.03541887
Iteration 626, loss = 0.03536163
Iteration 627, loss = 0.03529603
Iteration 628, loss = 0.03524507
Iteration 629, loss = 0.03526612
Iteration 630, loss = 0.03524042
Iteration 631, loss = 0.03530526
Iteration 632, loss = 0.03510909
Iteration 633, loss = 0.03491635
Iteration 634, loss = 0.03481672
Iteration 635, loss = 0.03462016
Iteration 636, loss = 0.03458219
Iteration 637, loss = 0.03445663
Iteration 638, loss = 0.03441821
Iteration 639, loss = 0.03433748
Iteration 640, loss = 0.03427091
Iteration 641, loss = 0.03418183
Iteration 642, loss = 0.03411432
Iteration 643, loss = 0.03404373
Iteration 644, loss = 0.03401726
Iteration 645, loss = 0.03402347
Iteration 646, loss = 0.03406375
Iteration 647, loss = 0.03414830
Iteration 648, loss = 0.03420660
Iteration 649, loss = 0.03401916
Iteration 650, loss = 0.03384921
Iteration 651, loss = 0.03368790
Iteration 652, loss = 0.03352795
Iteration 653, loss = 0.03342745
Iteration 654, loss = 0.03322625
Iteration 655, loss = 0.03318842
Iteration 656, loss = 0.03329112
Iteration 657, loss = 0.03319236
Iteration 658, loss = 0.03308345
Iteration 659, loss = 0.03289380
Iteration 660, loss = 0.03282786
Iteration 661, loss = 0.03271466
Iteration 662, loss = 0.03267885
Iteration 663, loss = 0.03263072
Iteration 664, loss = 0.03260197
Iteration 665, loss = 0.03249598
Iteration 666, loss = 0.03239380
Iteration 667, loss = 0.03232009
Iteration 668, loss = 0.03233042
Iteration 669, loss = 0.03234470
Iteration 670, loss = 0.03228564
Iteration 671, loss = 0.03232328
Iteration 672, loss = 0.03203507
Iteration 673, loss = 0.03195264
Iteration 674, loss = 0.03176409
Iteration 675, loss = 0.03173273
Iteration 676, loss = 0.03165920
Iteration 677, loss = 0.03161287
Iteration 678, loss = 0.03151058
Iteration 679, loss = 0.03151790
Iteration 680, loss = 0.03152020
Iteration 681, loss = 0.03159482
Iteration 682, loss = 0.03155821
Iteration 683, loss = 0.03136859
Iteration 684, loss = 0.03124902
Iteration 685, loss = 0.03109469
Iteration 686, loss = 0.03098879
Iteration 687, loss = 0.03096797
Iteration 688, loss = 0.03108813
Iteration 689, loss = 0.03094959
Iteration 690, loss = 0.03083869
Iteration 691, loss = 0.03077101
Iteration 692, loss = 0.03071995
Iteration 693, loss = 0.03063148
Iteration 694, loss = 0.03061994
Iteration 695, loss = 0.03063320
Iteration 696, loss = 0.03057560
Iteration 697, loss = 0.03049139
Iteration 698, loss = 0.03036634
Iteration 699, loss = 0.03022635
Iteration 700, loss = 0.03013201
PARAMETROS DE INICIALIZACAO DA REDE
NUMERO DE NEURONIOS 
Camada de Entrada: 34
Camada Escondida: 3
Camada de Saida: 1

--- PARAMETROS DE CONFIGURACAO DA REDE ---
Numero de Epocas: 700
Taxa de Aprendizado: adaptive
Taxa de Aprendizado Inicial: 0.1
METRICAS

RESULTADOS:

[0 0 1 0 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 1 1
 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 1 1 1
 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 0]
ACURACIA: 0.9150943396226415

