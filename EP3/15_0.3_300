Pesos Camada de Entrada: 
[[ 5.46984605e-02 -1.87831959e-01  4.37917493e-02 -4.48243711e-02
  -1.56495798e-01 -5.64075836e-02 -1.27454190e-01 -1.96138805e-01
   1.43357572e-01  1.50007702e-01  1.62439222e-01  5.70557274e-02
  -5.63983323e-02  1.36747620e-01 -1.01537563e-02]
 [-3.78701174e-02 -3.94868955e-02 -1.89079471e-02  1.93299058e-01
  -1.53357590e-01  8.15396116e-02 -1.89501427e-01 -1.31645878e-01
  -1.74134780e-01  6.28705568e-02 -1.62679060e-01  6.71192690e-02
   1.18620459e-02  1.93486409e-01  1.31889501e-01]
 [-1.39558355e-01  9.78821973e-03 -9.34229334e-02  6.17100421e-02
   1.37865311e-01  1.59356970e-01 -2.01577687e-01 -1.28205029e-02
  -1.55072812e-01 -8.58886186e-02  2.00003340e-01  1.62954665e-01
   1.57088103e-01  2.12834250e-02 -1.95045519e-01]
 [-1.07316801e-01  1.76623076e-01 -1.79807323e-02  1.72592131e-01
  -4.49026973e-03 -4.55585228e-02  1.62965652e-01 -1.57013834e-01
  -1.92062578e-01 -2.03253298e-02  1.96097783e-01  1.80793275e-01
   5.86491496e-02 -1.85791369e-01 -7.20661696e-02]
 [ 4.12721978e-02 -1.51828682e-01  1.17553690e-01  6.24994005e-02
  -1.97030206e-02 -1.31362741e-01  5.24435832e-02  5.14395132e-02
   7.55765478e-02  7.67373054e-02  8.33270694e-02 -9.25003058e-02
  -1.10479659e-02  2.02600319e-02  5.66966147e-02]
 [ 1.98029778e-01 -8.64385072e-02 -2.02318043e-02 -3.31824942e-02
   1.90330635e-01 -1.51501020e-02 -1.72610497e-01  1.00637702e-01
   1.76131887e-01 -7.10761307e-02  1.42086053e-01 -1.21827814e-01
   1.49266917e-01  1.20702941e-01 -1.66025902e-01]
 [-1.35322562e-01  1.94697363e-01  1.74078083e-01  1.35491621e-01
   8.54939302e-02 -1.56425191e-02 -7.68845448e-02  1.26504267e-01
  -1.60304236e-01  1.36025756e-01  1.76027144e-01  3.46881933e-02
  -1.38372527e-01 -1.84268072e-01  2.43008463e-03]
 [ 8.92577837e-02  2.19017773e-02 -4.45166439e-02 -1.54782927e-01
  -1.27797742e-01  3.43333024e-02  9.11705899e-03 -1.35024446e-01
   9.72575094e-02 -1.49241412e-02  1.20520973e-01  2.18949367e-02
  -9.19625515e-02  1.99700281e-01 -9.27417083e-02]
 [ 8.21475220e-03  1.22440465e-01 -5.29362043e-02 -3.80170524e-02
  -3.14515795e-02  3.98732912e-02 -9.66288760e-02 -7.41985505e-02
   1.47024484e-01 -5.74319599e-02 -1.52325326e-01  8.62564717e-02
   6.58632691e-02 -7.45117477e-02  4.31710002e-02]
 [-1.72919784e-01 -5.03219863e-02  1.57069683e-01 -3.50970353e-02
  -3.19923491e-02 -8.21362099e-02 -1.74867372e-01  1.57852844e-01
   1.05217150e-01 -1.46383494e-01  3.63147592e-03 -4.83257053e-03
   1.07476460e-01  8.35141703e-02 -1.32642966e-01]
 [-2.59506220e-02 -2.57946071e-02  1.80461163e-01 -4.68345664e-02
  -1.91058355e-01 -1.60005949e-01 -4.57269490e-02 -1.85536824e-01
  -1.98321272e-01 -1.71343528e-01 -1.82180576e-01  2.02025209e-01
  -1.58280605e-01 -1.37380516e-01 -1.15480998e-02]
 [-9.08723700e-02 -6.58911098e-02 -1.70040675e-01  4.15977472e-02
  -2.06674495e-02  1.97761360e-01 -3.81396642e-02 -1.31333429e-01
  -6.27398222e-02 -4.18306845e-02 -4.74205073e-02  2.71541325e-02
   9.98858860e-02 -4.73961815e-02  9.83149382e-02]
 [ 1.56536679e-02 -1.15049244e-01  1.46455359e-01 -1.86068563e-01
   2.56258832e-02 -1.06639736e-01  1.26540348e-05  1.52025689e-01
  -1.13683093e-01 -8.77724208e-02 -1.41880698e-01 -8.79602907e-03
  -1.92210970e-01  5.68430900e-02 -1.23323742e-01]
 [ 9.03110098e-02  5.16036785e-02 -1.68114741e-01 -1.31797963e-02
  -8.62790370e-02 -1.47020490e-01  5.92152485e-02  1.89717821e-01
  -8.96320646e-02  1.61287725e-01  1.05742918e-01  3.30590500e-02
   8.77830982e-02 -1.61666426e-03  1.43258624e-01]
 [-1.46380329e-01  1.09357643e-02  1.77327609e-01 -1.35218017e-01
  -1.03657574e-01  4.94457756e-02  6.29659879e-02 -9.79262370e-03
   1.18249721e-01 -2.01517101e-01  1.27501202e-01  9.06279821e-02
   1.33942630e-01  4.66013742e-02 -6.48946695e-02]
 [ 1.40337977e-01 -1.60230327e-01 -1.73292963e-02 -9.97794791e-02
   5.12596340e-02  9.00637431e-02  5.35028371e-02 -2.18311756e-02
   4.47803144e-02 -1.20046654e-01 -5.77015176e-02 -2.61296164e-02
  -9.78106246e-02 -2.85147233e-02  1.28952264e-01]
 [ 1.74294481e-01  1.54865995e-01 -6.46625194e-02 -1.33826967e-01
   1.80339678e-01  2.65118683e-02 -4.06511908e-02  5.21743690e-02
   1.16874419e-01 -1.17745887e-01  2.46484196e-02 -1.19004224e-01
   9.20903530e-02  8.40214270e-02  7.17604161e-02]
 [ 9.34342762e-02  9.79412287e-02  1.25429676e-01 -9.15161815e-02
  -1.96535436e-01 -2.24038466e-03 -2.62226016e-02  1.09019245e-01
   1.15016931e-01  4.30759937e-02  1.65131465e-01  1.09307619e-01
  -1.36739250e-02 -1.90404604e-01 -1.57669468e-01]
 [-8.14224831e-02  1.95377157e-01  1.82964341e-01  5.91051994e-02
   1.68409017e-02  4.87313914e-02 -1.90156756e-01  1.57995560e-01
  -1.07088427e-01  1.75635916e-01  1.21514900e-01  1.25924197e-01
   1.58025246e-01 -1.02106745e-01 -9.77578478e-02]
 [-1.04904047e-01  4.96918724e-02  6.23520202e-02  1.27798404e-01
  -2.88967637e-02  4.13475508e-02  4.08536954e-02 -3.71239689e-02
   5.81124374e-02  2.59889288e-02 -1.09712433e-01  1.80839063e-01
  -1.26573175e-01 -1.00242650e-01 -1.48670404e-01]
 [-1.26841836e-01  2.32349538e-04  1.03794727e-01 -1.24557489e-01
   5.79607402e-03  1.34788419e-02  1.71591319e-01  1.47539008e-01
  -1.25078878e-01  4.65579109e-02  1.69142415e-01  8.70559029e-02
  -1.87602914e-01 -1.96716229e-03 -1.64452901e-01]
 [-3.07769104e-02  8.41702985e-02 -1.56997113e-01 -1.74766612e-01
  -2.14715742e-02  2.36262934e-02  5.77298290e-02  6.42077636e-02
  -5.92834746e-02 -9.43141109e-02 -1.32683643e-02 -7.87279057e-02
  -1.21453469e-02 -7.11639334e-02  5.62944780e-02]
 [ 7.96590586e-02  3.86516344e-02  7.60046120e-02  1.14591549e-01
   9.59645682e-02  6.22944052e-02 -1.33637019e-01 -8.71164184e-02
   1.42889999e-01 -1.29987465e-02 -1.07963287e-01 -1.81420637e-01
   4.97579342e-02  1.92271616e-01 -1.40134643e-02]
 [ 5.04949117e-02 -4.90527797e-02  1.75334539e-01  1.58645461e-01
   1.55785130e-01 -1.50467443e-01  1.85679692e-01  1.09883678e-01
   9.21070220e-02 -1.26279332e-01  1.17829704e-01 -1.23136159e-03
  -1.54103544e-01  1.86255518e-01 -1.91917147e-01]
 [ 3.58139935e-03  2.65129419e-02 -7.57828584e-02  8.93879853e-02
   5.70357088e-02 -2.85945212e-02  1.37835154e-01  1.47646853e-01
  -1.17298629e-01  1.38614366e-01 -6.20118900e-02  6.28566208e-02
  -1.72907566e-01 -1.34272434e-01 -1.32999369e-01]
 [-4.82886970e-02 -1.69984059e-01 -2.50101569e-02 -1.83193442e-01
   1.51006079e-02 -7.08004410e-02  1.94082822e-01 -1.18017580e-01
  -1.93251752e-01  1.82747185e-01  1.03693972e-01 -5.81233998e-02
  -1.49066925e-01  3.80621661e-02 -1.01101600e-01]
 [ 1.75882296e-01 -9.96257830e-02  1.37052122e-01  1.28620550e-01
   1.26730650e-01  1.61341453e-01  1.33582292e-01 -6.27043220e-02
  -1.99055179e-02  7.83666491e-02 -1.39971545e-01  6.73211255e-02
  -1.96817126e-02  1.92152106e-01  1.09943652e-01]
 [-8.00461298e-02 -1.98745059e-01 -7.66326654e-02  4.56283719e-02
  -1.41862200e-01  9.70752945e-02 -7.08802007e-02  1.35706852e-01
   1.69588117e-01 -8.32545305e-02  1.35081851e-01  1.43628857e-01
   2.86005817e-02 -2.05289203e-02  1.63895431e-01]
 [-1.06236633e-01 -1.05288888e-01  1.21578166e-01  6.21349487e-02
  -1.89338126e-01 -1.19679661e-01 -8.13462459e-02 -2.52344791e-02
   1.94926920e-01 -9.99906965e-02  3.93662041e-02 -1.41227373e-01
  -1.56071472e-01  1.31686849e-01  1.41655398e-02]
 [ 2.57554857e-02  7.36824967e-02  1.51489362e-01 -1.70466083e-01
  -1.25285765e-01 -1.70922705e-02 -1.09302822e-01  1.25150054e-01
  -4.93176306e-02  1.26920333e-01 -7.72652248e-02 -7.00000891e-02
   4.60206272e-03 -2.00017797e-01 -7.95018261e-02]
 [-1.89823025e-01  3.54915903e-02 -1.14778799e-01  1.59639192e-01
   5.38419871e-03 -7.65861986e-02  1.71856036e-01 -1.55907873e-01
  -9.03250701e-02 -1.90255850e-01 -1.05232102e-01 -1.52552095e-01
   9.69333843e-03 -9.05514282e-03  3.34897822e-02]
 [ 1.92657296e-01 -6.06879444e-03  1.97058847e-01  1.91073597e-01
   1.29629017e-01  1.43815591e-01  3.77413455e-02 -1.44933793e-01
   7.81163022e-02 -9.41600371e-02  1.32309435e-01 -1.46978569e-01
   1.84221923e-01 -1.37404173e-01 -1.06322538e-01]
 [ 7.21887945e-02 -1.84767375e-01  9.23695540e-02 -1.18821954e-01
   3.53303047e-02 -1.77873657e-01  2.43847417e-02  3.20994129e-02
  -1.47591308e-01 -4.32570282e-02 -8.08824398e-02 -1.24952549e-01
  -4.79081339e-03  1.30858088e-01  1.55475437e-01]
 [ 1.58583976e-01  6.37514307e-02  1.42658320e-01 -1.16014019e-01
   1.95283443e-01  1.28124185e-01  1.83010866e-01 -1.01654640e-01
   7.16577891e-02  3.59738246e-03 -7.76234635e-02 -1.94244583e-01
  -1.08635736e-01 -1.95318611e-01 -7.36930591e-02]]
Bias Camada de Entrada: 
[ 0.12839566  0.10299765 -0.10164599 -0.02798461  0.15905177  0.11638322
 -0.07977152 -0.10714919 -0.15728842 -0.13344238  0.0570279  -0.01698498
 -0.01252277  0.02890559 -0.11996857]
Pesos Camada Escondida: 
[[ 0.05902238]
 [ 0.1347123 ]
 [-0.14453825]
 [ 0.16706242]
 [-0.02621008]
 [-0.19957854]
 [ 0.01953077]
 [-0.09210755]
 [-0.20246811]
 [ 0.10937243]
 [ 0.12042016]
 [ 0.03293827]
 [ 0.03325241]
 [-0.21713988]
 [-0.26953436]]
Bias Camada Escondida: 
[0.17448858]
Iteration 1, loss = 0.69306476
Iteration 2, loss = 0.65072362
Iteration 3, loss = 0.63866125
Iteration 4, loss = 0.63157106
Iteration 5, loss = 0.61895570
Iteration 6, loss = 0.60335549
Iteration 7, loss = 0.58517656
Iteration 8, loss = 0.56434226
Iteration 9, loss = 0.52752787
Iteration 10, loss = 0.50543857
Iteration 11, loss = 0.47194969
Iteration 12, loss = 0.44047557
Iteration 13, loss = 0.41450493
Iteration 14, loss = 0.39367357
Iteration 15, loss = 0.37231123
Iteration 16, loss = 0.35157438
Iteration 17, loss = 0.33587457
Iteration 18, loss = 0.32515322
Iteration 19, loss = 0.31723704
Iteration 20, loss = 0.30801327
Iteration 21, loss = 0.29907653
Iteration 22, loss = 0.29325616
Iteration 23, loss = 0.28774671
Iteration 24, loss = 0.28071708
Iteration 25, loss = 0.27375151
Iteration 26, loss = 0.27004820
Iteration 27, loss = 0.26307498
Iteration 28, loss = 0.25767052
Iteration 29, loss = 0.25277798
Iteration 30, loss = 0.24934885
Iteration 31, loss = 0.24698118
Iteration 32, loss = 0.25308561
Iteration 33, loss = 0.25040763
Iteration 34, loss = 0.22819968
Iteration 35, loss = 0.22807267
Iteration 36, loss = 0.21713203
Iteration 37, loss = 0.21451370
Iteration 38, loss = 0.21054002
Iteration 39, loss = 0.20159371
Iteration 40, loss = 0.21697819
Iteration 41, loss = 0.20415399
Iteration 42, loss = 0.18834120
Iteration 43, loss = 0.19180854
Iteration 44, loss = 0.18122469
Iteration 45, loss = 0.17704972
Iteration 46, loss = 0.17202049
Iteration 47, loss = 0.16908725
Iteration 48, loss = 0.16562480
Iteration 49, loss = 0.16285548
Iteration 50, loss = 0.15912033
Iteration 51, loss = 0.15685699
Iteration 52, loss = 0.15355826
Iteration 53, loss = 0.15025254
Iteration 54, loss = 0.15170368
Iteration 55, loss = 0.14507434
Iteration 56, loss = 0.14560944
Iteration 57, loss = 0.14686222
Iteration 58, loss = 0.14308069
Iteration 59, loss = 0.13720959
Iteration 60, loss = 0.13468410
Iteration 61, loss = 0.13156024
Iteration 62, loss = 0.12928881
Iteration 63, loss = 0.12743048
Iteration 64, loss = 0.12603716
Iteration 65, loss = 0.12494125
Iteration 66, loss = 0.12299374
Iteration 67, loss = 0.12028078
Iteration 68, loss = 0.11910856
Iteration 69, loss = 0.11699828
Iteration 70, loss = 0.11623455
Iteration 71, loss = 0.11605052
Iteration 72, loss = 0.11561930
Iteration 73, loss = 0.11493064
Iteration 74, loss = 0.11329827
Iteration 75, loss = 0.11014928
Iteration 76, loss = 0.10889478
Iteration 77, loss = 0.10747557
Iteration 78, loss = 0.10522645
Iteration 79, loss = 0.10382732
Iteration 80, loss = 0.10609649
Iteration 81, loss = 0.10102938
Iteration 82, loss = 0.10039466
Iteration 83, loss = 0.09877339
Iteration 84, loss = 0.09767886
Iteration 85, loss = 0.09644403
Iteration 86, loss = 0.09552207
Iteration 87, loss = 0.09433337
Iteration 88, loss = 0.09347509
Iteration 89, loss = 0.09261004
Iteration 90, loss = 0.09175070
Iteration 91, loss = 0.09016894
Iteration 92, loss = 0.08927497
Iteration 93, loss = 0.09046445
Iteration 94, loss = 0.08980966
Iteration 95, loss = 0.08681573
Iteration 96, loss = 0.08601212
Iteration 97, loss = 0.08488090
Iteration 98, loss = 0.08441885
Iteration 99, loss = 0.08449823
Iteration 100, loss = 0.08478548
Iteration 101, loss = 0.08407762
Iteration 102, loss = 0.08289519
Iteration 103, loss = 0.08222008
Iteration 104, loss = 0.08230279
Iteration 105, loss = 0.07920938
Iteration 106, loss = 0.07918739
Iteration 107, loss = 0.07829003
Iteration 108, loss = 0.07762881
Iteration 109, loss = 0.07682363
Iteration 110, loss = 0.07602016
Iteration 111, loss = 0.07547264
Iteration 112, loss = 0.07427001
Iteration 113, loss = 0.07386856
Iteration 114, loss = 0.07412980
Iteration 115, loss = 0.07451820
Iteration 116, loss = 0.07397429
Iteration 117, loss = 0.07278011
Iteration 118, loss = 0.07142175
Iteration 119, loss = 0.06982768
Iteration 120, loss = 0.06948605
Iteration 121, loss = 0.06904079
Iteration 122, loss = 0.06826745
Iteration 123, loss = 0.06776274
Iteration 124, loss = 0.06696311
Iteration 125, loss = 0.06644235
Iteration 126, loss = 0.06608515
Iteration 127, loss = 0.06559871
Iteration 128, loss = 0.06515293
Iteration 129, loss = 0.06474098
Iteration 130, loss = 0.06463586
Iteration 131, loss = 0.06438622
Iteration 132, loss = 0.06308775
Iteration 133, loss = 0.06242211
Iteration 134, loss = 0.06241756
Iteration 135, loss = 0.06130509
Iteration 136, loss = 0.06157301
Iteration 137, loss = 0.06078867
Iteration 138, loss = 0.05956468
Iteration 139, loss = 0.05908351
Iteration 140, loss = 0.05920362
Iteration 141, loss = 0.05887686
Iteration 142, loss = 0.05845388
Iteration 143, loss = 0.05766848
Iteration 144, loss = 0.05702638
Iteration 145, loss = 0.05700409
Iteration 146, loss = 0.05701285
Iteration 147, loss = 0.05684313
Iteration 148, loss = 0.05614447
Iteration 149, loss = 0.05524593
Iteration 150, loss = 0.05530108
Iteration 151, loss = 0.05415162
Iteration 152, loss = 0.05393207
Iteration 153, loss = 0.05362383
Iteration 154, loss = 0.05313453
Iteration 155, loss = 0.05287531
Iteration 156, loss = 0.05225781
Iteration 157, loss = 0.05218425
Iteration 158, loss = 0.05228705
Iteration 159, loss = 0.05093935
Iteration 160, loss = 0.05119246
Iteration 161, loss = 0.05090095
Iteration 162, loss = 0.05044075
Iteration 163, loss = 0.04982472
Iteration 164, loss = 0.04924958
Iteration 165, loss = 0.04948214
Iteration 166, loss = 0.04989486
Iteration 167, loss = 0.04853223
Iteration 168, loss = 0.04782824
Iteration 169, loss = 0.04763623
Iteration 170, loss = 0.04727870
Iteration 171, loss = 0.04688917
Iteration 172, loss = 0.04665016
Iteration 173, loss = 0.04742932
Iteration 174, loss = 0.04667488
Iteration 175, loss = 0.04581584
Iteration 176, loss = 0.04525194
Iteration 177, loss = 0.04487902
Iteration 178, loss = 0.04480268
Iteration 179, loss = 0.04546270
Iteration 180, loss = 0.04504782
Iteration 181, loss = 0.04440972
Iteration 182, loss = 0.04364919
Iteration 183, loss = 0.04293283
Iteration 184, loss = 0.04263921
Iteration 185, loss = 0.04278857
Iteration 186, loss = 0.04286312
Iteration 187, loss = 0.04279479
Iteration 188, loss = 0.04218651
Iteration 189, loss = 0.04272577
Iteration 190, loss = 0.04343912
Iteration 191, loss = 0.04133866
Iteration 192, loss = 0.04060933
Iteration 193, loss = 0.04115212
Iteration 194, loss = 0.04060308
Iteration 195, loss = 0.04003004
Iteration 196, loss = 0.03951575
Iteration 197, loss = 0.03917176
Iteration 198, loss = 0.03874530
Iteration 199, loss = 0.03839721
Iteration 200, loss = 0.03812420
Iteration 201, loss = 0.03777364
Iteration 202, loss = 0.03755555
Iteration 203, loss = 0.03721561
Iteration 204, loss = 0.03690227
Iteration 205, loss = 0.03672126
Iteration 206, loss = 0.03617782
Iteration 207, loss = 0.03742966
Iteration 208, loss = 0.03671007
Iteration 209, loss = 0.03615461
Iteration 210, loss = 0.03566765
Iteration 211, loss = 0.03512842
Iteration 212, loss = 0.03477878
Iteration 213, loss = 0.03448974
Iteration 214, loss = 0.03442358
Iteration 215, loss = 0.03433336
Iteration 216, loss = 0.03419286
Iteration 217, loss = 0.03389106
Iteration 218, loss = 0.03363329
Iteration 219, loss = 0.03296122
Iteration 220, loss = 0.03308220
Iteration 221, loss = 0.03426505
Iteration 222, loss = 0.03348108
Iteration 223, loss = 0.03223156
Iteration 224, loss = 0.03167262
Iteration 225, loss = 0.03192000
Iteration 226, loss = 0.03194571
Iteration 227, loss = 0.03170546
Iteration 228, loss = 0.03149266
Iteration 229, loss = 0.03119368
Iteration 230, loss = 0.03071175
Iteration 231, loss = 0.03142933
Iteration 232, loss = 0.03024025
Iteration 233, loss = 0.02968412
Iteration 234, loss = 0.03029045
Iteration 235, loss = 0.02993010
Iteration 236, loss = 0.02978456
Iteration 237, loss = 0.02936146
Iteration 238, loss = 0.02881068
Iteration 239, loss = 0.02867505
Iteration 240, loss = 0.02838213
Iteration 241, loss = 0.02818908
Iteration 242, loss = 0.02801604
Iteration 243, loss = 0.02791652
Iteration 244, loss = 0.02788203
Iteration 245, loss = 0.02747838
Iteration 246, loss = 0.02724854
Iteration 247, loss = 0.02695210
Iteration 248, loss = 0.02669127
Iteration 249, loss = 0.02649507
Iteration 250, loss = 0.02646158
Iteration 251, loss = 0.02633615
Iteration 252, loss = 0.02609298
Iteration 253, loss = 0.02581241
Iteration 254, loss = 0.02569886
Iteration 255, loss = 0.02531221
Iteration 256, loss = 0.02590148
Iteration 257, loss = 0.02591996
Iteration 258, loss = 0.02530068
Iteration 259, loss = 0.02441556
Iteration 260, loss = 0.02467525
Iteration 261, loss = 0.02590677
Iteration 262, loss = 0.02452608
Iteration 263, loss = 0.02408846
Iteration 264, loss = 0.02367912
Iteration 265, loss = 0.02354829
Iteration 266, loss = 0.02327845
Iteration 267, loss = 0.02335185
Iteration 268, loss = 0.02334987
Iteration 269, loss = 0.02334441
Iteration 270, loss = 0.02345506
Iteration 271, loss = 0.02237649
Iteration 272, loss = 0.02270730
Iteration 273, loss = 0.02298599
Iteration 274, loss = 0.02285774
Iteration 275, loss = 0.02234084
Iteration 276, loss = 0.02181750
Iteration 277, loss = 0.02177749
Iteration 278, loss = 0.02153488
Iteration 279, loss = 0.02142057
Iteration 280, loss = 0.02146706
Iteration 281, loss = 0.02139352
Iteration 282, loss = 0.02094918
Iteration 283, loss = 0.02052330
Iteration 284, loss = 0.02073210
Iteration 285, loss = 0.02023149
Iteration 286, loss = 0.02039736
Iteration 287, loss = 0.02000270
Iteration 288, loss = 0.01985746
Iteration 289, loss = 0.01967823
Iteration 290, loss = 0.01955946
Iteration 291, loss = 0.01969712
Iteration 292, loss = 0.01983064
Iteration 293, loss = 0.01972331
Iteration 294, loss = 0.01931475
Iteration 295, loss = 0.01881896
Iteration 296, loss = 0.01869094
Iteration 297, loss = 0.01850847
Iteration 298, loss = 0.01846430
Iteration 299, loss = 0.01835866
Iteration 300, loss = 0.01814280
PARAMETROS DE INICIALIZACAO DA REDE
NUMERO DE NEURONIOS 
Camada de Entrada: 34
Camada Escondida: 15
Camada de Saida: 1

--- PARAMETROS DE CONFIGURACAO DA REDE ---
Numero de Epocas: 300
Taxa de Aprendizado: adaptive
Taxa de Aprendizado Inicial: 0.3
METRICAS

RESULTADOS:

[0 0 1 0 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1 1 1 1 1 1 1
 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 1 1 1
 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 1 0 0 1 1 0 1 1 1 0 1 1 0]
ACURACIA: 0.9245283018867925

