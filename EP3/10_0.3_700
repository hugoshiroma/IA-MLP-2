Pesos Camada de Entrada: 
[[-0.18073497 -0.08532263 -0.03759321  0.03637508  0.14354969 -0.18134085
  -0.00457533  0.06341594  0.15770753  0.17505825]
 [-0.00118767  0.0411729  -0.14461748 -0.02181925  0.13917979  0.10953653
  -0.14675455 -0.09440013 -0.18741713 -0.13386544]
 [-0.17549226  0.09076949 -0.12630401 -0.14526832  0.09609441  0.11059752
   0.08755335 -0.01552905 -0.02639812 -0.06919094]
 [ 0.13077215 -0.12583164  0.07777526  0.14878531  0.06500235 -0.07584685
   0.04670427  0.15690819 -0.18495036  0.11170683]
 [-0.00826766  0.08998538 -0.03161049 -0.16081865  0.12184653 -0.09330135
  -0.17834054 -0.06873502 -0.05389673  0.1010166 ]
 [ 0.16558383 -0.12984721  0.05622482  0.09862251 -0.05671801 -0.09640655
  -0.02018337  0.02626514  0.02782054  0.20936201]
 [ 0.0235111   0.1017584   0.13204089  0.14081324  0.14524818  0.06201241
  -0.04086934  0.13219233  0.13836625 -0.06343754]
 [ 0.09296149  0.1797735  -0.04426363 -0.17967057  0.10505393 -0.18259712
  -0.17336664  0.06800608 -0.2051334  -0.17858638]
 [-0.01760918 -0.00932573 -0.04149543  0.06912592  0.00181021 -0.0055614
   0.12851527  0.11520913  0.11303259 -0.15768439]
 [ 0.10596738  0.00576261 -0.12190796 -0.01428615 -0.0132623  -0.03141627
  -0.03824793 -0.18944342 -0.05371187  0.00286842]
 [ 0.04081419  0.00674018 -0.07481929 -0.10173299 -0.02298961  0.03126582
  -0.20578202  0.0066053   0.10389964  0.07408657]
 [ 0.07533163 -0.14095588 -0.11299485  0.08504459 -0.12183788 -0.19127354
  -0.00134471  0.03545855  0.07484915 -0.20119708]
 [ 0.00567892  0.03560041  0.06752953  0.18482363  0.18845077 -0.06393904
   0.00501835 -0.17089137  0.11796058  0.13383932]
 [-0.11403201 -0.15803019  0.17833712 -0.1337518  -0.16157898  0.12492108
  -0.14616039  0.16791465  0.15692918  0.10501725]
 [-0.09140353  0.08048271  0.08639629 -0.01852738  0.10989266 -0.01815307
   0.06022844  0.10455791  0.20439447  0.08563466]
 [-0.15608604 -0.20869482 -0.20371468 -0.1395019  -0.11104052 -0.14376776
   0.10578234  0.1914961  -0.17370976  0.18877436]
 [-0.16027753  0.08729581  0.11483978  0.05591547  0.19272214  0.09141099
   0.1366572   0.16016145 -0.13284581 -0.0661623 ]
 [-0.0959351  -0.05112351  0.14153067  0.00881258 -0.08216088  0.07989927
   0.03583925 -0.06443141 -0.14260603 -0.13873706]
 [-0.01259886 -0.10616486  0.01622905 -0.19498553 -0.1224142  -0.01642167
   0.11294461  0.13772929 -0.02298389  0.0116604 ]
 [ 0.1711554   0.06628564 -0.13946097 -0.06714502 -0.14162502 -0.12509656
  -0.0168865   0.06348844  0.01663944  0.11482833]
 [ 0.13787137  0.09745107 -0.06075721  0.04837963 -0.01104718 -0.17479292
   0.15362902 -0.09169965 -0.15562418 -0.14017224]
 [ 0.0590562  -0.16146228 -0.02704047 -0.20129704  0.09571328 -0.13107412
   0.0284797  -0.15228975 -0.10502777 -0.12132122]
 [ 0.19644781 -0.04978979  0.05673464 -0.16693341  0.12043094  0.01739111
   0.19416176  0.07668813  0.02853161 -0.17189064]
 [-0.0337128   0.09555015 -0.02670319  0.1306044  -0.09861458  0.06743691
  -0.21311193  0.13041115  0.17672655  0.10233321]
 [-0.02214124  0.0813303   0.15143566  0.14142938 -0.09285564  0.18740835
   0.0067964  -0.08518523 -0.07080254 -0.08461205]
 [-0.19783818  0.11413238 -0.19814984 -0.07054803  0.04233535 -0.02002352
  -0.10526991 -0.0332383   0.15931206 -0.09021962]
 [ 0.17213873  0.09764137  0.10829666 -0.17041048 -0.18824239  0.02562633
   0.1271594  -0.01029841 -0.00862851  0.09650576]
 [-0.02842882  0.00762669  0.17163617  0.15303983  0.06927566  0.00577911
  -0.04964652 -0.10255314  0.13060101  0.06613128]
 [ 0.04792104 -0.07990682  0.19677422 -0.11581031  0.15402855 -0.02355396
   0.17423941  0.02004433 -0.03630824  0.09407155]
 [ 0.02410413  0.03252188  0.16665175 -0.09307757 -0.04813744 -0.02512973
  -0.15524365 -0.12133092 -0.02737261  0.14063177]
 [-0.09257796  0.14085543 -0.02006186  0.09753822 -0.16681181  0.01860934
  -0.18375865  0.08678338  0.04595374 -0.09856939]
 [-0.20234222 -0.1605269  -0.1448197  -0.01550819 -0.00366692  0.01514447
   0.15846626  0.03222987 -0.109785    0.03497478]
 [ 0.1677227  -0.11041806 -0.15314735 -0.17901155  0.01501732 -0.12300764
  -0.00597941  0.14361048  0.21143286 -0.04861925]
 [ 0.11872907  0.19350231 -0.08947292  0.05346993  0.0529947   0.20248895
  -0.04518672  0.01255418  0.03255997  0.16506952]]
Bias Camada de Entrada: 
[ 0.06128747 -0.1963036   0.11538701  0.00579995  0.09447831  0.0981808
 -0.07127171 -0.03588842 -0.05382969 -0.01961979]
Pesos Camada Escondida: 
[[-0.05796059]
 [-0.13680739]
 [ 0.03018766]
 [ 0.08115575]
 [ 0.35451202]
 [-0.33464514]
 [ 0.34580931]
 [-0.33694955]
 [-0.01889684]
 [ 0.23768687]]
Bias Camada Escondida: 
[-0.28374218]
Iteration 1, loss = 0.70709416
Iteration 2, loss = 0.64086170
Iteration 3, loss = 0.65657205
Iteration 4, loss = 0.62497941
Iteration 5, loss = 0.60979309
Iteration 6, loss = 0.59580005
Iteration 7, loss = 0.57326563
Iteration 8, loss = 0.55399843
Iteration 9, loss = 0.53031674
Iteration 10, loss = 0.50231658
Iteration 11, loss = 0.47421301
Iteration 12, loss = 0.44942152
Iteration 13, loss = 0.42511744
Iteration 14, loss = 0.40217096
Iteration 15, loss = 0.38459357
Iteration 16, loss = 0.37093330
Iteration 17, loss = 0.35304840
Iteration 18, loss = 0.34883049
Iteration 19, loss = 0.33909047
Iteration 20, loss = 0.32293078
Iteration 21, loss = 0.31673143
Iteration 22, loss = 0.30781579
Iteration 23, loss = 0.30254489
Iteration 24, loss = 0.29946888
Iteration 25, loss = 0.28791841
Iteration 26, loss = 0.28294218
Iteration 27, loss = 0.27848603
Iteration 28, loss = 0.27754304
Iteration 29, loss = 0.26637654
Iteration 30, loss = 0.25911169
Iteration 31, loss = 0.25746281
Iteration 32, loss = 0.25564315
Iteration 33, loss = 0.24377046
Iteration 34, loss = 0.23870330
Iteration 35, loss = 0.23591612
Iteration 36, loss = 0.23196679
Iteration 37, loss = 0.22789373
Iteration 38, loss = 0.22162974
Iteration 39, loss = 0.21637023
Iteration 40, loss = 0.21324218
Iteration 41, loss = 0.20979235
Iteration 42, loss = 0.20519265
Iteration 43, loss = 0.20473927
Iteration 44, loss = 0.19843806
Iteration 45, loss = 0.19590804
Iteration 46, loss = 0.19160380
Iteration 47, loss = 0.18854987
Iteration 48, loss = 0.18384151
Iteration 49, loss = 0.17983708
Iteration 50, loss = 0.17493892
Iteration 51, loss = 0.17238684
Iteration 52, loss = 0.16713376
Iteration 53, loss = 0.16662316
Iteration 54, loss = 0.16279891
Iteration 55, loss = 0.15883164
Iteration 56, loss = 0.15501033
Iteration 57, loss = 0.15278687
Iteration 58, loss = 0.15111043
Iteration 59, loss = 0.14857660
Iteration 60, loss = 0.14631359
Iteration 61, loss = 0.14452899
Iteration 62, loss = 0.14375455
Iteration 63, loss = 0.14195684
Iteration 64, loss = 0.13862627
Iteration 65, loss = 0.13957690
Iteration 66, loss = 0.13622388
Iteration 67, loss = 0.13252858
Iteration 68, loss = 0.13026805
Iteration 69, loss = 0.12967465
Iteration 70, loss = 0.12910285
Iteration 71, loss = 0.12807439
Iteration 72, loss = 0.12585735
Iteration 73, loss = 0.12418410
Iteration 74, loss = 0.12394582
Iteration 75, loss = 0.12335659
Iteration 76, loss = 0.12077469
Iteration 77, loss = 0.11875616
Iteration 78, loss = 0.11862600
Iteration 79, loss = 0.11627941
Iteration 80, loss = 0.11527424
Iteration 81, loss = 0.11344408
Iteration 82, loss = 0.11203968
Iteration 83, loss = 0.11133238
Iteration 84, loss = 0.11145464
Iteration 85, loss = 0.11062519
Iteration 86, loss = 0.10912020
Iteration 87, loss = 0.10734047
Iteration 88, loss = 0.10609658
Iteration 89, loss = 0.10632842
Iteration 90, loss = 0.10626945
Iteration 91, loss = 0.10481084
Iteration 92, loss = 0.10278980
Iteration 93, loss = 0.10099921
Iteration 94, loss = 0.09949637
Iteration 95, loss = 0.09858844
Iteration 96, loss = 0.09869218
Iteration 97, loss = 0.09642328
Iteration 98, loss = 0.09580911
Iteration 99, loss = 0.09595491
Iteration 100, loss = 0.09393554
Iteration 101, loss = 0.09326171
Iteration 102, loss = 0.09256772
Iteration 103, loss = 0.09182936
Iteration 104, loss = 0.09154027
Iteration 105, loss = 0.09107246
Iteration 106, loss = 0.09005190
Iteration 107, loss = 0.08894476
Iteration 108, loss = 0.08784915
Iteration 109, loss = 0.08695703
Iteration 110, loss = 0.08645635
Iteration 111, loss = 0.08595196
Iteration 112, loss = 0.08529320
Iteration 113, loss = 0.08430827
Iteration 114, loss = 0.08327385
Iteration 115, loss = 0.08390845
Iteration 116, loss = 0.08205839
Iteration 117, loss = 0.08092326
Iteration 118, loss = 0.08195305
Iteration 119, loss = 0.08123575
Iteration 120, loss = 0.07956827
Iteration 121, loss = 0.07835250
Iteration 122, loss = 0.07755559
Iteration 123, loss = 0.07692961
Iteration 124, loss = 0.07646937
Iteration 125, loss = 0.07592410
Iteration 126, loss = 0.07511522
Iteration 127, loss = 0.07452004
Iteration 128, loss = 0.07358823
Iteration 129, loss = 0.07346426
Iteration 130, loss = 0.07275271
Iteration 131, loss = 0.07256386
Iteration 132, loss = 0.07222728
Iteration 133, loss = 0.07103044
Iteration 134, loss = 0.07017198
Iteration 135, loss = 0.06945781
Iteration 136, loss = 0.06873979
Iteration 137, loss = 0.06818724
Iteration 138, loss = 0.06773560
Iteration 139, loss = 0.06688136
Iteration 140, loss = 0.06678954
Iteration 141, loss = 0.06758741
Iteration 142, loss = 0.06577892
Iteration 143, loss = 0.06510265
Iteration 144, loss = 0.06527176
Iteration 145, loss = 0.06412235
Iteration 146, loss = 0.06342104
Iteration 147, loss = 0.06279231
Iteration 148, loss = 0.06198362
Iteration 149, loss = 0.06130912
Iteration 150, loss = 0.06078555
Iteration 151, loss = 0.06028597
Iteration 152, loss = 0.06028153
Iteration 153, loss = 0.05965909
Iteration 154, loss = 0.05889979
Iteration 155, loss = 0.05828684
Iteration 156, loss = 0.05777769
Iteration 157, loss = 0.05762036
Iteration 158, loss = 0.05686145
Iteration 159, loss = 0.05638760
Iteration 160, loss = 0.05659691
Iteration 161, loss = 0.05563820
Iteration 162, loss = 0.05517286
Iteration 163, loss = 0.05443431
Iteration 164, loss = 0.05430031
Iteration 165, loss = 0.05456935
Iteration 166, loss = 0.05431872
Iteration 167, loss = 0.05294796
Iteration 168, loss = 0.05192355
Iteration 169, loss = 0.05070486
Iteration 170, loss = 0.05013376
Iteration 171, loss = 0.04971736
Iteration 172, loss = 0.04938249
Iteration 173, loss = 0.04878823
Iteration 174, loss = 0.04823416
Iteration 175, loss = 0.04784783
Iteration 176, loss = 0.04756734
Iteration 177, loss = 0.04800546
Iteration 178, loss = 0.04660003
Iteration 179, loss = 0.04627383
Iteration 180, loss = 0.04603042
Iteration 181, loss = 0.04566388
Iteration 182, loss = 0.04547979
Iteration 183, loss = 0.04480715
Iteration 184, loss = 0.04444861
Iteration 185, loss = 0.04398372
Iteration 186, loss = 0.04365754
Iteration 187, loss = 0.04348686
Iteration 188, loss = 0.04330443
Iteration 189, loss = 0.04340506
Iteration 190, loss = 0.04316114
Iteration 191, loss = 0.04232267
Iteration 192, loss = 0.04144996
Iteration 193, loss = 0.04077229
Iteration 194, loss = 0.04054749
Iteration 195, loss = 0.04009603
Iteration 196, loss = 0.04022449
Iteration 197, loss = 0.04140039
Iteration 198, loss = 0.04211439
Iteration 199, loss = 0.04009424
Iteration 200, loss = 0.03841266
Iteration 201, loss = 0.03731680
Iteration 202, loss = 0.03748921
Iteration 203, loss = 0.03682279
Iteration 204, loss = 0.03651421
Iteration 205, loss = 0.03699128
Iteration 206, loss = 0.03637826
Iteration 207, loss = 0.03597532
Iteration 208, loss = 0.03550366
Iteration 209, loss = 0.03497396
Iteration 210, loss = 0.03473399
Iteration 211, loss = 0.03484446
Iteration 212, loss = 0.03497521
Iteration 213, loss = 0.03307610
Iteration 214, loss = 0.03312252
Iteration 215, loss = 0.03377683
Iteration 216, loss = 0.03262056
Iteration 217, loss = 0.03218547
Iteration 218, loss = 0.03302349
Iteration 219, loss = 0.03201253
Iteration 220, loss = 0.03129521
Iteration 221, loss = 0.03093796
Iteration 222, loss = 0.03042697
Iteration 223, loss = 0.03028561
Iteration 224, loss = 0.02980505
Iteration 225, loss = 0.03028080
Iteration 226, loss = 0.02971352
Iteration 227, loss = 0.02966493
Iteration 228, loss = 0.02977375
Iteration 229, loss = 0.02959356
Iteration 230, loss = 0.02919142
Iteration 231, loss = 0.02858586
Iteration 232, loss = 0.02825669
Iteration 233, loss = 0.02785088
Iteration 234, loss = 0.02736016
Iteration 235, loss = 0.02699006
Iteration 236, loss = 0.02701545
Iteration 237, loss = 0.02715959
Iteration 238, loss = 0.02743479
Iteration 239, loss = 0.02774300
Iteration 240, loss = 0.02774098
Iteration 241, loss = 0.02705052
Iteration 242, loss = 0.02621838
Iteration 243, loss = 0.02571840
Iteration 244, loss = 0.02551254
Iteration 245, loss = 0.02543810
Iteration 246, loss = 0.02591155
Iteration 247, loss = 0.02676237
Iteration 248, loss = 0.02585814
Iteration 249, loss = 0.02506696
Iteration 250, loss = 0.02411094
Iteration 251, loss = 0.02407071
Iteration 252, loss = 0.02373666
Iteration 253, loss = 0.02422939
Iteration 254, loss = 0.02410378
Iteration 255, loss = 0.02363743
Iteration 256, loss = 0.02312996
Iteration 257, loss = 0.02282898
Iteration 258, loss = 0.02289824
Iteration 259, loss = 0.02256416
Iteration 260, loss = 0.02247299
Iteration 261, loss = 0.02237607
Iteration 262, loss = 0.02212425
Iteration 263, loss = 0.02182105
Iteration 264, loss = 0.02163697
Iteration 265, loss = 0.02187926
Iteration 266, loss = 0.02125027
Iteration 267, loss = 0.02122505
Iteration 268, loss = 0.02180820
Iteration 269, loss = 0.02117985
Iteration 270, loss = 0.02080836
Iteration 271, loss = 0.02043488
Iteration 272, loss = 0.02037881
Iteration 273, loss = 0.02035098
Iteration 274, loss = 0.02026822
Iteration 275, loss = 0.01975520
Iteration 276, loss = 0.02013430
Iteration 277, loss = 0.02022531
Iteration 278, loss = 0.02007848
Iteration 279, loss = 0.01961084
Iteration 280, loss = 0.01931482
Iteration 281, loss = 0.01941379
Iteration 282, loss = 0.01922799
Iteration 283, loss = 0.01900573
Iteration 284, loss = 0.01899431
Iteration 285, loss = 0.01919124
Iteration 286, loss = 0.01933674
Iteration 287, loss = 0.01918331
Iteration 288, loss = 0.01820271
Iteration 289, loss = 0.01817924
Iteration 290, loss = 0.01891267
Iteration 291, loss = 0.01865740
Iteration 292, loss = 0.01809304
Iteration 293, loss = 0.01775390
Iteration 294, loss = 0.01747554
Iteration 295, loss = 0.01733474
Iteration 296, loss = 0.01721480
Iteration 297, loss = 0.01708663
Iteration 298, loss = 0.01698684
Iteration 299, loss = 0.01689474
Iteration 300, loss = 0.01683620
Iteration 301, loss = 0.01695176
Iteration 302, loss = 0.01665310
Iteration 303, loss = 0.01642908
Iteration 304, loss = 0.01628120
Iteration 305, loss = 0.01631812
Iteration 306, loss = 0.01639580
Iteration 307, loss = 0.01634213
Iteration 308, loss = 0.01599893
Iteration 309, loss = 0.01603578
Iteration 310, loss = 0.01580253
Iteration 311, loss = 0.01582035
Iteration 312, loss = 0.01555907
Iteration 313, loss = 0.01557301
Iteration 314, loss = 0.01532752
Iteration 315, loss = 0.01525932
Iteration 316, loss = 0.01518846
Iteration 317, loss = 0.01516966
Iteration 318, loss = 0.01521009
Iteration 319, loss = 0.01504492
Iteration 320, loss = 0.01489726
Iteration 321, loss = 0.01475375
Iteration 322, loss = 0.01470093
Iteration 323, loss = 0.01464884
Iteration 324, loss = 0.01444802
Iteration 325, loss = 0.01445256
Iteration 326, loss = 0.01428040
Iteration 327, loss = 0.01415287
Iteration 328, loss = 0.01405779
Iteration 329, loss = 0.01397870
Iteration 330, loss = 0.01389664
Iteration 331, loss = 0.01385383
Iteration 332, loss = 0.01384436
Iteration 333, loss = 0.01374261
Iteration 334, loss = 0.01369876
Iteration 335, loss = 0.01363964
Iteration 336, loss = 0.01353206
Iteration 337, loss = 0.01354440
Iteration 338, loss = 0.01351113
Iteration 339, loss = 0.01336718
Iteration 340, loss = 0.01338445
Iteration 341, loss = 0.01320766
Iteration 342, loss = 0.01312217
Iteration 343, loss = 0.01314306
Iteration 344, loss = 0.01298634
Iteration 345, loss = 0.01293746
Iteration 346, loss = 0.01296604
Iteration 347, loss = 0.01299415
Iteration 348, loss = 0.01287312
Iteration 349, loss = 0.01271496
Iteration 350, loss = 0.01255124
Iteration 351, loss = 0.01248013
Iteration 352, loss = 0.01247134
Iteration 353, loss = 0.01244029
Iteration 354, loss = 0.01239641
Iteration 355, loss = 0.01224172
Iteration 356, loss = 0.01225260
Iteration 357, loss = 0.01210720
Iteration 358, loss = 0.01204658
Iteration 359, loss = 0.01204592
Iteration 360, loss = 0.01200206
Iteration 361, loss = 0.01191568
Iteration 362, loss = 0.01187728
Iteration 363, loss = 0.01189501
Iteration 364, loss = 0.01195384
Iteration 365, loss = 0.01177588
Iteration 366, loss = 0.01162282
Iteration 367, loss = 0.01152823
Iteration 368, loss = 0.01160965
Iteration 369, loss = 0.01171125
Iteration 370, loss = 0.01168790
Iteration 371, loss = 0.01155992
Iteration 372, loss = 0.01138770
Iteration 373, loss = 0.01122024
Iteration 374, loss = 0.01127797
Iteration 375, loss = 0.01114159
Iteration 376, loss = 0.01114856
Iteration 377, loss = 0.01115638
Iteration 378, loss = 0.01114614
Iteration 379, loss = 0.01089715
Iteration 380, loss = 0.01099770
Iteration 381, loss = 0.01082889
Iteration 382, loss = 0.01073050
Iteration 383, loss = 0.01078891
Iteration 384, loss = 0.01067686
Iteration 385, loss = 0.01060887
Iteration 386, loss = 0.01054983
Iteration 387, loss = 0.01050708
Iteration 388, loss = 0.01042776
Iteration 389, loss = 0.01039971
Iteration 390, loss = 0.01034484
Iteration 391, loss = 0.01030767
Iteration 392, loss = 0.01026840
Iteration 393, loss = 0.01024294
Iteration 394, loss = 0.01021005
Iteration 395, loss = 0.01017191
Iteration 396, loss = 0.01013215
Iteration 397, loss = 0.01008858
Iteration 398, loss = 0.01007843
Iteration 399, loss = 0.01001282
Iteration 400, loss = 0.00995294
Iteration 401, loss = 0.00984212
Iteration 402, loss = 0.00976667
Iteration 403, loss = 0.00970995
Iteration 404, loss = 0.00972010
Iteration 405, loss = 0.00987955
Iteration 406, loss = 0.00964058
Iteration 407, loss = 0.00955119
Iteration 408, loss = 0.00952331
Iteration 409, loss = 0.00952299
Iteration 410, loss = 0.00947052
Iteration 411, loss = 0.00942157
Iteration 412, loss = 0.00940557
Iteration 413, loss = 0.00935210
Iteration 414, loss = 0.00934638
Iteration 415, loss = 0.00930311
Iteration 416, loss = 0.00923470
Iteration 417, loss = 0.00918480
Iteration 418, loss = 0.00914827
Iteration 419, loss = 0.00918094
Iteration 420, loss = 0.00908619
Iteration 421, loss = 0.00905183
Iteration 422, loss = 0.00900386
Iteration 423, loss = 0.00899765
Iteration 424, loss = 0.00899069
Iteration 425, loss = 0.00899070
Iteration 426, loss = 0.00896242
Iteration 427, loss = 0.00891417
Iteration 428, loss = 0.00884045
Iteration 429, loss = 0.00876985
Iteration 430, loss = 0.00873144
Iteration 431, loss = 0.00870516
Iteration 432, loss = 0.00867175
Iteration 433, loss = 0.00864824
Iteration 434, loss = 0.00864227
Iteration 435, loss = 0.00862478
Iteration 436, loss = 0.00859884
Iteration 437, loss = 0.00856396
Iteration 438, loss = 0.00856646
Iteration 439, loss = 0.00852309
Iteration 440, loss = 0.00850807
Iteration 441, loss = 0.00848961
Iteration 442, loss = 0.00847573
Iteration 443, loss = 0.00841430
Iteration 444, loss = 0.00839072
Iteration 445, loss = 0.00835910
Iteration 446, loss = 0.00829930
Iteration 447, loss = 0.00827175
Iteration 448, loss = 0.00826126
Iteration 449, loss = 0.00825069
Iteration 450, loss = 0.00814806
Iteration 451, loss = 0.00819071
Iteration 452, loss = 0.00813977
Iteration 453, loss = 0.00817829
Iteration 454, loss = 0.00815785
Iteration 455, loss = 0.00808111
Iteration 456, loss = 0.00800581
Iteration 457, loss = 0.00797184
Iteration 458, loss = 0.00796982
Iteration 459, loss = 0.00792268
Iteration 460, loss = 0.00787441
Iteration 461, loss = 0.00782969
Iteration 462, loss = 0.00778052
Iteration 463, loss = 0.00777396
Iteration 464, loss = 0.00779694
Iteration 465, loss = 0.00788200
Iteration 466, loss = 0.00790839
Iteration 467, loss = 0.00792216
Iteration 468, loss = 0.00771515
Iteration 469, loss = 0.00760303
Iteration 470, loss = 0.00756128
Iteration 471, loss = 0.00754684
Iteration 472, loss = 0.00762427
Iteration 473, loss = 0.00760095
Iteration 474, loss = 0.00757177
Iteration 475, loss = 0.00753587
Iteration 476, loss = 0.00746253
Iteration 477, loss = 0.00743676
Iteration 478, loss = 0.00744361
Iteration 479, loss = 0.00733446
Iteration 480, loss = 0.00732289
Iteration 481, loss = 0.00733193
Iteration 482, loss = 0.00728418
Iteration 483, loss = 0.00723050
Iteration 484, loss = 0.00719254
Iteration 485, loss = 0.00717016
Iteration 486, loss = 0.00718259
Iteration 487, loss = 0.00724459
Iteration 488, loss = 0.00713838
Iteration 489, loss = 0.00711390
Iteration 490, loss = 0.00705608
Iteration 491, loss = 0.00701449
Iteration 492, loss = 0.00701298
Iteration 493, loss = 0.00701066
Iteration 494, loss = 0.00698998
Iteration 495, loss = 0.00696683
Iteration 496, loss = 0.00696490
Iteration 497, loss = 0.00694748
Iteration 498, loss = 0.00691619
Iteration 499, loss = 0.00687813
Iteration 500, loss = 0.00683914
Iteration 501, loss = 0.00681903
Iteration 502, loss = 0.00678373
Iteration 503, loss = 0.00677133
Iteration 504, loss = 0.00675407
Iteration 505, loss = 0.00675080
Iteration 506, loss = 0.00676290
Iteration 507, loss = 0.00666680
Iteration 508, loss = 0.00663691
Iteration 509, loss = 0.00671173
Iteration 510, loss = 0.00665385
Iteration 511, loss = 0.00660963
Iteration 512, loss = 0.00657631
Iteration 513, loss = 0.00656107
Iteration 514, loss = 0.00653908
Iteration 515, loss = 0.00653383
Iteration 516, loss = 0.00654731
Iteration 517, loss = 0.00648430
Iteration 518, loss = 0.00643018
Iteration 519, loss = 0.00641896
Iteration 520, loss = 0.00643545
Iteration 521, loss = 0.00644502
Iteration 522, loss = 0.00643471
Iteration 523, loss = 0.00643095
Iteration 524, loss = 0.00635156
Iteration 525, loss = 0.00631503
Iteration 526, loss = 0.00636013
Iteration 527, loss = 0.00626279
Iteration 528, loss = 0.00625489
Iteration 529, loss = 0.00621820
Iteration 530, loss = 0.00619434
Iteration 531, loss = 0.00618800
Iteration 532, loss = 0.00618252
Iteration 533, loss = 0.00615922
Iteration 534, loss = 0.00614157
Iteration 535, loss = 0.00614125
Iteration 536, loss = 0.00617005
Iteration 537, loss = 0.00608818
Iteration 538, loss = 0.00608092
Iteration 539, loss = 0.00613673
Iteration 540, loss = 0.00610881
Iteration 541, loss = 0.00607790
Iteration 542, loss = 0.00599833
Iteration 543, loss = 0.00594749
Iteration 544, loss = 0.00600241
Iteration 545, loss = 0.00598493
Iteration 546, loss = 0.00594120
Iteration 547, loss = 0.00589723
Iteration 548, loss = 0.00587772
Iteration 549, loss = 0.00584879
Iteration 550, loss = 0.00583612
Iteration 551, loss = 0.00581768
Iteration 552, loss = 0.00581269
Iteration 553, loss = 0.00581313
Iteration 554, loss = 0.00580693
Iteration 555, loss = 0.00578822
Iteration 556, loss = 0.00576196
Iteration 557, loss = 0.00573081
Iteration 558, loss = 0.00571083
Iteration 559, loss = 0.00567774
Iteration 560, loss = 0.00565650
Iteration 561, loss = 0.00565075
Iteration 562, loss = 0.00565484
Iteration 563, loss = 0.00566546
Iteration 564, loss = 0.00565867
Iteration 565, loss = 0.00564792
Iteration 566, loss = 0.00559095
Iteration 567, loss = 0.00556741
Iteration 568, loss = 0.00558997
Iteration 569, loss = 0.00558539
Iteration 570, loss = 0.00558683
Iteration 571, loss = 0.00560005
Iteration 572, loss = 0.00553776
Iteration 573, loss = 0.00548789
Iteration 574, loss = 0.00546860
Iteration 575, loss = 0.00545352
Iteration 576, loss = 0.00545238
Iteration 577, loss = 0.00543832
Iteration 578, loss = 0.00541535
Iteration 579, loss = 0.00541150
Iteration 580, loss = 0.00538150
Iteration 581, loss = 0.00537013
Iteration 582, loss = 0.00535256
Iteration 583, loss = 0.00533449
Iteration 584, loss = 0.00536018
Iteration 585, loss = 0.00530938
Iteration 586, loss = 0.00529810
Iteration 587, loss = 0.00528561
Iteration 588, loss = 0.00527837
Iteration 589, loss = 0.00527170
Iteration 590, loss = 0.00526590
Iteration 591, loss = 0.00526370
Iteration 592, loss = 0.00524065
Iteration 593, loss = 0.00523286
Iteration 594, loss = 0.00524516
Iteration 595, loss = 0.00517712
Iteration 596, loss = 0.00515869
Iteration 597, loss = 0.00514036
Iteration 598, loss = 0.00512037
Iteration 599, loss = 0.00510454
Iteration 600, loss = 0.00509711
Iteration 601, loss = 0.00507983
Iteration 602, loss = 0.00507971
Iteration 603, loss = 0.00507630
Iteration 604, loss = 0.00506068
Iteration 605, loss = 0.00503660
Iteration 606, loss = 0.00501003
Iteration 607, loss = 0.00500593
Iteration 608, loss = 0.00499258
Iteration 609, loss = 0.00497587
Iteration 610, loss = 0.00496435
Iteration 611, loss = 0.00494954
Iteration 612, loss = 0.00493664
Iteration 613, loss = 0.00493759
Iteration 614, loss = 0.00492058
Iteration 615, loss = 0.00491295
Iteration 616, loss = 0.00491603
Iteration 617, loss = 0.00492585
Iteration 618, loss = 0.00494063
Iteration 619, loss = 0.00496360
Iteration 620, loss = 0.00486457
Iteration 621, loss = 0.00485934
Iteration 622, loss = 0.00484037
Iteration 623, loss = 0.00483258
Iteration 624, loss = 0.00482231
Iteration 625, loss = 0.00482962
Iteration 626, loss = 0.00486988
Iteration 627, loss = 0.00481841
Iteration 628, loss = 0.00479499
Iteration 629, loss = 0.00476129
Iteration 630, loss = 0.00474933
Iteration 631, loss = 0.00472965
Iteration 632, loss = 0.00472401
Iteration 633, loss = 0.00471735
Iteration 634, loss = 0.00472275
Iteration 635, loss = 0.00473684
Iteration 636, loss = 0.00471091
Iteration 637, loss = 0.00468099
Iteration 638, loss = 0.00466363
Iteration 639, loss = 0.00464451
Iteration 640, loss = 0.00466821
Iteration 641, loss = 0.00462039
Iteration 642, loss = 0.00460280
Iteration 643, loss = 0.00459443
Iteration 644, loss = 0.00457737
Iteration 645, loss = 0.00457253
Iteration 646, loss = 0.00457414
Iteration 647, loss = 0.00455017
Iteration 648, loss = 0.00452508
Iteration 649, loss = 0.00451890
Iteration 650, loss = 0.00454127
Iteration 651, loss = 0.00448877
Iteration 652, loss = 0.00446888
Iteration 653, loss = 0.00447606
Iteration 654, loss = 0.00447385
Iteration 655, loss = 0.00448130
Iteration 656, loss = 0.00447492
Iteration 657, loss = 0.00446723
Iteration 658, loss = 0.00445710
Iteration 659, loss = 0.00444879
Iteration 660, loss = 0.00443049
Iteration 661, loss = 0.00439711
Iteration 662, loss = 0.00437818
Iteration 663, loss = 0.00438143
Iteration 664, loss = 0.00437442
Iteration 665, loss = 0.00437586
Iteration 666, loss = 0.00437115
Iteration 667, loss = 0.00436418
Iteration 668, loss = 0.00435682
Iteration 669, loss = 0.00433967
Iteration 670, loss = 0.00431895
Iteration 671, loss = 0.00429642
Iteration 672, loss = 0.00428385
Iteration 673, loss = 0.00427554
Iteration 674, loss = 0.00427561
Iteration 675, loss = 0.00429028
Iteration 676, loss = 0.00426691
Iteration 677, loss = 0.00425942
Iteration 678, loss = 0.00425431
Iteration 679, loss = 0.00424815
Iteration 680, loss = 0.00425117
Iteration 681, loss = 0.00427567
Iteration 682, loss = 0.00421750
Iteration 683, loss = 0.00421423
Iteration 684, loss = 0.00419112
Iteration 685, loss = 0.00419570
Iteration 686, loss = 0.00415511
Iteration 687, loss = 0.00414073
Iteration 688, loss = 0.00415633
Iteration 689, loss = 0.00412599
Iteration 690, loss = 0.00411544
Iteration 691, loss = 0.00410549
Iteration 692, loss = 0.00410131
Iteration 693, loss = 0.00410662
Iteration 694, loss = 0.00409494
Iteration 695, loss = 0.00408845
Iteration 696, loss = 0.00406987
Iteration 697, loss = 0.00404690
Iteration 698, loss = 0.00404140
Iteration 699, loss = 0.00403781
Iteration 700, loss = 0.00402768
PARAMETROS DE INICIALIZACAO DA REDE
NUMERO DE NEURONIOS 
Camada de Entrada: 34
Camada Escondida: 10
Camada de Saida: 1

--- PARAMETROS DE CONFIGURACAO DA REDE ---
Numero de Epocas: 700
Taxa de Aprendizado: adaptive
Taxa de Aprendizado Inicial: 0.3
METRICAS

RESULTADOS:

[0 0 1 0 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1 1 1 1 1 1 1
 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 1 1 1
 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 1 0]
ACURACIA: 0.9339622641509434

