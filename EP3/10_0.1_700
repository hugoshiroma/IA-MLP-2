Pesos Camada de Entrada: 
[[-0.05935868 -0.10551924 -0.20959576 -0.0284667  -0.06547916 -0.06721078
  -0.01822688 -0.04527045  0.03405903  0.18991902]
 [ 0.13946803  0.00756287  0.19377044  0.03727832  0.10872631 -0.11168399
  -0.00742111  0.17045519 -0.17051061  0.01020194]
 [ 0.1319411   0.0353247  -0.18093903 -0.08271741 -0.1397298   0.16799135
   0.21125195 -0.0965765  -0.15106409 -0.01596596]
 [-0.0419533   0.11918865 -0.12591791  0.04638708 -0.19992979  0.0094076
   0.07260477  0.15403837  0.04802529 -0.09875086]
 [ 0.17752304 -0.21235291 -0.16644878  0.08690663  0.02642456 -0.09229977
  -0.02286803  0.10355227  0.09876948 -0.05491928]
 [-0.12925905  0.05370663  0.18368444 -0.07803305  0.15986483 -0.10496119
  -0.08438721 -0.20617804  0.02198047  0.01193596]
 [ 0.1589491  -0.06570536  0.03292156 -0.01330589 -0.08205679  0.1451396
   0.20702972  0.02691607 -0.17955054  0.13143352]
 [ 0.00174745  0.08924554  0.00081405  0.03321363 -0.09081435 -0.08919382
  -0.2027314   0.00353582  0.02940065 -0.04701457]
 [ 0.06423744  0.09882916  0.14277646  0.19536517 -0.11252397  0.01297996
  -0.13681563 -0.04275466 -0.20454402  0.03408658]
 [ 0.11435272  0.11929722 -0.06468687 -0.02416437  0.06607512  0.18779985
  -0.13231364  0.00810609  0.09115143 -0.12229183]
 [ 0.13710581  0.13870098 -0.02397167 -0.0264097   0.09964197  0.0554132
  -0.03618502  0.03513793  0.15750657  0.20418037]
 [ 0.09442756  0.01522491 -0.11388628  0.07970451  0.16365811  0.05083595
   0.0666993  -0.05944804  0.13460495 -0.0456324 ]
 [ 0.10836305 -0.02430883  0.17268281  0.01176813  0.19622196  0.05279295
  -0.0942525  -0.04198611  0.14684535  0.1250059 ]
 [ 0.11980523  0.01284552  0.2056755  -0.10888587 -0.13864748 -0.21225143
  -0.14745512  0.14587553  0.18877774 -0.15117513]
 [-0.02693438 -0.13181173 -0.15936489 -0.17524263  0.10162583  0.04242336
   0.11903061 -0.19103231  0.05126727  0.03264424]
 [-0.07753448  0.03868217  0.15851054 -0.12613186 -0.0511899  -0.09921413
  -0.17035167  0.17826938 -0.01373491  0.10341706]
 [-0.19494915 -0.11365241 -0.04300482  0.12184894 -0.01144923  0.11561334
  -0.20277112  0.10203288 -0.18078718 -0.20955918]
 [-0.00144468 -0.03610573  0.00393961 -0.13663482  0.09660237 -0.06354989
  -0.06326678  0.13782373  0.13530193  0.03199686]
 [-0.02970112  0.00769495 -0.11889684  0.07758712 -0.11170459  0.1174348
  -0.05225351  0.10604914  0.14633427 -0.13301663]
 [ 0.05863144 -0.17768575 -0.00084278  0.07682049  0.08797394 -0.15918694
  -0.17008434  0.15176618 -0.16178206  0.05377743]
 [-0.02290564 -0.18102762  0.12689413  0.06354636 -0.10013521  0.1924236
   0.18354874 -0.06143804 -0.11505403 -0.00176457]
 [-0.02710163 -0.1358017   0.07295587 -0.16590387  0.10394168 -0.14341389
  -0.0378916  -0.09054435  0.01205086 -0.10787472]
 [-0.02148638 -0.00323974 -0.08146458 -0.10870821  0.17185905 -0.05426235
  -0.16475108  0.03710237  0.19823914  0.06993092]
 [-0.08455158 -0.00310267 -0.01204277  0.13401349 -0.00077595 -0.11233377
   0.1003161  -0.02397488  0.02853745 -0.08198522]
 [ 0.19929111 -0.04468494  0.0601132  -0.09377331 -0.11301848 -0.20237109
  -0.07151768  0.004009   -0.06126039 -0.13325031]
 [ 0.11686698 -0.01526921  0.13931696  0.15503975  0.02851403  0.01188658
  -0.17510843 -0.12356324  0.20160618 -0.17148315]
 [-0.11727912 -0.03667743 -0.20093575 -0.12962195  0.10413405  0.13035446
  -0.08853196 -0.05198681 -0.19137308 -0.07489748]
 [-0.02728305  0.20974647 -0.12641178  0.1969954   0.07323116 -0.11605529
  -0.17589421  0.14326593 -0.03086399 -0.20449731]
 [-0.11158416 -0.02707811  0.03985612  0.00851428 -0.09768828 -0.10943054
   0.06455677  0.18106572 -0.04747555 -0.0624299 ]
 [-0.18470042  0.08559822  0.1248439   0.08062333  0.01058263 -0.10839923
   0.17491398 -0.17146943 -0.18745918 -0.05184123]
 [ 0.12567035 -0.02696224  0.15043411  0.14954129 -0.06864274  0.05974258
  -0.05136067  0.01498195 -0.07750862 -0.1282121 ]
 [ 0.04515115  0.14586595  0.06081205 -0.05205287  0.08081282 -0.08724407
  -0.13761076 -0.13478496  0.11474778  0.07266921]
 [ 0.17095809 -0.19947791  0.06375474 -0.20472543 -0.08601546 -0.13958595
  -0.18945857  0.03891279 -0.12268394  0.12490795]
 [ 0.20032298  0.14753637  0.03252818 -0.11516597  0.0544404   0.04405057
   0.13941451 -0.20493595 -0.04370768 -0.02991662]]
Bias Camada de Entrada: 
[-0.10160184 -0.02209665  0.05915178  0.1644656   0.02511154 -0.10425934
  0.20591208  0.11358437  0.18170361  0.16614989]
Pesos Camada Escondida: 
[[ 0.18996875]
 [ 0.02040292]
 [ 0.34459931]
 [ 0.33692553]
 [ 0.26944795]
 [-0.07669549]
 [-0.40778802]
 [-0.09177302]
 [-0.21458539]
 [-0.39242651]]
Bias Camada Escondida: 
[0.06194168]
Iteration 1, loss = 0.69523253
Iteration 2, loss = 0.66629400
Iteration 3, loss = 0.64708384
Iteration 4, loss = 0.64186688
Iteration 5, loss = 0.64200027
Iteration 6, loss = 0.64425331
Iteration 7, loss = 0.63665643
Iteration 8, loss = 0.62108526
Iteration 9, loss = 0.60926488
Iteration 10, loss = 0.60150228
Iteration 11, loss = 0.59469610
Iteration 12, loss = 0.58432974
Iteration 13, loss = 0.57401866
Iteration 14, loss = 0.56296969
Iteration 15, loss = 0.55175475
Iteration 16, loss = 0.54017933
Iteration 17, loss = 0.52920012
Iteration 18, loss = 0.51816511
Iteration 19, loss = 0.50657042
Iteration 20, loss = 0.49365140
Iteration 21, loss = 0.48262576
Iteration 22, loss = 0.47025718
Iteration 23, loss = 0.45928561
Iteration 24, loss = 0.44842127
Iteration 25, loss = 0.43873844
Iteration 26, loss = 0.42797136
Iteration 27, loss = 0.41853567
Iteration 28, loss = 0.41005876
Iteration 29, loss = 0.40315680
Iteration 30, loss = 0.39501567
Iteration 31, loss = 0.38534835
Iteration 32, loss = 0.37850123
Iteration 33, loss = 0.37231142
Iteration 34, loss = 0.36542137
Iteration 35, loss = 0.35935707
Iteration 36, loss = 0.35381737
Iteration 37, loss = 0.34891449
Iteration 38, loss = 0.34442638
Iteration 39, loss = 0.34008135
Iteration 40, loss = 0.33591344
Iteration 41, loss = 0.33232275
Iteration 42, loss = 0.32850493
Iteration 43, loss = 0.32420214
Iteration 44, loss = 0.32064293
Iteration 45, loss = 0.31884060
Iteration 46, loss = 0.31465087
Iteration 47, loss = 0.31081919
Iteration 48, loss = 0.30701889
Iteration 49, loss = 0.30435387
Iteration 50, loss = 0.30133141
Iteration 51, loss = 0.29834483
Iteration 52, loss = 0.29547694
Iteration 53, loss = 0.29098374
Iteration 54, loss = 0.28925745
Iteration 55, loss = 0.28798001
Iteration 56, loss = 0.28734431
Iteration 57, loss = 0.28135826
Iteration 58, loss = 0.27953248
Iteration 59, loss = 0.27628925
Iteration 60, loss = 0.27360887
Iteration 61, loss = 0.27197213
Iteration 62, loss = 0.26977784
Iteration 63, loss = 0.26766269
Iteration 64, loss = 0.26559632
Iteration 65, loss = 0.26351348
Iteration 66, loss = 0.26146386
Iteration 67, loss = 0.25936676
Iteration 68, loss = 0.25693535
Iteration 69, loss = 0.25516509
Iteration 70, loss = 0.25317704
Iteration 71, loss = 0.25086577
Iteration 72, loss = 0.24888273
Iteration 73, loss = 0.24663564
Iteration 74, loss = 0.24468253
Iteration 75, loss = 0.24299749
Iteration 76, loss = 0.24126227
Iteration 77, loss = 0.24054846
Iteration 78, loss = 0.23728814
Iteration 79, loss = 0.23522418
Iteration 80, loss = 0.23392768
Iteration 81, loss = 0.23231137
Iteration 82, loss = 0.23049272
Iteration 83, loss = 0.22754043
Iteration 84, loss = 0.22654359
Iteration 85, loss = 0.22406904
Iteration 86, loss = 0.22215719
Iteration 87, loss = 0.22025249
Iteration 88, loss = 0.21897491
Iteration 89, loss = 0.21687049
Iteration 90, loss = 0.21512369
Iteration 91, loss = 0.21331886
Iteration 92, loss = 0.21172374
Iteration 93, loss = 0.21053797
Iteration 94, loss = 0.20848334
Iteration 95, loss = 0.20676586
Iteration 96, loss = 0.20523982
Iteration 97, loss = 0.20414753
Iteration 98, loss = 0.20196318
Iteration 99, loss = 0.20073691
Iteration 100, loss = 0.19918724
Iteration 101, loss = 0.19757617
Iteration 102, loss = 0.19570613
Iteration 103, loss = 0.19388003
Iteration 104, loss = 0.19272176
Iteration 105, loss = 0.19094878
Iteration 106, loss = 0.18915770
Iteration 107, loss = 0.18798491
Iteration 108, loss = 0.18635157
Iteration 109, loss = 0.18478323
Iteration 110, loss = 0.18350046
Iteration 111, loss = 0.18248324
Iteration 112, loss = 0.18161690
Iteration 113, loss = 0.17965715
Iteration 114, loss = 0.17736138
Iteration 115, loss = 0.17620735
Iteration 116, loss = 0.17511239
Iteration 117, loss = 0.17420229
Iteration 118, loss = 0.17339297
Iteration 119, loss = 0.17242655
Iteration 120, loss = 0.17076868
Iteration 121, loss = 0.16903019
Iteration 122, loss = 0.16789160
Iteration 123, loss = 0.16621228
Iteration 124, loss = 0.16526603
Iteration 125, loss = 0.16448928
Iteration 126, loss = 0.16354503
Iteration 127, loss = 0.16266300
Iteration 128, loss = 0.16163859
Iteration 129, loss = 0.16047807
Iteration 130, loss = 0.15944373
Iteration 131, loss = 0.15863858
Iteration 132, loss = 0.15811099
Iteration 133, loss = 0.15818517
Iteration 134, loss = 0.15713034
Iteration 135, loss = 0.15548890
Iteration 136, loss = 0.15396845
Iteration 137, loss = 0.15257235
Iteration 138, loss = 0.15200953
Iteration 139, loss = 0.15083432
Iteration 140, loss = 0.14966056
Iteration 141, loss = 0.14913527
Iteration 142, loss = 0.14834170
Iteration 143, loss = 0.14745874
Iteration 144, loss = 0.14640838
Iteration 145, loss = 0.14520202
Iteration 146, loss = 0.14434773
Iteration 147, loss = 0.14461954
Iteration 148, loss = 0.14462408
Iteration 149, loss = 0.14346922
Iteration 150, loss = 0.14191532
Iteration 151, loss = 0.14082662
Iteration 152, loss = 0.14002795
Iteration 153, loss = 0.13920708
Iteration 154, loss = 0.13841641
Iteration 155, loss = 0.13773691
Iteration 156, loss = 0.13703909
Iteration 157, loss = 0.13627225
Iteration 158, loss = 0.13569187
Iteration 159, loss = 0.13481617
Iteration 160, loss = 0.13412145
Iteration 161, loss = 0.13338926
Iteration 162, loss = 0.13278467
Iteration 163, loss = 0.13268438
Iteration 164, loss = 0.13172426
Iteration 165, loss = 0.13093860
Iteration 166, loss = 0.13037748
Iteration 167, loss = 0.12946608
Iteration 168, loss = 0.12878052
Iteration 169, loss = 0.12824354
Iteration 170, loss = 0.12805210
Iteration 171, loss = 0.12750072
Iteration 172, loss = 0.12662288
Iteration 173, loss = 0.12623418
Iteration 174, loss = 0.12537195
Iteration 175, loss = 0.12441775
Iteration 176, loss = 0.12401457
Iteration 177, loss = 0.12352791
Iteration 178, loss = 0.12281740
Iteration 179, loss = 0.12226382
Iteration 180, loss = 0.12164367
Iteration 181, loss = 0.12116636
Iteration 182, loss = 0.12084272
Iteration 183, loss = 0.12016416
Iteration 184, loss = 0.11932210
Iteration 185, loss = 0.11857993
Iteration 186, loss = 0.11829597
Iteration 187, loss = 0.11849952
Iteration 188, loss = 0.11840893
Iteration 189, loss = 0.11810084
Iteration 190, loss = 0.11751890
Iteration 191, loss = 0.11672924
Iteration 192, loss = 0.11607728
Iteration 193, loss = 0.11543788
Iteration 194, loss = 0.11491377
Iteration 195, loss = 0.11430713
Iteration 196, loss = 0.11371685
Iteration 197, loss = 0.11313726
Iteration 198, loss = 0.11267475
Iteration 199, loss = 0.11214203
Iteration 200, loss = 0.11160432
Iteration 201, loss = 0.11105000
Iteration 202, loss = 0.11057602
Iteration 203, loss = 0.11016935
Iteration 204, loss = 0.10987854
Iteration 205, loss = 0.10952676
Iteration 206, loss = 0.10915431
Iteration 207, loss = 0.10877110
Iteration 208, loss = 0.10825613
Iteration 209, loss = 0.10760243
Iteration 210, loss = 0.10751167
Iteration 211, loss = 0.10680044
Iteration 212, loss = 0.10647451
Iteration 213, loss = 0.10588497
Iteration 214, loss = 0.10565798
Iteration 215, loss = 0.10586035
Iteration 216, loss = 0.10497526
Iteration 217, loss = 0.10434784
Iteration 218, loss = 0.10380204
Iteration 219, loss = 0.10366576
Iteration 220, loss = 0.10308415
Iteration 221, loss = 0.10292581
Iteration 222, loss = 0.10239703
Iteration 223, loss = 0.10185145
Iteration 224, loss = 0.10138465
Iteration 225, loss = 0.10102439
Iteration 226, loss = 0.10070341
Iteration 227, loss = 0.10123031
Iteration 228, loss = 0.09997654
Iteration 229, loss = 0.09928683
Iteration 230, loss = 0.09915788
Iteration 231, loss = 0.10023505
Iteration 232, loss = 0.10024372
Iteration 233, loss = 0.09929471
Iteration 234, loss = 0.09789065
Iteration 235, loss = 0.09698416
Iteration 236, loss = 0.09656099
Iteration 237, loss = 0.09664588
Iteration 238, loss = 0.09622799
Iteration 239, loss = 0.09595277
Iteration 240, loss = 0.09533309
Iteration 241, loss = 0.09475062
Iteration 242, loss = 0.09420647
Iteration 243, loss = 0.09402460
Iteration 244, loss = 0.09376978
Iteration 245, loss = 0.09351884
Iteration 246, loss = 0.09320085
Iteration 247, loss = 0.09310105
Iteration 248, loss = 0.09302608
Iteration 249, loss = 0.09296519
Iteration 250, loss = 0.09209587
Iteration 251, loss = 0.09117490
Iteration 252, loss = 0.09050677
Iteration 253, loss = 0.09112247
Iteration 254, loss = 0.09056565
Iteration 255, loss = 0.09039056
Iteration 256, loss = 0.09001381
Iteration 257, loss = 0.08964562
Iteration 258, loss = 0.08923534
Iteration 259, loss = 0.08863850
Iteration 260, loss = 0.08803490
Iteration 261, loss = 0.08761765
Iteration 262, loss = 0.08719760
Iteration 263, loss = 0.08689098
Iteration 264, loss = 0.08664736
Iteration 265, loss = 0.08650669
Iteration 266, loss = 0.08632122
Iteration 267, loss = 0.08582616
Iteration 268, loss = 0.08531888
Iteration 269, loss = 0.08508618
Iteration 270, loss = 0.08485912
Iteration 271, loss = 0.08487977
Iteration 272, loss = 0.08494437
Iteration 273, loss = 0.08507092
Iteration 274, loss = 0.08465177
Iteration 275, loss = 0.08414466
Iteration 276, loss = 0.08367946
Iteration 277, loss = 0.08294871
Iteration 278, loss = 0.08263788
Iteration 279, loss = 0.08207174
Iteration 280, loss = 0.08171428
Iteration 281, loss = 0.08147132
Iteration 282, loss = 0.08118571
Iteration 283, loss = 0.08090930
Iteration 284, loss = 0.08067904
Iteration 285, loss = 0.08040132
Iteration 286, loss = 0.08024116
Iteration 287, loss = 0.08022622
Iteration 288, loss = 0.07975587
Iteration 289, loss = 0.07914503
Iteration 290, loss = 0.07861642
Iteration 291, loss = 0.07890688
Iteration 292, loss = 0.07887036
Iteration 293, loss = 0.07871537
Iteration 294, loss = 0.07820623
Iteration 295, loss = 0.07747511
Iteration 296, loss = 0.07683480
Iteration 297, loss = 0.07675250
Iteration 298, loss = 0.07694688
Iteration 299, loss = 0.07694428
Iteration 300, loss = 0.07653580
Iteration 301, loss = 0.07605647
Iteration 302, loss = 0.07567221
Iteration 303, loss = 0.07530886
Iteration 304, loss = 0.07480494
Iteration 305, loss = 0.07519198
Iteration 306, loss = 0.07475179
Iteration 307, loss = 0.07467997
Iteration 308, loss = 0.07401597
Iteration 309, loss = 0.07372086
Iteration 310, loss = 0.07402337
Iteration 311, loss = 0.07390227
Iteration 312, loss = 0.07347505
Iteration 313, loss = 0.07297877
Iteration 314, loss = 0.07243107
Iteration 315, loss = 0.07176873
Iteration 316, loss = 0.07171236
Iteration 317, loss = 0.07168266
Iteration 318, loss = 0.07158776
Iteration 319, loss = 0.07143249
Iteration 320, loss = 0.07082592
Iteration 321, loss = 0.07042970
Iteration 322, loss = 0.06995592
Iteration 323, loss = 0.06960820
Iteration 324, loss = 0.06991180
Iteration 325, loss = 0.06938352
Iteration 326, loss = 0.06908254
Iteration 327, loss = 0.06875303
Iteration 328, loss = 0.06852225
Iteration 329, loss = 0.06869450
Iteration 330, loss = 0.06842583
Iteration 331, loss = 0.06783367
Iteration 332, loss = 0.06741774
Iteration 333, loss = 0.06750628
Iteration 334, loss = 0.06744314
Iteration 335, loss = 0.06759282
Iteration 336, loss = 0.06726401
Iteration 337, loss = 0.06687080
Iteration 338, loss = 0.06653760
Iteration 339, loss = 0.06630319
Iteration 340, loss = 0.06587020
Iteration 341, loss = 0.06590721
Iteration 342, loss = 0.06598624
Iteration 343, loss = 0.06613406
Iteration 344, loss = 0.06631272
Iteration 345, loss = 0.06669909
Iteration 346, loss = 0.06660007
Iteration 347, loss = 0.06579313
Iteration 348, loss = 0.06481667
Iteration 349, loss = 0.06398753
Iteration 350, loss = 0.06357798
Iteration 351, loss = 0.06325192
Iteration 352, loss = 0.06366064
Iteration 353, loss = 0.06404012
Iteration 354, loss = 0.06436026
Iteration 355, loss = 0.06458047
Iteration 356, loss = 0.06354044
Iteration 357, loss = 0.06271953
Iteration 358, loss = 0.06203636
Iteration 359, loss = 0.06185626
Iteration 360, loss = 0.06154077
Iteration 361, loss = 0.06137789
Iteration 362, loss = 0.06154960
Iteration 363, loss = 0.06112788
Iteration 364, loss = 0.06071770
Iteration 365, loss = 0.06035302
Iteration 366, loss = 0.06045272
Iteration 367, loss = 0.06005142
Iteration 368, loss = 0.05987039
Iteration 369, loss = 0.05962811
Iteration 370, loss = 0.05936957
Iteration 371, loss = 0.05915801
Iteration 372, loss = 0.05901468
Iteration 373, loss = 0.05893712
Iteration 374, loss = 0.05895048
Iteration 375, loss = 0.05915631
Iteration 376, loss = 0.05871708
Iteration 377, loss = 0.05839721
Iteration 378, loss = 0.05808330
Iteration 379, loss = 0.05783820
Iteration 380, loss = 0.05764076
Iteration 381, loss = 0.05745990
Iteration 382, loss = 0.05729176
Iteration 383, loss = 0.05728452
Iteration 384, loss = 0.05737506
Iteration 385, loss = 0.05735865
Iteration 386, loss = 0.05719953
Iteration 387, loss = 0.05703375
Iteration 388, loss = 0.05694903
Iteration 389, loss = 0.05703728
Iteration 390, loss = 0.05754073
Iteration 391, loss = 0.05721443
Iteration 392, loss = 0.05672250
Iteration 393, loss = 0.05547861
Iteration 394, loss = 0.05508854
Iteration 395, loss = 0.05516497
Iteration 396, loss = 0.05621345
Iteration 397, loss = 0.05631632
Iteration 398, loss = 0.05598595
Iteration 399, loss = 0.05499714
Iteration 400, loss = 0.05415378
Iteration 401, loss = 0.05385698
Iteration 402, loss = 0.05348202
Iteration 403, loss = 0.05314359
Iteration 404, loss = 0.05298128
Iteration 405, loss = 0.05295729
Iteration 406, loss = 0.05291697
Iteration 407, loss = 0.05249167
Iteration 408, loss = 0.05232805
Iteration 409, loss = 0.05279734
Iteration 410, loss = 0.05382102
Iteration 411, loss = 0.05382559
Iteration 412, loss = 0.05336620
Iteration 413, loss = 0.05251233
Iteration 414, loss = 0.05204836
Iteration 415, loss = 0.05161699
Iteration 416, loss = 0.05120701
Iteration 417, loss = 0.05099053
Iteration 418, loss = 0.05057096
Iteration 419, loss = 0.05039076
Iteration 420, loss = 0.05005500
Iteration 421, loss = 0.04986750
Iteration 422, loss = 0.04970062
Iteration 423, loss = 0.04961098
Iteration 424, loss = 0.04959240
Iteration 425, loss = 0.04951534
Iteration 426, loss = 0.04944655
Iteration 427, loss = 0.04939573
Iteration 428, loss = 0.04939043
Iteration 429, loss = 0.04909448
Iteration 430, loss = 0.04877100
Iteration 431, loss = 0.04854748
Iteration 432, loss = 0.04831338
Iteration 433, loss = 0.04816367
Iteration 434, loss = 0.04798299
Iteration 435, loss = 0.04784390
Iteration 436, loss = 0.04774049
Iteration 437, loss = 0.04762213
Iteration 438, loss = 0.04756651
Iteration 439, loss = 0.04739006
Iteration 440, loss = 0.04719308
Iteration 441, loss = 0.04690834
Iteration 442, loss = 0.04672417
Iteration 443, loss = 0.04660667
Iteration 444, loss = 0.04644760
Iteration 445, loss = 0.04628289
Iteration 446, loss = 0.04636423
Iteration 447, loss = 0.04621810
Iteration 448, loss = 0.04591105
Iteration 449, loss = 0.04569371
Iteration 450, loss = 0.04571628
Iteration 451, loss = 0.04605862
Iteration 452, loss = 0.04680949
Iteration 453, loss = 0.04686349
Iteration 454, loss = 0.04598401
Iteration 455, loss = 0.04518709
Iteration 456, loss = 0.04496423
Iteration 457, loss = 0.04493845
Iteration 458, loss = 0.04527678
Iteration 459, loss = 0.04539404
Iteration 460, loss = 0.04522235
Iteration 461, loss = 0.04474792
Iteration 462, loss = 0.04418663
Iteration 463, loss = 0.04396745
Iteration 464, loss = 0.04369744
Iteration 465, loss = 0.04355464
Iteration 466, loss = 0.04366596
Iteration 467, loss = 0.04377890
Iteration 468, loss = 0.04395611
Iteration 469, loss = 0.04410338
Iteration 470, loss = 0.04395761
Iteration 471, loss = 0.04349521
Iteration 472, loss = 0.04310664
Iteration 473, loss = 0.04287908
Iteration 474, loss = 0.04274172
Iteration 475, loss = 0.04241528
Iteration 476, loss = 0.04246615
Iteration 477, loss = 0.04268915
Iteration 478, loss = 0.04281903
Iteration 479, loss = 0.04243345
Iteration 480, loss = 0.04206643
Iteration 481, loss = 0.04189742
Iteration 482, loss = 0.04172246
Iteration 483, loss = 0.04170931
Iteration 484, loss = 0.04178833
Iteration 485, loss = 0.04143889
Iteration 486, loss = 0.04142265
Iteration 487, loss = 0.04129997
Iteration 488, loss = 0.04112136
Iteration 489, loss = 0.04083094
Iteration 490, loss = 0.04059747
Iteration 491, loss = 0.04060839
Iteration 492, loss = 0.04041970
Iteration 493, loss = 0.04033645
Iteration 494, loss = 0.04029314
Iteration 495, loss = 0.04019145
Iteration 496, loss = 0.04009281
Iteration 497, loss = 0.03999527
Iteration 498, loss = 0.03996802
Iteration 499, loss = 0.03964157
Iteration 500, loss = 0.03949643
Iteration 501, loss = 0.03942045
Iteration 502, loss = 0.03917474
Iteration 503, loss = 0.03908606
Iteration 504, loss = 0.03893285
Iteration 505, loss = 0.03881547
Iteration 506, loss = 0.03872139
Iteration 507, loss = 0.03865592
Iteration 508, loss = 0.03864382
Iteration 509, loss = 0.03858439
Iteration 510, loss = 0.03850187
Iteration 511, loss = 0.03859578
Iteration 512, loss = 0.03818584
Iteration 513, loss = 0.03807738
Iteration 514, loss = 0.03822930
Iteration 515, loss = 0.03826284
Iteration 516, loss = 0.03863866
Iteration 517, loss = 0.03788477
Iteration 518, loss = 0.03737798
Iteration 519, loss = 0.03714686
Iteration 520, loss = 0.03712985
Iteration 521, loss = 0.03699815
Iteration 522, loss = 0.03695262
Iteration 523, loss = 0.03669857
Iteration 524, loss = 0.03665146
Iteration 525, loss = 0.03648021
Iteration 526, loss = 0.03637485
Iteration 527, loss = 0.03625616
Iteration 528, loss = 0.03614695
Iteration 529, loss = 0.03619306
Iteration 530, loss = 0.03607108
Iteration 531, loss = 0.03587417
Iteration 532, loss = 0.03579821
Iteration 533, loss = 0.03601216
Iteration 534, loss = 0.03616476
Iteration 535, loss = 0.03635843
Iteration 536, loss = 0.03627125
Iteration 537, loss = 0.03598769
Iteration 538, loss = 0.03559346
Iteration 539, loss = 0.03528537
Iteration 540, loss = 0.03521143
Iteration 541, loss = 0.03505429
Iteration 542, loss = 0.03499984
Iteration 543, loss = 0.03479956
Iteration 544, loss = 0.03453144
Iteration 545, loss = 0.03453519
Iteration 546, loss = 0.03429265
Iteration 547, loss = 0.03419462
Iteration 548, loss = 0.03436943
Iteration 549, loss = 0.03398491
Iteration 550, loss = 0.03378369
Iteration 551, loss = 0.03369082
Iteration 552, loss = 0.03356923
Iteration 553, loss = 0.03347657
Iteration 554, loss = 0.03337542
Iteration 555, loss = 0.03323205
Iteration 556, loss = 0.03313221
Iteration 557, loss = 0.03306388
Iteration 558, loss = 0.03302015
Iteration 559, loss = 0.03302113
Iteration 560, loss = 0.03310421
Iteration 561, loss = 0.03284853
Iteration 562, loss = 0.03271030
Iteration 563, loss = 0.03262438
Iteration 564, loss = 0.03249059
Iteration 565, loss = 0.03238781
Iteration 566, loss = 0.03224215
Iteration 567, loss = 0.03213292
Iteration 568, loss = 0.03201885
Iteration 569, loss = 0.03191107
Iteration 570, loss = 0.03198775
Iteration 571, loss = 0.03176400
Iteration 572, loss = 0.03169045
Iteration 573, loss = 0.03159662
Iteration 574, loss = 0.03160029
Iteration 575, loss = 0.03154927
Iteration 576, loss = 0.03144155
Iteration 577, loss = 0.03137021
Iteration 578, loss = 0.03126691
Iteration 579, loss = 0.03133281
Iteration 580, loss = 0.03109173
Iteration 581, loss = 0.03108953
Iteration 582, loss = 0.03125079
Iteration 583, loss = 0.03109933
Iteration 584, loss = 0.03102480
Iteration 585, loss = 0.03081044
Iteration 586, loss = 0.03089489
Iteration 587, loss = 0.03059308
Iteration 588, loss = 0.03054874
Iteration 589, loss = 0.03027102
Iteration 590, loss = 0.03020165
Iteration 591, loss = 0.03012587
Iteration 592, loss = 0.03001897
Iteration 593, loss = 0.02995847
Iteration 594, loss = 0.02991828
Iteration 595, loss = 0.02992778
Iteration 596, loss = 0.02986086
Iteration 597, loss = 0.02968322
Iteration 598, loss = 0.02954020
Iteration 599, loss = 0.02946824
Iteration 600, loss = 0.02937619
Iteration 601, loss = 0.02929076
Iteration 602, loss = 0.02921221
Iteration 603, loss = 0.02927199
Iteration 604, loss = 0.02919349
Iteration 605, loss = 0.02927814
Iteration 606, loss = 0.02919255
Iteration 607, loss = 0.02902649
Iteration 608, loss = 0.02880251
Iteration 609, loss = 0.02880397
Iteration 610, loss = 0.02858562
Iteration 611, loss = 0.02857098
Iteration 612, loss = 0.02857102
Iteration 613, loss = 0.02854145
Iteration 614, loss = 0.02836513
Iteration 615, loss = 0.02821674
Iteration 616, loss = 0.02815120
Iteration 617, loss = 0.02812892
Iteration 618, loss = 0.02796737
Iteration 619, loss = 0.02786314
Iteration 620, loss = 0.02776421
Iteration 621, loss = 0.02777158
Iteration 622, loss = 0.02766805
Iteration 623, loss = 0.02766380
Iteration 624, loss = 0.02770233
Iteration 625, loss = 0.02770265
Iteration 626, loss = 0.02777293
Iteration 627, loss = 0.02784618
Iteration 628, loss = 0.02771301
Iteration 629, loss = 0.02747132
Iteration 630, loss = 0.02717122
Iteration 631, loss = 0.02711956
Iteration 632, loss = 0.02688080
Iteration 633, loss = 0.02698131
Iteration 634, loss = 0.02697489
Iteration 635, loss = 0.02691793
Iteration 636, loss = 0.02680770
Iteration 637, loss = 0.02667529
Iteration 638, loss = 0.02660548
Iteration 639, loss = 0.02646657
Iteration 640, loss = 0.02636584
Iteration 641, loss = 0.02631284
Iteration 642, loss = 0.02629739
Iteration 643, loss = 0.02612085
Iteration 644, loss = 0.02602073
Iteration 645, loss = 0.02614863
Iteration 646, loss = 0.02590508
Iteration 647, loss = 0.02579676
Iteration 648, loss = 0.02592564
Iteration 649, loss = 0.02578266
Iteration 650, loss = 0.02571375
Iteration 651, loss = 0.02565536
Iteration 652, loss = 0.02547106
Iteration 653, loss = 0.02556100
Iteration 654, loss = 0.02542512
Iteration 655, loss = 0.02538251
Iteration 656, loss = 0.02546455
Iteration 657, loss = 0.02535699
Iteration 658, loss = 0.02523066
Iteration 659, loss = 0.02511325
Iteration 660, loss = 0.02516865
Iteration 661, loss = 0.02501309
Iteration 662, loss = 0.02527633
Iteration 663, loss = 0.02538517
Iteration 664, loss = 0.02527847
Iteration 665, loss = 0.02517818
Iteration 666, loss = 0.02500548
Iteration 667, loss = 0.02478581
Iteration 668, loss = 0.02455755
Iteration 669, loss = 0.02467728
Iteration 670, loss = 0.02437152
Iteration 671, loss = 0.02430604
Iteration 672, loss = 0.02404826
Iteration 673, loss = 0.02408516
Iteration 674, loss = 0.02420084
Iteration 675, loss = 0.02402564
Iteration 676, loss = 0.02395705
Iteration 677, loss = 0.02391602
Iteration 678, loss = 0.02378961
Iteration 679, loss = 0.02375106
Iteration 680, loss = 0.02365097
Iteration 681, loss = 0.02353686
Iteration 682, loss = 0.02346411
Iteration 683, loss = 0.02342809
Iteration 684, loss = 0.02339075
Iteration 685, loss = 0.02330913
Iteration 686, loss = 0.02328965
Iteration 687, loss = 0.02335844
Iteration 688, loss = 0.02356042
Iteration 689, loss = 0.02352244
Iteration 690, loss = 0.02332454
Iteration 691, loss = 0.02313232
Iteration 692, loss = 0.02292841
Iteration 693, loss = 0.02284030
Iteration 694, loss = 0.02276177
Iteration 695, loss = 0.02277158
Iteration 696, loss = 0.02276028
Iteration 697, loss = 0.02278944
Iteration 698, loss = 0.02271352
Iteration 699, loss = 0.02261400
Iteration 700, loss = 0.02251852
PARAMETROS DE INICIALIZACAO DA REDE
NUMERO DE NEURONIOS 
Camada de Entrada: 34
Camada Escondida: 10
Camada de Saida: 1

--- PARAMETROS DE CONFIGURACAO DA REDE ---
Numero de Epocas: 700
Taxa de Aprendizado: adaptive
Taxa de Aprendizado Inicial: 0.1
METRICAS

RESULTADOS:

[0 0 1 0 1 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1 1 1 1 1 1 1
 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 1 1 1
 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 1 0]
ACURACIA: 0.9433962264150944

