Pesos Camada de Entrada: 
[[ 0.0153956   0.01590721  0.07448308  0.09320579 -0.100225   -0.05055888
   0.09173258  0.01683912  0.00858509 -0.11402306 -0.15176185  0.09433901
   0.14885399  0.0240844   0.2016493 ]
 [ 0.1813654   0.19745115 -0.08664311 -0.03137156  0.19818916 -0.06061615
  -0.06775025 -0.2003639  -0.06036059  0.19656851  0.08316882  0.11273921
   0.02693933  0.02122823 -0.10766419]
 [ 0.07394627  0.17981311 -0.02591505  0.06047294  0.09056218  0.03055986
  -0.06662087  0.11848211 -0.17968015  0.01716388  0.09109932 -0.10141279
  -0.03609467 -0.07970187 -0.15657954]
 [ 0.1217164   0.19738015 -0.13854469  0.06443202 -0.01582775  0.06577986
  -0.1254483   0.20048877 -0.04976577 -0.04288393  0.06955579 -0.18234367
   0.18323285 -0.12416548 -0.00462048]
 [-0.00341572 -0.04568239 -0.14972538  0.11873706  0.16856667  0.08646582
  -0.18175457  0.07231906  0.12442169  0.02717379  0.08765783 -0.14829999
   0.10244816 -0.20036061 -0.00170912]
 [ 0.02239683 -0.17835336  0.16972999 -0.07877844 -0.1708785  -0.06229111
  -0.18351087  0.06061222  0.13910309  0.01853566  0.07053263  0.05741439
   0.0468224  -0.05038561 -0.04834295]
 [-0.10310486 -0.10205568  0.17448868 -0.13707672 -0.1619207   0.02695996
   0.18969029 -0.04488469  0.00105487 -0.15018273  0.07293043  0.03306628
  -0.07810418  0.02477105 -0.10501894]
 [ 0.16455497  0.17353884 -0.00389107  0.15121176  0.12773786 -0.20178195
  -0.08604405  0.19958522  0.0347127   0.04627773 -0.11758229 -0.07508519
   0.18973127 -0.08242585 -0.02937944]
 [-0.07388787 -0.17922858 -0.1978324   0.04116858 -0.09913993 -0.18276426
  -0.11838516  0.0737656  -0.16620279  0.05889901 -0.15361558 -0.00493033
   0.04251088 -0.20110799  0.01622157]
 [ 0.18343848  0.03616043  0.10095866 -0.15476964  0.17456509  0.0365424
   0.18843004 -0.17913515 -0.18384347 -0.07138819  0.05685358  0.05113037
   0.1344383  -0.0750232   0.16222838]
 [ 0.1604618  -0.01333935  0.0526808  -0.10667497  0.14049867  0.13064897
  -0.17033965  0.10972926 -0.07966207  0.09007435  0.11155472  0.04085129
  -0.1771928  -0.17630511 -0.04037904]
 [ 0.01816678 -0.19043187 -0.17947195 -0.05665344 -0.10544106  0.11195362
  -0.00426678 -0.11131473  0.04124439 -0.12757764 -0.17719688  0.18960148
   0.02051253  0.18214842  0.08125573]
 [-0.1684034   0.00290515 -0.16566838 -0.19749094  0.14765686  0.05964607
  -0.13932903  0.06292773  0.01032157  0.08864412  0.09776986 -0.01437073
  -0.11832576 -0.19769087  0.03177771]
 [-0.18392521  0.09279681  0.042597   -0.14602441 -0.1695102  -0.18647865
  -0.0285572  -0.05886533 -0.1518908   0.16482742 -0.18919605 -0.12801706
   0.15102346 -0.19902187 -0.14799556]
 [ 0.03505647  0.00664326 -0.0890107   0.01475361 -0.13445128 -0.15924717
   0.07344271  0.15889125 -0.11296473 -0.05419486 -0.10139386 -0.20141369
  -0.16269257 -0.0015472   0.02536169]
 [ 0.19646354 -0.08047335 -0.09732691  0.08685659  0.00123546 -0.00130884
   0.1302992   0.07923181 -0.1440461   0.12874117  0.11289688 -0.05986566
   0.00820223 -0.00182031  0.04523321]
 [ 0.07569039 -0.01020487 -0.15625383 -0.13072258 -0.07012028  0.1335354
   0.06381061 -0.11286001 -0.0408098   0.03050155 -0.15859453  0.09492921
  -0.1567572   0.03935499  0.10566397]
 [-0.04974928 -0.15457442 -0.04336353 -0.08174472 -0.18433626  0.0167727
  -0.04542623 -0.0692768  -0.16370268  0.0452166  -0.15578426 -0.13554689
  -0.17345536 -0.09868771 -0.11366783]
 [-0.09886672 -0.02597394 -0.05485337  0.13639571  0.03898253 -0.01305309
  -0.08370103 -0.19414     0.02079492 -0.10661721 -0.11424912 -0.00120073
   0.16504391  0.00544279  0.17896259]
 [-0.10764526  0.16602013  0.14477923  0.07227205 -0.10609219 -0.02276826
  -0.12316016 -0.10274452 -0.0713183   0.09496933 -0.03431414 -0.10403864
   0.08730332 -0.03523152 -0.16908223]
 [ 0.15799986  0.06684639  0.19373345 -0.14443373  0.19163684  0.0838218
   0.10526244  0.19582727 -0.07845562  0.07783562  0.15029259  0.1117842
   0.07560707  0.08676414  0.16594549]
 [ 0.18454946 -0.15542916 -0.14443631  0.07033062 -0.05895286 -0.19678925
   0.12363316  0.10860426 -0.12836281  0.18226769 -0.04784641  0.07421494
   0.19199112  0.02036851  0.06076005]
 [-0.01844178 -0.05069037 -0.18271678  0.04460958  0.14880984  0.06243327
  -0.04463362  0.04835696  0.07252941  0.02552581 -0.09027085  0.16751751
  -0.09969142 -0.04659015  0.07972068]
 [-0.11362006  0.09904106 -0.15722176  0.18081012  0.13854378  0.02784432
  -0.09909695  0.14101982  0.14468454  0.15685289 -0.03908498  0.08519882
  -0.04185551  0.1972996  -0.07249833]
 [-0.12977391 -0.13268931 -0.13518353 -0.02043566  0.1932952   0.06636258
  -0.05386637  0.04856609 -0.07149155  0.18117294 -0.12034923  0.03165596
   0.04062868 -0.02618467 -0.08208556]
 [-0.05652454 -0.18823245  0.17735139  0.19616566  0.09733515 -0.0415572
  -0.06723123  0.02456216  0.02515617  0.08903087 -0.1512997   0.12068834
  -0.06156728  0.16764381 -0.18768173]
 [ 0.18990969  0.03639708 -0.0419974  -0.14930392  0.16731663  0.10745476
  -0.04295191  0.13776259  0.1926931   0.02201246  0.1376343  -0.11207962
   0.04050291 -0.14811175  0.12645618]
 [ 0.1449693  -0.11933217 -0.17726433  0.08200241 -0.11858268 -0.05723675
   0.11197674  0.16197101  0.13074095 -0.119253   -0.148525   -0.05638073
   0.19250042  0.13977728  0.05411971]
 [ 0.14214441 -0.15822839  0.05274783 -0.00856751 -0.08104966  0.16857246
   0.18358806  0.04170397 -0.05279224  0.18917392 -0.06110196  0.04944951
   0.13772637  0.18881144 -0.00924237]
 [ 0.187784   -0.16065651 -0.12423885  0.0899736   0.17889054  0.09075183
   0.03310883 -0.15168311  0.06712608  0.19415856  0.05411632 -0.18583123
   0.01884308  0.12998457 -0.09070805]
 [-0.18767942 -0.08195054 -0.1352665  -0.06222106  0.14810932  0.0129324
   0.15289857 -0.18057579 -0.14840209 -0.15994836 -0.0469567   0.08276689
   0.0872033   0.12059913  0.16771007]
 [ 0.07922676  0.11670651 -0.10630731 -0.15637165  0.14860536 -0.09709144
  -0.20144924 -0.0441041  -0.05127987  0.01720743 -0.03618916  0.16155779
  -0.15938452  0.09717667  0.05090646]
 [ 0.12759298 -0.12340443 -0.0077182   0.04554504  0.06507672 -0.00200505
  -0.1810919   0.02189113  0.14442177 -0.06352739  0.08030681  0.1412326
   0.11218528  0.17085642 -0.01436979]
 [ 0.00097139  0.17814382  0.1826079   0.08562984  0.18994687  0.19141175
  -0.06129259 -0.00192515  0.14280187  0.0099838  -0.03691044  0.18609089
   0.14449787 -0.14734143  0.02594811]]
Bias Camada de Entrada: 
[ 0.16016155 -0.0126741  -0.19925946 -0.12474368  0.05507869 -0.0789265
  0.05181863 -0.02966286 -0.14484166 -0.06800463  0.09019357  0.00045276
 -0.06488218 -0.19680688 -0.18077887]
Pesos Camada Escondida: 
[[ 0.20001574]
 [ 0.00310886]
 [ 0.08287444]
 [-0.13421477]
 [-0.31088542]
 [ 0.15435559]
 [-0.06704288]
 [ 0.16057209]
 [-0.26520807]
 [-0.20576489]
 [ 0.2538578 ]
 [-0.00035856]
 [-0.07777255]
 [-0.20986106]
 [ 0.09735929]]
Bias Camada Escondida: 
[-0.09724322]
Iteration 1, loss = 0.69129972
Iteration 2, loss = 0.71415096
Iteration 3, loss = 0.63278435
Iteration 4, loss = 0.61360379
Iteration 5, loss = 0.58941304
Iteration 6, loss = 0.54647181
Iteration 7, loss = 0.52209954
Iteration 8, loss = 0.47409423
Iteration 9, loss = 0.41189227
Iteration 10, loss = 0.37541983
Iteration 11, loss = 0.35416139
Iteration 12, loss = 0.35085900
Iteration 13, loss = 0.31584899
Iteration 14, loss = 0.30534166
Iteration 15, loss = 0.30115430
Iteration 16, loss = 0.27947058
Iteration 17, loss = 0.26828542
Iteration 18, loss = 0.26384981
Iteration 19, loss = 0.27458101
Iteration 20, loss = 0.24371785
Iteration 21, loss = 0.23715507
Iteration 22, loss = 0.23039540
Iteration 23, loss = 0.21933388
Iteration 24, loss = 0.21309938
Iteration 25, loss = 0.22693502
Iteration 26, loss = 0.19590627
Iteration 27, loss = 0.18534654
Iteration 28, loss = 0.17821228
Iteration 29, loss = 0.17236343
Iteration 30, loss = 0.16621606
Iteration 31, loss = 0.16103505
Iteration 32, loss = 0.15887296
Iteration 33, loss = 0.15180476
Iteration 34, loss = 0.14460725
Iteration 35, loss = 0.13830950
Iteration 36, loss = 0.13613613
Iteration 37, loss = 0.14185598
Iteration 38, loss = 0.12879345
Iteration 39, loss = 0.12679564
Iteration 40, loss = 0.12557990
Iteration 41, loss = 0.11899197
Iteration 42, loss = 0.11509421
Iteration 43, loss = 0.11263730
Iteration 44, loss = 0.11154814
Iteration 45, loss = 0.11061207
Iteration 46, loss = 0.10618024
Iteration 47, loss = 0.10409046
Iteration 48, loss = 0.10194755
Iteration 49, loss = 0.09925534
Iteration 50, loss = 0.09718830
Iteration 51, loss = 0.09532372
Iteration 52, loss = 0.09547318
Iteration 53, loss = 0.09303888
Iteration 54, loss = 0.09569938
Iteration 55, loss = 0.08842960
Iteration 56, loss = 0.08790581
Iteration 57, loss = 0.08467093
Iteration 58, loss = 0.08324569
Iteration 59, loss = 0.08235601
Iteration 60, loss = 0.07936215
Iteration 61, loss = 0.08044629
Iteration 62, loss = 0.08110277
Iteration 63, loss = 0.08166548
Iteration 64, loss = 0.07287748
Iteration 65, loss = 0.07289295
Iteration 66, loss = 0.07393700
Iteration 67, loss = 0.06887358
Iteration 68, loss = 0.06712592
Iteration 69, loss = 0.06908084
Iteration 70, loss = 0.06814822
Iteration 71, loss = 0.06339870
Iteration 72, loss = 0.06299068
Iteration 73, loss = 0.06232903
Iteration 74, loss = 0.06103594
Iteration 75, loss = 0.05889738
Iteration 76, loss = 0.05889972
Iteration 77, loss = 0.05745855
Iteration 78, loss = 0.05510689
Iteration 79, loss = 0.05381716
Iteration 80, loss = 0.05469929
Iteration 81, loss = 0.05365043
Iteration 82, loss = 0.05278676
Iteration 83, loss = 0.05423253
Iteration 84, loss = 0.04919162
Iteration 85, loss = 0.04731019
Iteration 86, loss = 0.04621658
Iteration 87, loss = 0.04464878
Iteration 88, loss = 0.04368631
Iteration 89, loss = 0.04375349
Iteration 90, loss = 0.04241969
Iteration 91, loss = 0.04135470
Iteration 92, loss = 0.04116893
Iteration 93, loss = 0.04103993
Iteration 94, loss = 0.03913312
Iteration 95, loss = 0.03800406
Iteration 96, loss = 0.03715038
Iteration 97, loss = 0.03624762
Iteration 98, loss = 0.03585430
Iteration 99, loss = 0.03553500
Iteration 100, loss = 0.03478872
Iteration 101, loss = 0.03427848
Iteration 102, loss = 0.03414163
Iteration 103, loss = 0.03474359
Iteration 104, loss = 0.03299983
Iteration 105, loss = 0.03170157
Iteration 106, loss = 0.03097290
Iteration 107, loss = 0.03038829
Iteration 108, loss = 0.03003440
Iteration 109, loss = 0.02958791
Iteration 110, loss = 0.02941655
Iteration 111, loss = 0.02866996
Iteration 112, loss = 0.02789813
Iteration 113, loss = 0.02847967
Iteration 114, loss = 0.02698807
Iteration 115, loss = 0.02700320
Iteration 116, loss = 0.02974774
Iteration 117, loss = 0.02781637
Iteration 118, loss = 0.02576458
Iteration 119, loss = 0.02609721
Iteration 120, loss = 0.02474301
Iteration 121, loss = 0.02371383
Iteration 122, loss = 0.02305274
Iteration 123, loss = 0.02275816
Iteration 124, loss = 0.02256173
Iteration 125, loss = 0.02265829
Iteration 126, loss = 0.02206953
Iteration 127, loss = 0.02144415
Iteration 128, loss = 0.02108784
Iteration 129, loss = 0.02048706
Iteration 130, loss = 0.02031673
Iteration 131, loss = 0.01993642
Iteration 132, loss = 0.01988892
Iteration 133, loss = 0.01963912
Iteration 134, loss = 0.01916466
Iteration 135, loss = 0.01994073
Iteration 136, loss = 0.01843788
Iteration 137, loss = 0.01866416
Iteration 138, loss = 0.01843140
Iteration 139, loss = 0.01775155
Iteration 140, loss = 0.01771998
Iteration 141, loss = 0.01740638
Iteration 142, loss = 0.01691789
Iteration 143, loss = 0.01709700
Iteration 144, loss = 0.01734293
Iteration 145, loss = 0.01682077
Iteration 146, loss = 0.01619559
Iteration 147, loss = 0.01592106
Iteration 148, loss = 0.01674669
Iteration 149, loss = 0.01564802
Iteration 150, loss = 0.01521965
Iteration 151, loss = 0.01525500
Iteration 152, loss = 0.01501995
Iteration 153, loss = 0.01597695
Iteration 154, loss = 0.01659341
Iteration 155, loss = 0.01482230
Iteration 156, loss = 0.01397539
Iteration 157, loss = 0.01402973
Iteration 158, loss = 0.01417790
Iteration 159, loss = 0.01373696
Iteration 160, loss = 0.01327501
Iteration 161, loss = 0.01304308
Iteration 162, loss = 0.01301664
Iteration 163, loss = 0.01281847
Iteration 164, loss = 0.01262026
Iteration 165, loss = 0.01270876
Iteration 166, loss = 0.01270772
Iteration 167, loss = 0.01262772
Iteration 168, loss = 0.01238798
Iteration 169, loss = 0.01212993
Iteration 170, loss = 0.01190587
Iteration 171, loss = 0.01177829
Iteration 172, loss = 0.01181408
Iteration 173, loss = 0.01188726
Iteration 174, loss = 0.01165791
Iteration 175, loss = 0.01136924
Iteration 176, loss = 0.01130961
Iteration 177, loss = 0.01134948
Iteration 178, loss = 0.01133968
Iteration 179, loss = 0.01096083
Iteration 180, loss = 0.01083235
Iteration 181, loss = 0.01084450
Iteration 182, loss = 0.01064071
Iteration 183, loss = 0.01052306
Iteration 184, loss = 0.01048579
Iteration 185, loss = 0.01049266
Iteration 186, loss = 0.01019476
Iteration 187, loss = 0.01029850
Iteration 188, loss = 0.01037594
Iteration 189, loss = 0.01003456
Iteration 190, loss = 0.00990041
Iteration 191, loss = 0.00980062
Iteration 192, loss = 0.00978816
Iteration 193, loss = 0.00976099
Iteration 194, loss = 0.00958163
Iteration 195, loss = 0.00947297
Iteration 196, loss = 0.00941950
Iteration 197, loss = 0.00927888
Iteration 198, loss = 0.00926473
Iteration 199, loss = 0.00923501
Iteration 200, loss = 0.00914260
Iteration 201, loss = 0.00906428
Iteration 202, loss = 0.00892999
Iteration 203, loss = 0.00882971
Iteration 204, loss = 0.00873708
Iteration 205, loss = 0.00860267
Iteration 206, loss = 0.00856713
Iteration 207, loss = 0.00848637
Iteration 208, loss = 0.00844868
Iteration 209, loss = 0.00841171
Iteration 210, loss = 0.00840945
Iteration 211, loss = 0.00833133
Iteration 212, loss = 0.00816699
Iteration 213, loss = 0.00814454
Iteration 214, loss = 0.00797372
Iteration 215, loss = 0.00790689
Iteration 216, loss = 0.00797700
Iteration 217, loss = 0.00800797
Iteration 218, loss = 0.00787479
Iteration 219, loss = 0.00787635
Iteration 220, loss = 0.00784926
Iteration 221, loss = 0.00767039
Iteration 222, loss = 0.00752422
Iteration 223, loss = 0.00741884
Iteration 224, loss = 0.00739029
Iteration 225, loss = 0.00735399
Iteration 226, loss = 0.00737229
Iteration 227, loss = 0.00727321
Iteration 228, loss = 0.00710570
Iteration 229, loss = 0.00702834
Iteration 230, loss = 0.00704312
Iteration 231, loss = 0.00701563
Iteration 232, loss = 0.00694638
Iteration 233, loss = 0.00690424
Iteration 234, loss = 0.00680288
Iteration 235, loss = 0.00675294
Iteration 236, loss = 0.00679408
Iteration 237, loss = 0.00679643
Iteration 238, loss = 0.00669356
Iteration 239, loss = 0.00658766
Iteration 240, loss = 0.00651463
Iteration 241, loss = 0.00644442
Iteration 242, loss = 0.00637045
Iteration 243, loss = 0.00631984
Iteration 244, loss = 0.00631462
Iteration 245, loss = 0.00628593
Iteration 246, loss = 0.00626515
Iteration 247, loss = 0.00627645
Iteration 248, loss = 0.00627412
Iteration 249, loss = 0.00614943
Iteration 250, loss = 0.00602137
Iteration 251, loss = 0.00598286
Iteration 252, loss = 0.00611864
Iteration 253, loss = 0.00599008
Iteration 254, loss = 0.00589807
Iteration 255, loss = 0.00583290
Iteration 256, loss = 0.00578815
Iteration 257, loss = 0.00578901
Iteration 258, loss = 0.00584109
Iteration 259, loss = 0.00588276
Iteration 260, loss = 0.00581276
Iteration 261, loss = 0.00575634
Iteration 262, loss = 0.00575404
Iteration 263, loss = 0.00560935
Iteration 264, loss = 0.00553424
Iteration 265, loss = 0.00549760
Iteration 266, loss = 0.00550861
Iteration 267, loss = 0.00547087
Iteration 268, loss = 0.00546095
Iteration 269, loss = 0.00541599
Iteration 270, loss = 0.00536281
Iteration 271, loss = 0.00531729
Iteration 272, loss = 0.00527887
Iteration 273, loss = 0.00526974
Iteration 274, loss = 0.00522912
Iteration 275, loss = 0.00520678
Iteration 276, loss = 0.00519356
Iteration 277, loss = 0.00521946
Iteration 278, loss = 0.00526034
Iteration 279, loss = 0.00525942
Iteration 280, loss = 0.00522539
Iteration 281, loss = 0.00512766
Iteration 282, loss = 0.00504936
Iteration 283, loss = 0.00499304
Iteration 284, loss = 0.00499745
Iteration 285, loss = 0.00496918
Iteration 286, loss = 0.00493312
Iteration 287, loss = 0.00488617
Iteration 288, loss = 0.00487507
Iteration 289, loss = 0.00488582
Iteration 290, loss = 0.00481103
Iteration 291, loss = 0.00476185
Iteration 292, loss = 0.00471431
Iteration 293, loss = 0.00469140
Iteration 294, loss = 0.00469751
Iteration 295, loss = 0.00468349
Iteration 296, loss = 0.00467085
Iteration 297, loss = 0.00466456
Iteration 298, loss = 0.00467010
Iteration 299, loss = 0.00465486
Iteration 300, loss = 0.00459763
PARAMETROS DE INICIALIZACAO DA REDE
NUMERO DE NEURONIOS 
Camada de Entrada: 34
Camada Escondida: 15
Camada de Saida: 1

--- PARAMETROS DE CONFIGURACAO DA REDE ---
Numero de Epocas: 300
Taxa de Aprendizado: adaptive
Taxa de Aprendizado Inicial: 0.6
METRICAS

RESULTADOS:

[0 0 1 0 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 1 1 1 1 1
 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 1 1 1
 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 1 0]
ACURACIA: 0.9245283018867925

