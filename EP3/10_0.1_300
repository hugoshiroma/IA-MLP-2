Pesos Camada de Entrada: 
[[-0.14230589  0.07916999  0.06483252  0.12181302 -0.16642614 -0.19550561
  -0.01683568  0.07960445 -0.10410694 -0.01918802]
 [-0.01893139 -0.05354553 -0.12490745  0.09255443 -0.17786361 -0.02271406
  -0.16326533 -0.04351216  0.10670559 -0.16141194]
 [-0.03763791  0.0348801  -0.18227663  0.06580978  0.04684726  0.09751274
  -0.08817537 -0.00794243  0.12627098 -0.13656651]
 [-0.14662538 -0.08055893 -0.02031265 -0.06840614 -0.10619354 -0.12495894
   0.01114063  0.02906651  0.03823934 -0.12689596]
 [-0.10021501 -0.03578483 -0.11015889  0.13414278  0.0984588   0.19641439
  -0.06633016 -0.10225845  0.11749006  0.08458577]
 [ 0.09198018 -0.08042204 -0.06309743 -0.20133847 -0.20705886 -0.19113398
   0.12616663 -0.19944814 -0.03536608 -0.13093733]
 [ 0.20546774  0.02945942 -0.0621773   0.11221663  0.05662847 -0.1302714
  -0.0208256  -0.12082457 -0.18533943 -0.02501073]
 [ 0.11798538 -0.17543049  0.14565985  0.16745366  0.06453768 -0.02896724
   0.10187362  0.19868113  0.08863886 -0.11883859]
 [ 0.03480652  0.01323886  0.18631945 -0.12480055 -0.20939345  0.14298731
   0.10592136 -0.11090262 -0.21081022 -0.16791605]
 [ 0.20614407  0.11948067 -0.06819286 -0.09153464  0.12990967 -0.20697298
  -0.18488461 -0.12236922 -0.13743826 -0.08529511]
 [-0.07754592 -0.01635582  0.04257805 -0.17878228 -0.01064423  0.11089846
   0.17575049 -0.13271533  0.20615484  0.1889397 ]
 [-0.1286896  -0.00575776 -0.11321288 -0.18708062  0.05323929  0.1872993
   0.20674104  0.10233679  0.02651845 -0.05262382]
 [ 0.00066042  0.02204065 -0.0378732   0.14742916  0.12633037 -0.15877837
   0.02738611  0.10008024 -0.10366609  0.18807839]
 [-0.19551314  0.07200551  0.03872313  0.1594942   0.05456818 -0.08650228
   0.06449023  0.06695018  0.13510232 -0.18580842]
 [ 0.05415029  0.01433437 -0.141968    0.03884933 -0.1511635  -0.11502315
  -0.12240691  0.1875127  -0.17625881  0.15064383]
 [ 0.10655429  0.19022775 -0.17663787 -0.06249577  0.03457154 -0.0498566
   0.16661209 -0.13938054 -0.08786519 -0.07811554]
 [-0.04448698 -0.06852725 -0.1576677   0.16248805  0.06188233  0.08093265
  -0.21274749  0.19172069 -0.17139598 -0.0919798 ]
 [-0.01416479  0.18176738  0.19833193  0.18389332 -0.1543495  -0.0718597
  -0.01757818  0.19001875 -0.00033163  0.06515645]
 [ 0.12745237  0.04373305 -0.20012373 -0.18937423 -0.18907057  0.13775067
  -0.07329849  0.14032255  0.15545387  0.07116921]
 [-0.06702656  0.04410263 -0.19794299  0.10970691 -0.21127352  0.01461391
  -0.1565955  -0.15421017  0.10411586 -0.18558085]
 [ 0.11720882 -0.17116743  0.03266005 -0.20771451  0.02865136  0.00126613
  -0.1975897  -0.20827372 -0.0129409   0.17776271]
 [ 0.20747343 -0.05070755 -0.00811549  0.17782737 -0.00058821  0.11438827
   0.01024187 -0.1957761  -0.16281879  0.15825454]
 [ 0.07289704  0.05393     0.04407498  0.06526205  0.0133858   0.14287581
   0.07462595  0.02735971  0.05390327 -0.03113612]
 [-0.16172082 -0.09773864 -0.17134779  0.15227954  0.1669619   0.04467917
   0.10120033  0.05720335 -0.18130417 -0.1192631 ]
 [-0.01381607 -0.17197482 -0.051995   -0.15103923 -0.1927733  -0.03609373
   0.07358628 -0.07264523 -0.13122211  0.01853299]
 [-0.05651223 -0.19638452  0.15507954 -0.07195195 -0.18914163  0.17394918
   0.04010754 -0.19859081  0.06236466  0.06125089]
 [ 0.04351401  0.21250524  0.1641787  -0.06394285 -0.04189031  0.02348659
   0.12088841  0.14594717  0.16072231  0.13404738]
 [ 0.02583243 -0.09102296 -0.01193425  0.02204281  0.05045185 -0.10914041
   0.12276874 -0.15381699  0.07184213 -0.10664957]
 [-0.02787383 -0.02609633  0.12414086 -0.03920595  0.00698047 -0.09277386
   0.045609    0.14288334 -0.13034054 -0.13974182]
 [-0.15636043 -0.19236924 -0.06658739 -0.19493953 -0.01629224 -0.12976515
  -0.05161479 -0.16654067  0.02827013  0.18868239]
 [-0.11135196 -0.03516456 -0.15031585  0.13133163 -0.10579918 -0.1557711
  -0.02856082  0.16386054  0.09201055 -0.0837536 ]
 [-0.00260535  0.17832341 -0.16428683  0.06734717  0.20352274 -0.16954505
  -0.1300125   0.20411915 -0.07805258  0.14952759]
 [ 0.03340357 -0.20793772  0.07573728 -0.19113962 -0.15324522 -0.16497833
   0.1791657   0.12625017  0.19974911 -0.04767869]
 [-0.07184814 -0.12562963 -0.08370451 -0.07528604  0.15479005 -0.20147674
  -0.16812608  0.02288967 -0.11561301  0.07230373]]
Bias Camada de Entrada: 
[ 0.18939329  0.10630039 -0.19887013 -0.03022163 -0.16936781  0.20134395
  0.09651472 -0.06621223  0.09797191 -0.09661609]
Pesos Camada Escondida: 
[[-0.25919847]
 [ 0.039247  ]
 [-0.37184318]
 [-0.00903022]
 [ 0.1380482 ]
 [-0.42531992]
 [ 0.12797599]
 [-0.32383426]
 [-0.09378576]
 [ 0.16884275]]
Bias Camada Escondida: 
[-0.24528438]
Iteration 1, loss = 0.85748101
Iteration 2, loss = 0.72746882
Iteration 3, loss = 0.65742866
Iteration 4, loss = 0.64956532
Iteration 5, loss = 0.66065065
Iteration 6, loss = 0.66211911
Iteration 7, loss = 0.64675032
Iteration 8, loss = 0.63089107
Iteration 9, loss = 0.62169661
Iteration 10, loss = 0.61865196
Iteration 11, loss = 0.61257822
Iteration 12, loss = 0.60330223
Iteration 13, loss = 0.59199462
Iteration 14, loss = 0.58209263
Iteration 15, loss = 0.57247011
Iteration 16, loss = 0.56217021
Iteration 17, loss = 0.55173994
Iteration 18, loss = 0.54048911
Iteration 19, loss = 0.52816797
Iteration 20, loss = 0.51593391
Iteration 21, loss = 0.50436060
Iteration 22, loss = 0.49214616
Iteration 23, loss = 0.48071012
Iteration 24, loss = 0.46943914
Iteration 25, loss = 0.45843635
Iteration 26, loss = 0.44749335
Iteration 27, loss = 0.43730261
Iteration 28, loss = 0.42805034
Iteration 29, loss = 0.41880153
Iteration 30, loss = 0.41008540
Iteration 31, loss = 0.40156322
Iteration 32, loss = 0.39502845
Iteration 33, loss = 0.38869872
Iteration 34, loss = 0.38154224
Iteration 35, loss = 0.37508351
Iteration 36, loss = 0.36892994
Iteration 37, loss = 0.36418751
Iteration 38, loss = 0.35777274
Iteration 39, loss = 0.35171888
Iteration 40, loss = 0.34729971
Iteration 41, loss = 0.34276657
Iteration 42, loss = 0.33898030
Iteration 43, loss = 0.33511069
Iteration 44, loss = 0.33176472
Iteration 45, loss = 0.32824225
Iteration 46, loss = 0.32452701
Iteration 47, loss = 0.32025967
Iteration 48, loss = 0.31650455
Iteration 49, loss = 0.31412270
Iteration 50, loss = 0.31060038
Iteration 51, loss = 0.30708151
Iteration 52, loss = 0.30401018
Iteration 53, loss = 0.30088756
Iteration 54, loss = 0.29827038
Iteration 55, loss = 0.29549803
Iteration 56, loss = 0.29295727
Iteration 57, loss = 0.29097864
Iteration 58, loss = 0.28900914
Iteration 59, loss = 0.28749448
Iteration 60, loss = 0.28473883
Iteration 61, loss = 0.28216143
Iteration 62, loss = 0.27984291
Iteration 63, loss = 0.27771153
Iteration 64, loss = 0.27560383
Iteration 65, loss = 0.27340075
Iteration 66, loss = 0.27104727
Iteration 67, loss = 0.26893441
Iteration 68, loss = 0.26726600
Iteration 69, loss = 0.26676581
Iteration 70, loss = 0.26390848
Iteration 71, loss = 0.26139257
Iteration 72, loss = 0.25897373
Iteration 73, loss = 0.25786426
Iteration 74, loss = 0.25547749
Iteration 75, loss = 0.25351382
Iteration 76, loss = 0.25197231
Iteration 77, loss = 0.24984539
Iteration 78, loss = 0.24904192
Iteration 79, loss = 0.24827501
Iteration 80, loss = 0.24444359
Iteration 81, loss = 0.24249551
Iteration 82, loss = 0.23976815
Iteration 83, loss = 0.23798778
Iteration 84, loss = 0.23574656
Iteration 85, loss = 0.23395394
Iteration 86, loss = 0.23226177
Iteration 87, loss = 0.23047298
Iteration 88, loss = 0.22834881
Iteration 89, loss = 0.22659297
Iteration 90, loss = 0.22463458
Iteration 91, loss = 0.22326281
Iteration 92, loss = 0.22126446
Iteration 93, loss = 0.21906504
Iteration 94, loss = 0.21761796
Iteration 95, loss = 0.21687123
Iteration 96, loss = 0.21393291
Iteration 97, loss = 0.21188526
Iteration 98, loss = 0.20996058
Iteration 99, loss = 0.20843731
Iteration 100, loss = 0.20659400
Iteration 101, loss = 0.20484132
Iteration 102, loss = 0.20378387
Iteration 103, loss = 0.20149094
Iteration 104, loss = 0.20088659
Iteration 105, loss = 0.19903793
Iteration 106, loss = 0.19740016
Iteration 107, loss = 0.19530066
Iteration 108, loss = 0.19367392
Iteration 109, loss = 0.19322590
Iteration 110, loss = 0.19158914
Iteration 111, loss = 0.18958123
Iteration 112, loss = 0.18764038
Iteration 113, loss = 0.18615044
Iteration 114, loss = 0.18557194
Iteration 115, loss = 0.18365732
Iteration 116, loss = 0.18233625
Iteration 117, loss = 0.18076628
Iteration 118, loss = 0.17935855
Iteration 119, loss = 0.17801841
Iteration 120, loss = 0.17673838
Iteration 121, loss = 0.17532318
Iteration 122, loss = 0.17414474
Iteration 123, loss = 0.17256867
Iteration 124, loss = 0.17162692
Iteration 125, loss = 0.17113571
Iteration 126, loss = 0.16861629
Iteration 127, loss = 0.16898996
Iteration 128, loss = 0.16823467
Iteration 129, loss = 0.16604157
Iteration 130, loss = 0.16455230
Iteration 131, loss = 0.16272478
Iteration 132, loss = 0.16146162
Iteration 133, loss = 0.16041367
Iteration 134, loss = 0.15958930
Iteration 135, loss = 0.15929861
Iteration 136, loss = 0.15928669
Iteration 137, loss = 0.15693871
Iteration 138, loss = 0.15484150
Iteration 139, loss = 0.15415034
Iteration 140, loss = 0.15421499
Iteration 141, loss = 0.15463309
Iteration 142, loss = 0.15392552
Iteration 143, loss = 0.15210558
Iteration 144, loss = 0.14957320
Iteration 145, loss = 0.14827087
Iteration 146, loss = 0.14814753
Iteration 147, loss = 0.14695245
Iteration 148, loss = 0.14624638
Iteration 149, loss = 0.14502258
Iteration 150, loss = 0.14433969
Iteration 151, loss = 0.14386468
Iteration 152, loss = 0.14336804
Iteration 153, loss = 0.14154051
Iteration 154, loss = 0.14068615
Iteration 155, loss = 0.14064236
Iteration 156, loss = 0.13935003
Iteration 157, loss = 0.13832518
Iteration 158, loss = 0.13734309
Iteration 159, loss = 0.13690844
Iteration 160, loss = 0.13637418
Iteration 161, loss = 0.13581872
Iteration 162, loss = 0.13474521
Iteration 163, loss = 0.13437603
Iteration 164, loss = 0.13350144
Iteration 165, loss = 0.13236475
Iteration 166, loss = 0.13129346
Iteration 167, loss = 0.13077822
Iteration 168, loss = 0.13012631
Iteration 169, loss = 0.12940583
Iteration 170, loss = 0.12853397
Iteration 171, loss = 0.12773196
Iteration 172, loss = 0.12724478
Iteration 173, loss = 0.12662846
Iteration 174, loss = 0.12598041
Iteration 175, loss = 0.12541686
Iteration 176, loss = 0.12466206
Iteration 177, loss = 0.12407916
Iteration 178, loss = 0.12334163
Iteration 179, loss = 0.12269906
Iteration 180, loss = 0.12223539
Iteration 181, loss = 0.12137886
Iteration 182, loss = 0.12133880
Iteration 183, loss = 0.12216206
Iteration 184, loss = 0.12036748
Iteration 185, loss = 0.11938301
Iteration 186, loss = 0.11864809
Iteration 187, loss = 0.11825071
Iteration 188, loss = 0.11775038
Iteration 189, loss = 0.11695358
Iteration 190, loss = 0.11619271
Iteration 191, loss = 0.11572408
Iteration 192, loss = 0.11522989
Iteration 193, loss = 0.11508959
Iteration 194, loss = 0.11463227
Iteration 195, loss = 0.11407685
Iteration 196, loss = 0.11362687
Iteration 197, loss = 0.11294557
Iteration 198, loss = 0.11206989
Iteration 199, loss = 0.11174729
Iteration 200, loss = 0.11214803
Iteration 201, loss = 0.11109878
Iteration 202, loss = 0.11029116
Iteration 203, loss = 0.10987936
Iteration 204, loss = 0.10915094
Iteration 205, loss = 0.10865781
Iteration 206, loss = 0.10820216
Iteration 207, loss = 0.10772052
Iteration 208, loss = 0.10727152
Iteration 209, loss = 0.10678184
Iteration 210, loss = 0.10644457
Iteration 211, loss = 0.10590044
Iteration 212, loss = 0.10561951
Iteration 213, loss = 0.10498133
Iteration 214, loss = 0.10444417
Iteration 215, loss = 0.10422969
Iteration 216, loss = 0.10390162
Iteration 217, loss = 0.10340081
Iteration 218, loss = 0.10311242
Iteration 219, loss = 0.10277266
Iteration 220, loss = 0.10312249
Iteration 221, loss = 0.10234756
Iteration 222, loss = 0.10158665
Iteration 223, loss = 0.10099397
Iteration 224, loss = 0.10045576
Iteration 225, loss = 0.09991648
Iteration 226, loss = 0.09938486
Iteration 227, loss = 0.09908767
Iteration 228, loss = 0.09894478
Iteration 229, loss = 0.09906734
Iteration 230, loss = 0.09832579
Iteration 231, loss = 0.09779573
Iteration 232, loss = 0.09716278
Iteration 233, loss = 0.09673126
Iteration 234, loss = 0.09635866
Iteration 235, loss = 0.09589670
Iteration 236, loss = 0.09565267
Iteration 237, loss = 0.09562390
Iteration 238, loss = 0.09634521
Iteration 239, loss = 0.09544123
Iteration 240, loss = 0.09413994
Iteration 241, loss = 0.09367757
Iteration 242, loss = 0.09342875
Iteration 243, loss = 0.09395861
Iteration 244, loss = 0.09347412
Iteration 245, loss = 0.09295602
Iteration 246, loss = 0.09187802
Iteration 247, loss = 0.09132152
Iteration 248, loss = 0.09132901
Iteration 249, loss = 0.09195573
Iteration 250, loss = 0.09208499
Iteration 251, loss = 0.09213123
Iteration 252, loss = 0.09131039
Iteration 253, loss = 0.09044230
Iteration 254, loss = 0.08961035
Iteration 255, loss = 0.08897045
Iteration 256, loss = 0.08845073
Iteration 257, loss = 0.08812819
Iteration 258, loss = 0.08782284
Iteration 259, loss = 0.08746069
Iteration 260, loss = 0.08722270
Iteration 261, loss = 0.08680575
Iteration 262, loss = 0.08654672
Iteration 263, loss = 0.08629789
Iteration 264, loss = 0.08568545
Iteration 265, loss = 0.08583154
Iteration 266, loss = 0.08525634
Iteration 267, loss = 0.08486394
Iteration 268, loss = 0.08460465
Iteration 269, loss = 0.08444258
Iteration 270, loss = 0.08403693
Iteration 271, loss = 0.08373074
Iteration 272, loss = 0.08363079
Iteration 273, loss = 0.08429651
Iteration 274, loss = 0.08400077
Iteration 275, loss = 0.08345005
Iteration 276, loss = 0.08274957
Iteration 277, loss = 0.08201587
Iteration 278, loss = 0.08155553
Iteration 279, loss = 0.08152400
Iteration 280, loss = 0.08112900
Iteration 281, loss = 0.08093376
Iteration 282, loss = 0.08060198
Iteration 283, loss = 0.08012946
Iteration 284, loss = 0.07980942
Iteration 285, loss = 0.07970222
Iteration 286, loss = 0.07957649
Iteration 287, loss = 0.08001069
Iteration 288, loss = 0.08000443
Iteration 289, loss = 0.07969178
Iteration 290, loss = 0.07909825
Iteration 291, loss = 0.07802447
Iteration 292, loss = 0.07762669
Iteration 293, loss = 0.07785665
Iteration 294, loss = 0.07769987
Iteration 295, loss = 0.07691609
Iteration 296, loss = 0.07664539
Iteration 297, loss = 0.07677483
Iteration 298, loss = 0.07667254
Iteration 299, loss = 0.07633681
Iteration 300, loss = 0.07592221
PARAMETROS DE INICIALIZACAO DA REDE
NUMERO DE NEURONIOS 
Camada de Entrada: 34
Camada Escondida: 10
Camada de Saida: 1

--- PARAMETROS DE CONFIGURACAO DA REDE ---
Numero de Epocas: 300
Taxa de Aprendizado: adaptive
Taxa de Aprendizado Inicial: 0.1
METRICAS

RESULTADOS:

[0 0 1 0 1 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1 1 1 1 1 1 1
 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 1 1 1
 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 1 0]
ACURACIA: 0.9433962264150944

