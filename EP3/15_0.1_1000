Pesos Camada de Entrada: 
[[ 0.06894981 -0.08478922 -0.05617642 -0.1391119   0.03469942  0.06326216
  -0.10284947  0.13517715  0.11507105  0.01697871 -0.06616848 -0.03552086
   0.02888614  0.10464258 -0.12442783]
 [-0.01667154 -0.00221411 -0.14720528 -0.05629832  0.15700615  0.06613805
   0.14776596  0.1652325  -0.02695491 -0.16735211  0.20116701  0.02188586
  -0.10250888 -0.17138824 -0.04390676]
 [ 0.10551642 -0.190361   -0.16666259  0.01706925  0.07633953 -0.0944583
   0.05158763 -0.09980992  0.12325022  0.17007433 -0.00432815 -0.06421726
   0.07703425  0.17818056  0.14505425]
 [-0.06044304  0.1548976   0.08856634 -0.11904315 -0.13739585  0.17485298
  -0.01590166 -0.19370553  0.02855788 -0.06782533 -0.12798195  0.08973933
  -0.14011869  0.08247054 -0.16863245]
 [-0.05548035  0.13173612 -0.00361794 -0.01068401  0.13694634 -0.08747743
  -0.15937606 -0.14657446 -0.01616527  0.12686091 -0.0176715   0.05971672
   0.16616481 -0.02897672 -0.17240069]
 [-0.00630399  0.1576257  -0.02610087 -0.02420959  0.00266067 -0.06444271
  -0.08229094  0.02807502  0.05291199 -0.17873966  0.12368223  0.13080896
  -0.01462145  0.07124473  0.16811595]
 [ 0.05590289 -0.11088736  0.01956553 -0.12620492 -0.18543475 -0.10084578
   0.20103986  0.1799598  -0.12666802 -0.14850387 -0.03570755  0.06867421
   0.1554192   0.07097483 -0.02422429]
 [ 0.20165103  0.09109464  0.07387333 -0.11078303  0.19024808  0.01465978
  -0.10360414  0.13685281  0.02438268  0.15793405  0.20153021 -0.1941588
  -0.134186   -0.09657154 -0.09927294]
 [-0.13507452 -0.18993228  0.19820706  0.19839028  0.12320614 -0.15625418
  -0.14813521 -0.06501133 -0.16399268  0.12456535  0.10410129  0.12354357
  -0.03209954 -0.16512647 -0.01408655]
 [-0.01246232  0.08090141 -0.0468294   0.02542482  0.19767118 -0.16745988
  -0.13524216 -0.1389025   0.170961   -0.00897897 -0.03781947  0.0648495
  -0.07472078  0.18148022  0.04880321]
 [-0.06031181  0.13097142 -0.04070624  0.05151824  0.0139524  -0.10652778
  -0.13657383 -0.07175195 -0.20171897  0.16397824  0.14054639 -0.1914235
   0.01475378  0.18500278 -0.1376975 ]
 [-0.046815    0.07635272 -0.13439391 -0.19549788 -0.04737417 -0.02541618
   0.06702009  0.04784313 -0.13893695 -0.13974326 -0.09470972  0.10920157
  -0.07581085  0.06156812 -0.06075007]
 [-0.1323404  -0.1879816  -0.00064365 -0.13163835  0.11908862 -0.04738281
   0.15180369 -0.00216096 -0.19826471  0.06107    -0.18680498  0.04606666
   0.14113733 -0.07093043 -0.03496636]
 [ 0.09389346  0.08954738  0.04352826  0.11421964 -0.20199339  0.15546161
   0.0772415  -0.11244254  0.14600913  0.02050173  0.02728459  0.07354397
  -0.06660217  0.01933708  0.1973713 ]
 [-0.04836388 -0.12747163  0.04899723  0.02162312 -0.19771294  0.02644764
   0.11889442 -0.08956916  0.15758957  0.08013664 -0.1003807   0.04204276
   0.18326025 -0.18667753  0.1936175 ]
 [ 0.04823729  0.15499331  0.16983846  0.06911929 -0.13667047 -0.02912594
  -0.01458418 -0.12859554 -0.15622313  0.00177647  0.17658434  0.01950996
   0.19515223  0.07265734 -0.08025849]
 [ 0.13452633  0.00995221 -0.15761945 -0.04375271 -0.01785581  0.19497417
   0.05936086 -0.00811555 -0.15871663 -0.15886352  0.18035029  0.1697151
   0.04923495  0.03777683  0.13101195]
 [ 0.08997579 -0.13157346 -0.05998614  0.12473017  0.11419515  0.08979598
   0.18312927 -0.05261006  0.07480402  0.05237823 -0.02456751 -0.19844682
   0.18542918  0.00450522  0.07785085]
 [ 0.19347915 -0.08551457 -0.10672674 -0.02493508  0.18794353  0.17651716
  -0.09864997 -0.08581673 -0.16111741  0.18758133  0.14068406 -0.11491606
   0.1230473  -0.06032886 -0.07422428]
 [-0.09943876 -0.13259472  0.03790282  0.00258871 -0.18245126  0.09905675
   0.171609    0.05446513  0.1736064   0.11805682 -0.02672076  0.09727404
  -0.00996718 -0.02054919  0.1893417 ]
 [ 0.17804589  0.17570569  0.09836322  0.15965297 -0.09501891 -0.07706265
  -0.01285673 -0.1462387  -0.07219904  0.0386253  -0.05886908 -0.05355227
  -0.19069033  0.16212394  0.07729097]
 [-0.17621342  0.18201968  0.07054773 -0.0627062   0.11003687 -0.00970745
   0.09842479 -0.14290487  0.10879388 -0.09614097 -0.02230423  0.19341588
   0.16001747 -0.06043749  0.20047572]
 [-0.19725325  0.05725978  0.11685748 -0.01690098 -0.06524958 -0.06921833
  -0.14342725  0.10807939  0.11888843 -0.15232947  0.15586738  0.16316496
   0.03630896  0.0222258   0.20075522]
 [ 0.0778768  -0.10180856  0.03420043  0.0824388  -0.05385657 -0.17304731
   0.11057695 -0.08947945 -0.12924079  0.14763062 -0.07577518  0.10569964
   0.11359332  0.10037688  0.13306447]
 [ 0.17239397  0.09019691 -0.04697696  0.17803021 -0.01511793 -0.09648961
   0.18366142 -0.08283358 -0.05543088 -0.09327052  0.0178463  -0.11951338
   0.1241493  -0.0907508  -0.02298237]
 [ 0.01037965  0.10451227  0.18082763 -0.01418293  0.07695012 -0.18490533
   0.05537948  0.20010632  0.0027403   0.19653202  0.01902963  0.05343147
  -0.09276739 -0.06789524  0.09476233]
 [-0.08921196 -0.19343074 -0.0257673  -0.10182384  0.11084619  0.0501158
   0.02071715  0.06159247 -0.08874179 -0.05110686  0.16821104 -0.01737703
  -0.19900957  0.16431546  0.11099956]
 [ 0.17919399 -0.03842543  0.14844035  0.09635431  0.15203736  0.19529944
   0.06882429  0.06936235  0.08323042  0.02123624  0.02254453  0.15626374
   0.0149478  -0.01603018 -0.15513496]
 [-0.08565089  0.00165803  0.15825806 -0.14795226 -0.05182559 -0.05520006
  -0.12338847 -0.08319373 -0.15280174  0.17097085 -0.04351773  0.1526847
   0.17260331 -0.0837861   0.10253074]
 [-0.12673003 -0.19000431 -0.17189048 -0.08321294 -0.04833859  0.1937059
  -0.03802119 -0.13871065 -0.20111087 -0.15489492  0.12329005  0.19235702
   0.01877223  0.02846111  0.0113888 ]
 [-0.06818269  0.07120525 -0.01649007 -0.06126581  0.19370704  0.00825013
  -0.04455179 -0.0550598  -0.14250249 -0.18201467  0.09785113 -0.14894022
   0.02927053 -0.11630005  0.09794579]
 [ 0.01728098  0.02309947 -0.05271505 -0.09054932  0.05801034  0.18391976
  -0.07257328  0.12422893  0.10678515 -0.08864264  0.11887171 -0.10790811
  -0.03224284 -0.06574808 -0.16767781]
 [ 0.0011486   0.13694755 -0.09975923 -0.14764359  0.00301318 -0.12176883
   0.17341194 -0.03029101  0.05869208 -0.16853371 -0.169113   -0.1316407
  -0.14266741  0.08250959  0.02426309]
 [-0.13730527  0.13031437  0.01634031 -0.14107428 -0.05624694 -0.0620876
   0.19939997 -0.09611795  0.04014447  0.11152753  0.16709764  0.05514222
   0.08339726 -0.11978191 -0.1434398 ]]
Bias Camada de Entrada: 
[-0.01567863 -0.11865189  0.1557548  -0.15640535 -0.19853381 -0.10087619
 -0.13608527 -0.1579024   0.14683211  0.06097174 -0.14895668 -0.19831259
  0.08922463  0.06162447 -0.08132517]
Pesos Camada Escondida: 
[[-0.18506176]
 [-0.34270161]
 [-0.28811745]
 [-0.30968863]
 [ 0.06694049]
 [-0.30581911]
 [-0.11717401]
 [ 0.23474593]
 [-0.02758208]
 [ 0.11123121]
 [ 0.25324693]
 [ 0.14534091]
 [-0.25908442]
 [ 0.17460346]
 [ 0.17344896]]
Bias Camada Escondida: 
[0.08924647]
Iteration 1, loss = 0.72162788
Iteration 2, loss = 0.67044769
Iteration 3, loss = 0.64394871
Iteration 4, loss = 0.64133536
Iteration 5, loss = 0.63985707
Iteration 6, loss = 0.63319261
Iteration 7, loss = 0.62435023
Iteration 8, loss = 0.61746335
Iteration 9, loss = 0.60889157
Iteration 10, loss = 0.60314305
Iteration 11, loss = 0.59809214
Iteration 12, loss = 0.58718482
Iteration 13, loss = 0.57423126
Iteration 14, loss = 0.56259733
Iteration 15, loss = 0.55346863
Iteration 16, loss = 0.54078087
Iteration 17, loss = 0.52891404
Iteration 18, loss = 0.51634475
Iteration 19, loss = 0.50375124
Iteration 20, loss = 0.49292093
Iteration 21, loss = 0.48140492
Iteration 22, loss = 0.46660152
Iteration 23, loss = 0.45441570
Iteration 24, loss = 0.44768065
Iteration 25, loss = 0.43354748
Iteration 26, loss = 0.42106549
Iteration 27, loss = 0.41213255
Iteration 28, loss = 0.40363874
Iteration 29, loss = 0.39541228
Iteration 30, loss = 0.38686570
Iteration 31, loss = 0.37884427
Iteration 32, loss = 0.37075593
Iteration 33, loss = 0.36379533
Iteration 34, loss = 0.35775396
Iteration 35, loss = 0.35236677
Iteration 36, loss = 0.34655889
Iteration 37, loss = 0.34114567
Iteration 38, loss = 0.33739976
Iteration 39, loss = 0.33205272
Iteration 40, loss = 0.32839083
Iteration 41, loss = 0.32429579
Iteration 42, loss = 0.32117924
Iteration 43, loss = 0.31798226
Iteration 44, loss = 0.31429421
Iteration 45, loss = 0.31050418
Iteration 46, loss = 0.30699943
Iteration 47, loss = 0.30398704
Iteration 48, loss = 0.30199270
Iteration 49, loss = 0.30027440
Iteration 50, loss = 0.29715832
Iteration 51, loss = 0.29347730
Iteration 52, loss = 0.28910783
Iteration 53, loss = 0.28627065
Iteration 54, loss = 0.28342231
Iteration 55, loss = 0.28079096
Iteration 56, loss = 0.27860856
Iteration 57, loss = 0.27596374
Iteration 58, loss = 0.27342835
Iteration 59, loss = 0.27302229
Iteration 60, loss = 0.26898131
Iteration 61, loss = 0.26765316
Iteration 62, loss = 0.26435529
Iteration 63, loss = 0.26373133
Iteration 64, loss = 0.25975930
Iteration 65, loss = 0.25879600
Iteration 66, loss = 0.25543810
Iteration 67, loss = 0.25310364
Iteration 68, loss = 0.25234506
Iteration 69, loss = 0.24909534
Iteration 70, loss = 0.24695168
Iteration 71, loss = 0.24479403
Iteration 72, loss = 0.24248686
Iteration 73, loss = 0.24105302
Iteration 74, loss = 0.23858164
Iteration 75, loss = 0.23630207
Iteration 76, loss = 0.23407983
Iteration 77, loss = 0.23220406
Iteration 78, loss = 0.23052715
Iteration 79, loss = 0.22947209
Iteration 80, loss = 0.22658052
Iteration 81, loss = 0.22449021
Iteration 82, loss = 0.22203971
Iteration 83, loss = 0.22005058
Iteration 84, loss = 0.21854164
Iteration 85, loss = 0.21704227
Iteration 86, loss = 0.21512717
Iteration 87, loss = 0.21343154
Iteration 88, loss = 0.21203526
Iteration 89, loss = 0.21187867
Iteration 90, loss = 0.20815647
Iteration 91, loss = 0.20676972
Iteration 92, loss = 0.20804570
Iteration 93, loss = 0.20732848
Iteration 94, loss = 0.20395376
Iteration 95, loss = 0.20060368
Iteration 96, loss = 0.19766141
Iteration 97, loss = 0.19564463
Iteration 98, loss = 0.19485185
Iteration 99, loss = 0.19267269
Iteration 100, loss = 0.19088368
Iteration 101, loss = 0.18934233
Iteration 102, loss = 0.18796987
Iteration 103, loss = 0.18652986
Iteration 104, loss = 0.18511420
Iteration 105, loss = 0.18402291
Iteration 106, loss = 0.18276829
Iteration 107, loss = 0.18159136
Iteration 108, loss = 0.18032271
Iteration 109, loss = 0.17889969
Iteration 110, loss = 0.17714075
Iteration 111, loss = 0.17551508
Iteration 112, loss = 0.17440202
Iteration 113, loss = 0.17448200
Iteration 114, loss = 0.17301729
Iteration 115, loss = 0.17073104
Iteration 116, loss = 0.16942734
Iteration 117, loss = 0.16853259
Iteration 118, loss = 0.16746392
Iteration 119, loss = 0.16650457
Iteration 120, loss = 0.16653534
Iteration 121, loss = 0.16577731
Iteration 122, loss = 0.16424265
Iteration 123, loss = 0.16182877
Iteration 124, loss = 0.16057934
Iteration 125, loss = 0.15987397
Iteration 126, loss = 0.15862641
Iteration 127, loss = 0.15766406
Iteration 128, loss = 0.15667658
Iteration 129, loss = 0.15583873
Iteration 130, loss = 0.15518113
Iteration 131, loss = 0.15513447
Iteration 132, loss = 0.15348911
Iteration 133, loss = 0.15241286
Iteration 134, loss = 0.15250678
Iteration 135, loss = 0.15106753
Iteration 136, loss = 0.14991614
Iteration 137, loss = 0.14921541
Iteration 138, loss = 0.14831983
Iteration 139, loss = 0.14698181
Iteration 140, loss = 0.14604151
Iteration 141, loss = 0.14567809
Iteration 142, loss = 0.14475646
Iteration 143, loss = 0.14378921
Iteration 144, loss = 0.14283619
Iteration 145, loss = 0.14220684
Iteration 146, loss = 0.14208773
Iteration 147, loss = 0.14152969
Iteration 148, loss = 0.13990581
Iteration 149, loss = 0.13915149
Iteration 150, loss = 0.13888446
Iteration 151, loss = 0.13840859
Iteration 152, loss = 0.13677946
Iteration 153, loss = 0.13631458
Iteration 154, loss = 0.13746930
Iteration 155, loss = 0.13724999
Iteration 156, loss = 0.13565207
Iteration 157, loss = 0.13408094
Iteration 158, loss = 0.13305476
Iteration 159, loss = 0.13241144
Iteration 160, loss = 0.13221641
Iteration 161, loss = 0.13156882
Iteration 162, loss = 0.13057980
Iteration 163, loss = 0.13029063
Iteration 164, loss = 0.13031131
Iteration 165, loss = 0.12957705
Iteration 166, loss = 0.12912665
Iteration 167, loss = 0.12782169
Iteration 168, loss = 0.12791179
Iteration 169, loss = 0.12831762
Iteration 170, loss = 0.12859809
Iteration 171, loss = 0.12736431
Iteration 172, loss = 0.12564665
Iteration 173, loss = 0.12432190
Iteration 174, loss = 0.12551950
Iteration 175, loss = 0.12554666
Iteration 176, loss = 0.12456312
Iteration 177, loss = 0.12304269
Iteration 178, loss = 0.12168689
Iteration 179, loss = 0.12169364
Iteration 180, loss = 0.12170893
Iteration 181, loss = 0.12173278
Iteration 182, loss = 0.12110463
Iteration 183, loss = 0.12016910
Iteration 184, loss = 0.11880265
Iteration 185, loss = 0.11844251
Iteration 186, loss = 0.11861941
Iteration 187, loss = 0.11744630
Iteration 188, loss = 0.11681324
Iteration 189, loss = 0.11640296
Iteration 190, loss = 0.11615922
Iteration 191, loss = 0.11601349
Iteration 192, loss = 0.11495615
Iteration 193, loss = 0.11458986
Iteration 194, loss = 0.11395251
Iteration 195, loss = 0.11351109
Iteration 196, loss = 0.11325714
Iteration 197, loss = 0.11282048
Iteration 198, loss = 0.11204294
Iteration 199, loss = 0.11168418
Iteration 200, loss = 0.11120430
Iteration 201, loss = 0.11106705
Iteration 202, loss = 0.11079745
Iteration 203, loss = 0.10999661
Iteration 204, loss = 0.10949575
Iteration 205, loss = 0.10917288
Iteration 206, loss = 0.10921362
Iteration 207, loss = 0.10850865
Iteration 208, loss = 0.10784122
Iteration 209, loss = 0.10750923
Iteration 210, loss = 0.10787078
Iteration 211, loss = 0.10692285
Iteration 212, loss = 0.10624207
Iteration 213, loss = 0.10595390
Iteration 214, loss = 0.10561334
Iteration 215, loss = 0.10503051
Iteration 216, loss = 0.10486350
Iteration 217, loss = 0.10414998
Iteration 218, loss = 0.10400074
Iteration 219, loss = 0.10379360
Iteration 220, loss = 0.10354474
Iteration 221, loss = 0.10315166
Iteration 222, loss = 0.10175400
Iteration 223, loss = 0.10199221
Iteration 224, loss = 0.10201728
Iteration 225, loss = 0.10185011
Iteration 226, loss = 0.10102741
Iteration 227, loss = 0.09988512
Iteration 228, loss = 0.09971403
Iteration 229, loss = 0.10085257
Iteration 230, loss = 0.10149267
Iteration 231, loss = 0.10098611
Iteration 232, loss = 0.09984752
Iteration 233, loss = 0.09826818
Iteration 234, loss = 0.09742819
Iteration 235, loss = 0.09708141
Iteration 236, loss = 0.09720292
Iteration 237, loss = 0.09774295
Iteration 238, loss = 0.09629557
Iteration 239, loss = 0.09578928
Iteration 240, loss = 0.09557108
Iteration 241, loss = 0.09593026
Iteration 242, loss = 0.09538238
Iteration 243, loss = 0.09426770
Iteration 244, loss = 0.09403402
Iteration 245, loss = 0.09403365
Iteration 246, loss = 0.09453246
Iteration 247, loss = 0.09399299
Iteration 248, loss = 0.09335844
Iteration 249, loss = 0.09246326
Iteration 250, loss = 0.09169971
Iteration 251, loss = 0.09141039
Iteration 252, loss = 0.09085323
Iteration 253, loss = 0.09076434
Iteration 254, loss = 0.09052611
Iteration 255, loss = 0.09005794
Iteration 256, loss = 0.08960329
Iteration 257, loss = 0.08916220
Iteration 258, loss = 0.08887851
Iteration 259, loss = 0.08867966
Iteration 260, loss = 0.08849775
Iteration 261, loss = 0.08779377
Iteration 262, loss = 0.08761456
Iteration 263, loss = 0.08719746
Iteration 264, loss = 0.08695008
Iteration 265, loss = 0.08663618
Iteration 266, loss = 0.08628254
Iteration 267, loss = 0.08563311
Iteration 268, loss = 0.08536841
Iteration 269, loss = 0.08508240
Iteration 270, loss = 0.08492612
Iteration 271, loss = 0.08417981
Iteration 272, loss = 0.08452039
Iteration 273, loss = 0.08491092
Iteration 274, loss = 0.08494160
Iteration 275, loss = 0.08439441
Iteration 276, loss = 0.08335207
Iteration 277, loss = 0.08260455
Iteration 278, loss = 0.08200317
Iteration 279, loss = 0.08211267
Iteration 280, loss = 0.08182996
Iteration 281, loss = 0.08175329
Iteration 282, loss = 0.08112550
Iteration 283, loss = 0.08062053
Iteration 284, loss = 0.08023695
Iteration 285, loss = 0.07989615
Iteration 286, loss = 0.07962822
Iteration 287, loss = 0.07929230
Iteration 288, loss = 0.07896114
Iteration 289, loss = 0.07865566
Iteration 290, loss = 0.07837931
Iteration 291, loss = 0.07802917
Iteration 292, loss = 0.07785178
Iteration 293, loss = 0.07756758
Iteration 294, loss = 0.07716456
Iteration 295, loss = 0.07689373
Iteration 296, loss = 0.07679919
Iteration 297, loss = 0.07655520
Iteration 298, loss = 0.07609711
Iteration 299, loss = 0.07582265
Iteration 300, loss = 0.07558439
Iteration 301, loss = 0.07531692
Iteration 302, loss = 0.07527866
Iteration 303, loss = 0.07508602
Iteration 304, loss = 0.07509186
Iteration 305, loss = 0.07443905
Iteration 306, loss = 0.07421882
Iteration 307, loss = 0.07388792
Iteration 308, loss = 0.07375586
Iteration 309, loss = 0.07337167
Iteration 310, loss = 0.07423235
Iteration 311, loss = 0.07334553
Iteration 312, loss = 0.07294840
Iteration 313, loss = 0.07270741
Iteration 314, loss = 0.07235254
Iteration 315, loss = 0.07243058
Iteration 316, loss = 0.07199034
Iteration 317, loss = 0.07155319
Iteration 318, loss = 0.07120820
Iteration 319, loss = 0.07112475
Iteration 320, loss = 0.07083072
Iteration 321, loss = 0.07054960
Iteration 322, loss = 0.07063801
Iteration 323, loss = 0.07001088
Iteration 324, loss = 0.07040296
Iteration 325, loss = 0.07090979
Iteration 326, loss = 0.06985636
Iteration 327, loss = 0.06925092
Iteration 328, loss = 0.06900037
Iteration 329, loss = 0.06893669
Iteration 330, loss = 0.06863596
Iteration 331, loss = 0.06856148
Iteration 332, loss = 0.06822414
Iteration 333, loss = 0.06806571
Iteration 334, loss = 0.06779820
Iteration 335, loss = 0.06749767
Iteration 336, loss = 0.06720925
Iteration 337, loss = 0.06716286
Iteration 338, loss = 0.06704217
Iteration 339, loss = 0.06681119
Iteration 340, loss = 0.06658097
Iteration 341, loss = 0.06626757
Iteration 342, loss = 0.06611769
Iteration 343, loss = 0.06576168
Iteration 344, loss = 0.06555385
Iteration 345, loss = 0.06529022
Iteration 346, loss = 0.06514466
Iteration 347, loss = 0.06489242
Iteration 348, loss = 0.06467789
Iteration 349, loss = 0.06450278
Iteration 350, loss = 0.06424891
Iteration 351, loss = 0.06399965
Iteration 352, loss = 0.06399085
Iteration 353, loss = 0.06412960
Iteration 354, loss = 0.06375010
Iteration 355, loss = 0.06342893
Iteration 356, loss = 0.06307844
Iteration 357, loss = 0.06297564
Iteration 358, loss = 0.06303418
Iteration 359, loss = 0.06312002
Iteration 360, loss = 0.06258414
Iteration 361, loss = 0.06218132
Iteration 362, loss = 0.06189087
Iteration 363, loss = 0.06172855
Iteration 364, loss = 0.06160032
Iteration 365, loss = 0.06131734
Iteration 366, loss = 0.06117668
Iteration 367, loss = 0.06085859
Iteration 368, loss = 0.06065349
Iteration 369, loss = 0.06054378
Iteration 370, loss = 0.06037488
Iteration 371, loss = 0.06015400
Iteration 372, loss = 0.05996497
Iteration 373, loss = 0.05984595
Iteration 374, loss = 0.05971538
Iteration 375, loss = 0.05977549
Iteration 376, loss = 0.05941336
Iteration 377, loss = 0.05884409
Iteration 378, loss = 0.05892343
Iteration 379, loss = 0.05957372
Iteration 380, loss = 0.05998723
Iteration 381, loss = 0.06022079
Iteration 382, loss = 0.06005796
Iteration 383, loss = 0.05939152
Iteration 384, loss = 0.05837416
Iteration 385, loss = 0.05770689
Iteration 386, loss = 0.05741446
Iteration 387, loss = 0.05784596
Iteration 388, loss = 0.05822645
Iteration 389, loss = 0.05803949
Iteration 390, loss = 0.05793827
Iteration 391, loss = 0.05751602
Iteration 392, loss = 0.05738291
Iteration 393, loss = 0.05725738
Iteration 394, loss = 0.05707124
Iteration 395, loss = 0.05636230
Iteration 396, loss = 0.05564508
Iteration 397, loss = 0.05555336
Iteration 398, loss = 0.05580803
Iteration 399, loss = 0.05578057
Iteration 400, loss = 0.05598357
Iteration 401, loss = 0.05484583
Iteration 402, loss = 0.05467540
Iteration 403, loss = 0.05433220
Iteration 404, loss = 0.05425285
Iteration 405, loss = 0.05422881
Iteration 406, loss = 0.05442181
Iteration 407, loss = 0.05393897
Iteration 408, loss = 0.05366082
Iteration 409, loss = 0.05334578
Iteration 410, loss = 0.05328919
Iteration 411, loss = 0.05343234
Iteration 412, loss = 0.05334541
Iteration 413, loss = 0.05316127
Iteration 414, loss = 0.05290196
Iteration 415, loss = 0.05294870
Iteration 416, loss = 0.05246219
Iteration 417, loss = 0.05233655
Iteration 418, loss = 0.05220330
Iteration 419, loss = 0.05207346
Iteration 420, loss = 0.05184659
Iteration 421, loss = 0.05154761
Iteration 422, loss = 0.05116662
Iteration 423, loss = 0.05094867
Iteration 424, loss = 0.05076498
Iteration 425, loss = 0.05062042
Iteration 426, loss = 0.05065595
Iteration 427, loss = 0.05021870
Iteration 428, loss = 0.05018284
Iteration 429, loss = 0.05084959
Iteration 430, loss = 0.05091953
Iteration 431, loss = 0.05064344
Iteration 432, loss = 0.05013902
Iteration 433, loss = 0.04967730
Iteration 434, loss = 0.04927161
Iteration 435, loss = 0.04898443
Iteration 436, loss = 0.04945794
Iteration 437, loss = 0.04928137
Iteration 438, loss = 0.04899235
Iteration 439, loss = 0.04862890
Iteration 440, loss = 0.04840746
Iteration 441, loss = 0.04813551
Iteration 442, loss = 0.04784846
Iteration 443, loss = 0.04796688
Iteration 444, loss = 0.04826745
Iteration 445, loss = 0.04828192
Iteration 446, loss = 0.04822387
Iteration 447, loss = 0.04791148
Iteration 448, loss = 0.04722745
Iteration 449, loss = 0.04679994
Iteration 450, loss = 0.04677051
Iteration 451, loss = 0.04677088
Iteration 452, loss = 0.04685498
Iteration 453, loss = 0.04690044
Iteration 454, loss = 0.04688583
Iteration 455, loss = 0.04681808
Iteration 456, loss = 0.04616653
Iteration 457, loss = 0.04581421
Iteration 458, loss = 0.04565321
Iteration 459, loss = 0.04545345
Iteration 460, loss = 0.04535401
Iteration 461, loss = 0.04538066
Iteration 462, loss = 0.04519676
Iteration 463, loss = 0.04499217
Iteration 464, loss = 0.04482387
Iteration 465, loss = 0.04466514
Iteration 466, loss = 0.04470180
Iteration 467, loss = 0.04446248
Iteration 468, loss = 0.04437480
Iteration 469, loss = 0.04428777
Iteration 470, loss = 0.04420803
Iteration 471, loss = 0.04411350
Iteration 472, loss = 0.04393174
Iteration 473, loss = 0.04371151
Iteration 474, loss = 0.04369365
Iteration 475, loss = 0.04360657
Iteration 476, loss = 0.04342327
Iteration 477, loss = 0.04332619
Iteration 478, loss = 0.04362106
Iteration 479, loss = 0.04452297
Iteration 480, loss = 0.04330503
Iteration 481, loss = 0.04255017
Iteration 482, loss = 0.04286538
Iteration 483, loss = 0.04326896
Iteration 484, loss = 0.04289516
Iteration 485, loss = 0.04248078
Iteration 486, loss = 0.04191400
Iteration 487, loss = 0.04190410
Iteration 488, loss = 0.04193102
Iteration 489, loss = 0.04189217
Iteration 490, loss = 0.04174913
Iteration 491, loss = 0.04135885
Iteration 492, loss = 0.04121761
Iteration 493, loss = 0.04106857
Iteration 494, loss = 0.04094222
Iteration 495, loss = 0.04067498
Iteration 496, loss = 0.04039583
Iteration 497, loss = 0.04017693
Iteration 498, loss = 0.04005417
Iteration 499, loss = 0.04017446
Iteration 500, loss = 0.03957714
Iteration 501, loss = 0.04005762
Iteration 502, loss = 0.04008317
Iteration 503, loss = 0.04012770
Iteration 504, loss = 0.03989680
Iteration 505, loss = 0.03955478
Iteration 506, loss = 0.03942572
Iteration 507, loss = 0.03936740
Iteration 508, loss = 0.03925328
Iteration 509, loss = 0.03914350
Iteration 510, loss = 0.03874299
Iteration 511, loss = 0.03830516
Iteration 512, loss = 0.03892755
Iteration 513, loss = 0.03819472
Iteration 514, loss = 0.03800018
Iteration 515, loss = 0.03779858
Iteration 516, loss = 0.03770343
Iteration 517, loss = 0.03763546
Iteration 518, loss = 0.03759470
Iteration 519, loss = 0.03749514
Iteration 520, loss = 0.03720452
Iteration 521, loss = 0.03704075
Iteration 522, loss = 0.03711932
Iteration 523, loss = 0.03696803
Iteration 524, loss = 0.03683468
Iteration 525, loss = 0.03672934
Iteration 526, loss = 0.03667773
Iteration 527, loss = 0.03663555
Iteration 528, loss = 0.03667367
Iteration 529, loss = 0.03655139
Iteration 530, loss = 0.03639059
Iteration 531, loss = 0.03621892
Iteration 532, loss = 0.03607988
Iteration 533, loss = 0.03601772
Iteration 534, loss = 0.03598962
Iteration 535, loss = 0.03574060
Iteration 536, loss = 0.03570689
Iteration 537, loss = 0.03553501
Iteration 538, loss = 0.03571091
Iteration 539, loss = 0.03634254
Iteration 540, loss = 0.03624317
Iteration 541, loss = 0.03594525
Iteration 542, loss = 0.03551242
Iteration 543, loss = 0.03516520
Iteration 544, loss = 0.03493448
Iteration 545, loss = 0.03470260
Iteration 546, loss = 0.03454109
Iteration 547, loss = 0.03445445
Iteration 548, loss = 0.03435162
Iteration 549, loss = 0.03427961
Iteration 550, loss = 0.03421793
Iteration 551, loss = 0.03416464
Iteration 552, loss = 0.03408960
Iteration 553, loss = 0.03398988
Iteration 554, loss = 0.03393558
Iteration 555, loss = 0.03381219
Iteration 556, loss = 0.03368645
Iteration 557, loss = 0.03366782
Iteration 558, loss = 0.03365297
Iteration 559, loss = 0.03357467
Iteration 560, loss = 0.03352040
Iteration 561, loss = 0.03337123
Iteration 562, loss = 0.03312888
Iteration 563, loss = 0.03298531
Iteration 564, loss = 0.03287305
Iteration 565, loss = 0.03284362
Iteration 566, loss = 0.03301641
Iteration 567, loss = 0.03319622
Iteration 568, loss = 0.03346057
Iteration 569, loss = 0.03311162
Iteration 570, loss = 0.03277492
Iteration 571, loss = 0.03256538
Iteration 572, loss = 0.03241678
Iteration 573, loss = 0.03228519
Iteration 574, loss = 0.03214518
Iteration 575, loss = 0.03235044
Iteration 576, loss = 0.03235924
Iteration 577, loss = 0.03214848
Iteration 578, loss = 0.03194547
Iteration 579, loss = 0.03175237
Iteration 580, loss = 0.03166270
Iteration 581, loss = 0.03153736
Iteration 582, loss = 0.03144076
Iteration 583, loss = 0.03153875
Iteration 584, loss = 0.03125414
Iteration 585, loss = 0.03115228
Iteration 586, loss = 0.03110904
Iteration 587, loss = 0.03099916
Iteration 588, loss = 0.03095296
Iteration 589, loss = 0.03096824
Iteration 590, loss = 0.03083548
Iteration 591, loss = 0.03067630
Iteration 592, loss = 0.03056358
Iteration 593, loss = 0.03048249
Iteration 594, loss = 0.03040182
Iteration 595, loss = 0.03039249
Iteration 596, loss = 0.03034637
Iteration 597, loss = 0.03036530
Iteration 598, loss = 0.03025217
Iteration 599, loss = 0.03027799
Iteration 600, loss = 0.03005243
Iteration 601, loss = 0.02998003
Iteration 602, loss = 0.02997373
Iteration 603, loss = 0.02991192
Iteration 604, loss = 0.02974068
Iteration 605, loss = 0.02952006
Iteration 606, loss = 0.02953627
Iteration 607, loss = 0.02943641
Iteration 608, loss = 0.02930666
Iteration 609, loss = 0.02923208
Iteration 610, loss = 0.02916506
Iteration 611, loss = 0.02910447
Iteration 612, loss = 0.02923582
Iteration 613, loss = 0.02896486
Iteration 614, loss = 0.02886439
Iteration 615, loss = 0.02883295
Iteration 616, loss = 0.02880635
Iteration 617, loss = 0.02874740
Iteration 618, loss = 0.02868212
Iteration 619, loss = 0.02865858
Iteration 620, loss = 0.02861801
Iteration 621, loss = 0.02858251
Iteration 622, loss = 0.02851211
Iteration 623, loss = 0.02847021
Iteration 624, loss = 0.02836847
Iteration 625, loss = 0.02825746
Iteration 626, loss = 0.02815758
Iteration 627, loss = 0.02808781
Iteration 628, loss = 0.02801725
Iteration 629, loss = 0.02790214
Iteration 630, loss = 0.02785399
Iteration 631, loss = 0.02773757
Iteration 632, loss = 0.02767600
Iteration 633, loss = 0.02762809
Iteration 634, loss = 0.02760314
Iteration 635, loss = 0.02751337
Iteration 636, loss = 0.02740605
Iteration 637, loss = 0.02736726
Iteration 638, loss = 0.02718619
Iteration 639, loss = 0.02711298
Iteration 640, loss = 0.02708068
Iteration 641, loss = 0.02701155
Iteration 642, loss = 0.02709784
Iteration 643, loss = 0.02701478
Iteration 644, loss = 0.02704361
Iteration 645, loss = 0.02681580
Iteration 646, loss = 0.02666283
Iteration 647, loss = 0.02651588
Iteration 648, loss = 0.02643729
Iteration 649, loss = 0.02645670
Iteration 650, loss = 0.02672996
Iteration 651, loss = 0.02640306
Iteration 652, loss = 0.02620231
Iteration 653, loss = 0.02606816
Iteration 654, loss = 0.02629536
Iteration 655, loss = 0.02620260
Iteration 656, loss = 0.02620171
Iteration 657, loss = 0.02601795
Iteration 658, loss = 0.02582660
Iteration 659, loss = 0.02570214
Iteration 660, loss = 0.02566325
Iteration 661, loss = 0.02575519
Iteration 662, loss = 0.02590819
Iteration 663, loss = 0.02552428
Iteration 664, loss = 0.02556842
Iteration 665, loss = 0.02549112
Iteration 666, loss = 0.02542066
Iteration 667, loss = 0.02531174
Iteration 668, loss = 0.02519603
Iteration 669, loss = 0.02517710
Iteration 670, loss = 0.02511826
Iteration 671, loss = 0.02510290
Iteration 672, loss = 0.02515010
Iteration 673, loss = 0.02524417
Iteration 674, loss = 0.02537111
Iteration 675, loss = 0.02549705
Iteration 676, loss = 0.02535260
Iteration 677, loss = 0.02519939
Iteration 678, loss = 0.02507147
Iteration 679, loss = 0.02454710
Iteration 680, loss = 0.02450302
Iteration 681, loss = 0.02454129
Iteration 682, loss = 0.02478523
Iteration 683, loss = 0.02488732
Iteration 684, loss = 0.02471245
Iteration 685, loss = 0.02440878
Iteration 686, loss = 0.02410718
Iteration 687, loss = 0.02407186
Iteration 688, loss = 0.02413125
Iteration 689, loss = 0.02403637
Iteration 690, loss = 0.02404234
Iteration 691, loss = 0.02403391
Iteration 692, loss = 0.02411118
Iteration 693, loss = 0.02407754
Iteration 694, loss = 0.02400606
Iteration 695, loss = 0.02391901
Iteration 696, loss = 0.02380767
Iteration 697, loss = 0.02368674
Iteration 698, loss = 0.02359606
Iteration 699, loss = 0.02351891
Iteration 700, loss = 0.02346493
Iteration 701, loss = 0.02338380
Iteration 702, loss = 0.02333660
Iteration 703, loss = 0.02329357
Iteration 704, loss = 0.02328366
Iteration 705, loss = 0.02324057
Iteration 706, loss = 0.02324693
Iteration 707, loss = 0.02314173
Iteration 708, loss = 0.02307221
Iteration 709, loss = 0.02300069
Iteration 710, loss = 0.02291079
Iteration 711, loss = 0.02297262
Iteration 712, loss = 0.02279117
Iteration 713, loss = 0.02268225
Iteration 714, loss = 0.02268535
Iteration 715, loss = 0.02262077
Iteration 716, loss = 0.02257240
Iteration 717, loss = 0.02253162
Iteration 718, loss = 0.02252893
Iteration 719, loss = 0.02244726
Iteration 720, loss = 0.02235845
Iteration 721, loss = 0.02232192
Iteration 722, loss = 0.02220678
Iteration 723, loss = 0.02216092
Iteration 724, loss = 0.02215170
Iteration 725, loss = 0.02214825
Iteration 726, loss = 0.02209743
Iteration 727, loss = 0.02203384
Iteration 728, loss = 0.02200149
Iteration 729, loss = 0.02195342
Iteration 730, loss = 0.02177827
Iteration 731, loss = 0.02173566
Iteration 732, loss = 0.02171047
Iteration 733, loss = 0.02167366
Iteration 734, loss = 0.02164371
Iteration 735, loss = 0.02164023
Iteration 736, loss = 0.02167149
Iteration 737, loss = 0.02175725
Iteration 738, loss = 0.02196963
Iteration 739, loss = 0.02205837
Iteration 740, loss = 0.02203413
Iteration 741, loss = 0.02191041
Iteration 742, loss = 0.02164367
Iteration 743, loss = 0.02132098
Iteration 744, loss = 0.02126699
Iteration 745, loss = 0.02105616
Iteration 746, loss = 0.02112053
Iteration 747, loss = 0.02109550
Iteration 748, loss = 0.02110984
Iteration 749, loss = 0.02128191
Iteration 750, loss = 0.02116012
Iteration 751, loss = 0.02103653
Iteration 752, loss = 0.02101913
Iteration 753, loss = 0.02088501
Iteration 754, loss = 0.02079684
Iteration 755, loss = 0.02070054
Iteration 756, loss = 0.02058688
Iteration 757, loss = 0.02058008
Iteration 758, loss = 0.02047082
Iteration 759, loss = 0.02044191
Iteration 760, loss = 0.02042694
Iteration 761, loss = 0.02051252
Iteration 762, loss = 0.02040028
Iteration 763, loss = 0.02038437
Iteration 764, loss = 0.02030412
Iteration 765, loss = 0.02029272
Iteration 766, loss = 0.02025778
Iteration 767, loss = 0.02022037
Iteration 768, loss = 0.02023332
Iteration 769, loss = 0.02034436
Iteration 770, loss = 0.02018885
Iteration 771, loss = 0.02007223
Iteration 772, loss = 0.02002995
Iteration 773, loss = 0.01995204
Iteration 774, loss = 0.01991586
Iteration 775, loss = 0.01987247
Iteration 776, loss = 0.01983079
Iteration 777, loss = 0.01980495
Iteration 778, loss = 0.01975159
Iteration 779, loss = 0.01975706
Iteration 780, loss = 0.01976523
Iteration 781, loss = 0.01969155
Iteration 782, loss = 0.01963147
Iteration 783, loss = 0.01957526
Iteration 784, loss = 0.01950333
Iteration 785, loss = 0.01946416
Iteration 786, loss = 0.01932583
Iteration 787, loss = 0.01927153
Iteration 788, loss = 0.01926618
Iteration 789, loss = 0.01921946
Iteration 790, loss = 0.01918427
Iteration 791, loss = 0.01912128
Iteration 792, loss = 0.01918280
Iteration 793, loss = 0.01909227
Iteration 794, loss = 0.01902597
Iteration 795, loss = 0.01897831
Iteration 796, loss = 0.01891507
Iteration 797, loss = 0.01892029
Iteration 798, loss = 0.01885566
Iteration 799, loss = 0.01882234
Iteration 800, loss = 0.01878144
Iteration 801, loss = 0.01876586
Iteration 802, loss = 0.01870646
Iteration 803, loss = 0.01876660
Iteration 804, loss = 0.01885240
Iteration 805, loss = 0.01884113
Iteration 806, loss = 0.01875350
Iteration 807, loss = 0.01853485
Iteration 808, loss = 0.01848767
Iteration 809, loss = 0.01843802
Iteration 810, loss = 0.01840064
Iteration 811, loss = 0.01844343
Iteration 812, loss = 0.01842803
Iteration 813, loss = 0.01842608
Iteration 814, loss = 0.01844835
Iteration 815, loss = 0.01835982
Iteration 816, loss = 0.01821963
Iteration 817, loss = 0.01814703
Iteration 818, loss = 0.01813208
Iteration 819, loss = 0.01822708
Iteration 820, loss = 0.01813334
Iteration 821, loss = 0.01811020
Iteration 822, loss = 0.01812793
Iteration 823, loss = 0.01806857
Iteration 824, loss = 0.01802589
Iteration 825, loss = 0.01801415
Iteration 826, loss = 0.01805065
Iteration 827, loss = 0.01794940
Iteration 828, loss = 0.01779427
Iteration 829, loss = 0.01776886
Iteration 830, loss = 0.01767394
Iteration 831, loss = 0.01772406
Iteration 832, loss = 0.01766702
Iteration 833, loss = 0.01765059
Iteration 834, loss = 0.01760642
Iteration 835, loss = 0.01761223
Iteration 836, loss = 0.01754982
Iteration 837, loss = 0.01750398
Iteration 838, loss = 0.01737148
Iteration 839, loss = 0.01734527
Iteration 840, loss = 0.01740718
Iteration 841, loss = 0.01742815
Iteration 842, loss = 0.01741222
Iteration 843, loss = 0.01739169
Iteration 844, loss = 0.01736236
Iteration 845, loss = 0.01730991
Iteration 846, loss = 0.01719402
Iteration 847, loss = 0.01711091
Iteration 848, loss = 0.01710152
Iteration 849, loss = 0.01701993
Iteration 850, loss = 0.01697658
Iteration 851, loss = 0.01696444
Iteration 852, loss = 0.01690256
Iteration 853, loss = 0.01687901
Iteration 854, loss = 0.01681232
Iteration 855, loss = 0.01678826
Iteration 856, loss = 0.01678958
Iteration 857, loss = 0.01677186
Iteration 858, loss = 0.01670575
Iteration 859, loss = 0.01670260
Iteration 860, loss = 0.01667930
Iteration 861, loss = 0.01665410
Iteration 862, loss = 0.01664160
Iteration 863, loss = 0.01662548
Iteration 864, loss = 0.01661537
Iteration 865, loss = 0.01660087
Iteration 866, loss = 0.01657192
Iteration 867, loss = 0.01646666
Iteration 868, loss = 0.01644663
Iteration 869, loss = 0.01634421
Iteration 870, loss = 0.01635893
Iteration 871, loss = 0.01627297
Iteration 872, loss = 0.01623385
Iteration 873, loss = 0.01624383
Iteration 874, loss = 0.01624130
Iteration 875, loss = 0.01613911
Iteration 876, loss = 0.01616379
Iteration 877, loss = 0.01616720
Iteration 878, loss = 0.01611492
Iteration 879, loss = 0.01607338
Iteration 880, loss = 0.01603311
Iteration 881, loss = 0.01599528
Iteration 882, loss = 0.01594831
Iteration 883, loss = 0.01598501
Iteration 884, loss = 0.01592054
Iteration 885, loss = 0.01587117
Iteration 886, loss = 0.01583779
Iteration 887, loss = 0.01581511
Iteration 888, loss = 0.01576477
Iteration 889, loss = 0.01572562
Iteration 890, loss = 0.01579072
Iteration 891, loss = 0.01594616
Iteration 892, loss = 0.01609801
Iteration 893, loss = 0.01612807
Iteration 894, loss = 0.01605975
Iteration 895, loss = 0.01587881
Iteration 896, loss = 0.01574729
Iteration 897, loss = 0.01559510
Iteration 898, loss = 0.01550009
Iteration 899, loss = 0.01546180
Iteration 900, loss = 0.01544636
Iteration 901, loss = 0.01548781
Iteration 902, loss = 0.01551578
Iteration 903, loss = 0.01538564
Iteration 904, loss = 0.01533344
Iteration 905, loss = 0.01530130
Iteration 906, loss = 0.01525929
Iteration 907, loss = 0.01527469
Iteration 908, loss = 0.01520497
Iteration 909, loss = 0.01516806
Iteration 910, loss = 0.01515340
Iteration 911, loss = 0.01511301
Iteration 912, loss = 0.01510509
Iteration 913, loss = 0.01506575
Iteration 914, loss = 0.01503917
Iteration 915, loss = 0.01500265
Iteration 916, loss = 0.01498241
Iteration 917, loss = 0.01497486
Iteration 918, loss = 0.01499101
Iteration 919, loss = 0.01500335
Iteration 920, loss = 0.01503256
Iteration 921, loss = 0.01497786
Iteration 922, loss = 0.01490483
Iteration 923, loss = 0.01482312
Iteration 924, loss = 0.01480953
Iteration 925, loss = 0.01476026
Iteration 926, loss = 0.01474301
Iteration 927, loss = 0.01476373
Iteration 928, loss = 0.01475984
Iteration 929, loss = 0.01474797
Iteration 930, loss = 0.01471547
Iteration 931, loss = 0.01468738
Iteration 932, loss = 0.01466380
Iteration 933, loss = 0.01465703
Iteration 934, loss = 0.01463369
Iteration 935, loss = 0.01462375
Iteration 936, loss = 0.01457019
Iteration 937, loss = 0.01450976
Iteration 938, loss = 0.01446258
Iteration 939, loss = 0.01441228
Iteration 940, loss = 0.01439097
Iteration 941, loss = 0.01434123
Iteration 942, loss = 0.01438520
Iteration 943, loss = 0.01425163
Iteration 944, loss = 0.01423044
Iteration 945, loss = 0.01421605
Iteration 946, loss = 0.01418548
Iteration 947, loss = 0.01417470
Iteration 948, loss = 0.01412900
Iteration 949, loss = 0.01415785
Iteration 950, loss = 0.01408275
Iteration 951, loss = 0.01406261
Iteration 952, loss = 0.01403352
Iteration 953, loss = 0.01401844
Iteration 954, loss = 0.01402008
Iteration 955, loss = 0.01401815
Iteration 956, loss = 0.01396575
Iteration 957, loss = 0.01395638
Iteration 958, loss = 0.01391161
Iteration 959, loss = 0.01387306
Iteration 960, loss = 0.01385861
Iteration 961, loss = 0.01382391
Iteration 962, loss = 0.01381120
Iteration 963, loss = 0.01377685
Iteration 964, loss = 0.01374340
Iteration 965, loss = 0.01370236
Iteration 966, loss = 0.01366961
Iteration 967, loss = 0.01366637
Iteration 968, loss = 0.01364155
Iteration 969, loss = 0.01364133
Iteration 970, loss = 0.01360812
Iteration 971, loss = 0.01352785
Iteration 972, loss = 0.01356210
Iteration 973, loss = 0.01356674
Iteration 974, loss = 0.01360701
Iteration 975, loss = 0.01366063
Iteration 976, loss = 0.01352201
Iteration 977, loss = 0.01342360
Iteration 978, loss = 0.01335023
Iteration 979, loss = 0.01335436
Iteration 980, loss = 0.01337233
Iteration 981, loss = 0.01334049
Iteration 982, loss = 0.01332583
Iteration 983, loss = 0.01330091
Iteration 984, loss = 0.01324716
Iteration 985, loss = 0.01319287
Iteration 986, loss = 0.01320037
Iteration 987, loss = 0.01319321
Iteration 988, loss = 0.01321313
Iteration 989, loss = 0.01318554
Iteration 990, loss = 0.01316570
Iteration 991, loss = 0.01318382
Iteration 992, loss = 0.01313310
Iteration 993, loss = 0.01308621
Iteration 994, loss = 0.01304194
Iteration 995, loss = 0.01299736
Iteration 996, loss = 0.01294180
Iteration 997, loss = 0.01291408
Iteration 998, loss = 0.01300704
Iteration 999, loss = 0.01291423
Iteration 1000, loss = 0.01287673
PARAMETROS DE INICIALIZACAO DA REDE
NUMERO DE NEURONIOS 
Camada de Entrada: 34
Camada Escondida: 15
Camada de Saida: 1

--- PARAMETROS DE CONFIGURACAO DA REDE ---
Numero de Epocas: 1000
Taxa de Aprendizado: adaptive
Taxa de Aprendizado Inicial: 0.1
METRICAS

RESULTADOS:

[0 0 1 0 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1 1 1 1 1 1 1
 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 1 0 1
 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 1 0]
ACURACIA: 0.9339622641509434

