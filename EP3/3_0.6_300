Pesos Camada de Entrada: 
[[-0.14066457  0.07878742 -0.16763884]
 [ 0.13027104  0.15304596  0.13253826]
 [ 0.02488211  0.14720631 -0.05801067]
 [-0.01686458  0.02936405 -0.15014124]
 [ 0.16690635 -0.01824938 -0.14957169]
 [ 0.15472786 -0.0348911  -0.04136606]
 [ 0.15855872  0.06814066  0.19488192]
 [ 0.16925091 -0.20416209 -0.15263597]
 [-0.20669479 -0.14549516  0.2300728 ]
 [-0.05666484  0.07089461 -0.02874772]
 [-0.08566518  0.18484731 -0.22331838]
 [ 0.03038897  0.10796979 -0.2122607 ]
 [ 0.15518693  0.12292858 -0.13432882]
 [ 0.12273418 -0.00439465 -0.00306009]
 [ 0.17141553  0.01020415  0.14106205]
 [-0.08368699 -0.03062471 -0.11302655]
 [-0.02222625  0.01411056  0.19563349]
 [ 0.1008163   0.14395321  0.03092457]
 [ 0.16191534  0.12949098 -0.13612966]
 [-0.19709493 -0.15243747 -0.13285152]
 [-0.12811157  0.17929031 -0.13858464]
 [ 0.15670769 -0.04930496 -0.20226414]
 [ 0.12595411 -0.06121076 -0.05535828]
 [-0.18982681 -0.0466345  -0.0412631 ]
 [-0.11229719 -0.16675088 -0.1158667 ]
 [-0.15771546 -0.01339998 -0.17761578]
 [ 0.1981992   0.02377086 -0.2189093 ]
 [-0.13931084 -0.0726991  -0.12438987]
 [-0.16282137 -0.12924564  0.0858819 ]
 [-0.00036124 -0.04381884 -0.15732719]
 [-0.12660344  0.11252668  0.14494222]
 [-0.14026674 -0.16266413 -0.11677532]
 [-0.16575748 -0.14649768  0.13708269]
 [-0.09795754  0.21993061  0.03749038]]
Bias Camada de Entrada: 
[0.08578104 0.11566013 0.03257083]
Pesos Camada Escondida: 
[[-0.39600939]
 [ 0.00771403]
 [-0.6307798 ]]
Bias Camada Escondida: 
[0.15409856]
Iteration 1, loss = 0.74136130
Iteration 2, loss = 0.64008107
Iteration 3, loss = 0.64990110
Iteration 4, loss = 0.63833649
Iteration 5, loss = 0.60755574
Iteration 6, loss = 0.57886927
Iteration 7, loss = 0.54582951
Iteration 8, loss = 0.50758890
Iteration 9, loss = 0.47069012
Iteration 10, loss = 0.42600354
Iteration 11, loss = 0.39561593
Iteration 12, loss = 0.37143241
Iteration 13, loss = 0.34966262
Iteration 14, loss = 0.32916468
Iteration 15, loss = 0.31411731
Iteration 16, loss = 0.30479812
Iteration 17, loss = 0.30731373
Iteration 18, loss = 0.29036819
Iteration 19, loss = 0.27658132
Iteration 20, loss = 0.27173263
Iteration 21, loss = 0.26265289
Iteration 22, loss = 0.25160085
Iteration 23, loss = 0.24570039
Iteration 24, loss = 0.23780703
Iteration 25, loss = 0.23141956
Iteration 26, loss = 0.22546132
Iteration 27, loss = 0.21410366
Iteration 28, loss = 0.20688519
Iteration 29, loss = 0.20089465
Iteration 30, loss = 0.21024417
Iteration 31, loss = 0.18884246
Iteration 32, loss = 0.18314136
Iteration 33, loss = 0.18082991
Iteration 34, loss = 0.17358521
Iteration 35, loss = 0.16729886
Iteration 36, loss = 0.16162640
Iteration 37, loss = 0.15905463
Iteration 38, loss = 0.15588193
Iteration 39, loss = 0.14810945
Iteration 40, loss = 0.14674509
Iteration 41, loss = 0.14199810
Iteration 42, loss = 0.13943844
Iteration 43, loss = 0.13665635
Iteration 44, loss = 0.13293238
Iteration 45, loss = 0.13192320
Iteration 46, loss = 0.12507779
Iteration 47, loss = 0.12210500
Iteration 48, loss = 0.11943663
Iteration 49, loss = 0.11869336
Iteration 50, loss = 0.11631042
Iteration 51, loss = 0.11445254
Iteration 52, loss = 0.11258342
Iteration 53, loss = 0.10999000
Iteration 54, loss = 0.10540995
Iteration 55, loss = 0.10317545
Iteration 56, loss = 0.10094314
Iteration 57, loss = 0.09788109
Iteration 58, loss = 0.09695649
Iteration 59, loss = 0.09437704
Iteration 60, loss = 0.09244149
Iteration 61, loss = 0.09048416
Iteration 62, loss = 0.08927807
Iteration 63, loss = 0.08755160
Iteration 64, loss = 0.08822385
Iteration 65, loss = 0.08243928
Iteration 66, loss = 0.08191773
Iteration 67, loss = 0.08169282
Iteration 68, loss = 0.07920254
Iteration 69, loss = 0.07621119
Iteration 70, loss = 0.07444844
Iteration 71, loss = 0.07296275
Iteration 72, loss = 0.07062806
Iteration 73, loss = 0.07132833
Iteration 74, loss = 0.06856531
Iteration 75, loss = 0.06707319
Iteration 76, loss = 0.06634657
Iteration 77, loss = 0.06537303
Iteration 78, loss = 0.06346897
Iteration 79, loss = 0.06168775
Iteration 80, loss = 0.06021862
Iteration 81, loss = 0.05854850
Iteration 82, loss = 0.05788060
Iteration 83, loss = 0.05710913
Iteration 84, loss = 0.05641559
Iteration 85, loss = 0.05720395
Iteration 86, loss = 0.05472053
Iteration 87, loss = 0.05115379
Iteration 88, loss = 0.05159823
Iteration 89, loss = 0.05007836
Iteration 90, loss = 0.04856513
Iteration 91, loss = 0.04881361
Iteration 92, loss = 0.04656974
Iteration 93, loss = 0.04629948
Iteration 94, loss = 0.04591845
Iteration 95, loss = 0.04476545
Iteration 96, loss = 0.04292927
Iteration 97, loss = 0.04355612
Iteration 98, loss = 0.04201528
Iteration 99, loss = 0.04134507
Iteration 100, loss = 0.04045583
Iteration 101, loss = 0.03986320
Iteration 102, loss = 0.03899084
Iteration 103, loss = 0.03950549
Iteration 104, loss = 0.03946224
Iteration 105, loss = 0.03852409
Iteration 106, loss = 0.03663218
Iteration 107, loss = 0.03718957
Iteration 108, loss = 0.03534915
Iteration 109, loss = 0.03482710
Iteration 110, loss = 0.03455562
Iteration 111, loss = 0.03401398
Iteration 112, loss = 0.03323437
Iteration 113, loss = 0.03255194
Iteration 114, loss = 0.03218537
Iteration 115, loss = 0.03174001
Iteration 116, loss = 0.03086937
Iteration 117, loss = 0.03047909
Iteration 118, loss = 0.03065634
Iteration 119, loss = 0.03000845
Iteration 120, loss = 0.02930667
Iteration 121, loss = 0.02934616
Iteration 122, loss = 0.03050955
Iteration 123, loss = 0.02954633
Iteration 124, loss = 0.02749572
Iteration 125, loss = 0.02802333
Iteration 126, loss = 0.02824538
Iteration 127, loss = 0.02763981
Iteration 128, loss = 0.02655442
Iteration 129, loss = 0.02594195
Iteration 130, loss = 0.02574508
Iteration 131, loss = 0.02544735
Iteration 132, loss = 0.02470792
Iteration 133, loss = 0.02436552
Iteration 134, loss = 0.02409792
Iteration 135, loss = 0.02388374
Iteration 136, loss = 0.02359416
Iteration 137, loss = 0.02327964
Iteration 138, loss = 0.02310999
Iteration 139, loss = 0.02281489
Iteration 140, loss = 0.02256681
Iteration 141, loss = 0.02239629
Iteration 142, loss = 0.02207246
Iteration 143, loss = 0.02212439
Iteration 144, loss = 0.02154516
Iteration 145, loss = 0.02132681
Iteration 146, loss = 0.02163823
Iteration 147, loss = 0.02244315
Iteration 148, loss = 0.02084916
Iteration 149, loss = 0.02031025
Iteration 150, loss = 0.02093417
Iteration 151, loss = 0.01995579
Iteration 152, loss = 0.01954515
Iteration 153, loss = 0.01945536
Iteration 154, loss = 0.01937347
Iteration 155, loss = 0.01918736
Iteration 156, loss = 0.01891974
Iteration 157, loss = 0.01867452
Iteration 158, loss = 0.01862143
Iteration 159, loss = 0.01829281
Iteration 160, loss = 0.01792307
Iteration 161, loss = 0.01774559
Iteration 162, loss = 0.01792845
Iteration 163, loss = 0.01756123
Iteration 164, loss = 0.01738433
Iteration 165, loss = 0.01707992
Iteration 166, loss = 0.01746866
Iteration 167, loss = 0.01708988
Iteration 168, loss = 0.01676208
Iteration 169, loss = 0.01640214
Iteration 170, loss = 0.01617828
Iteration 171, loss = 0.01617482
Iteration 172, loss = 0.01594469
Iteration 173, loss = 0.01576154
Iteration 174, loss = 0.01571854
Iteration 175, loss = 0.01584081
Iteration 176, loss = 0.01565500
Iteration 177, loss = 0.01535502
Iteration 178, loss = 0.01509467
Iteration 179, loss = 0.01490287
Iteration 180, loss = 0.01478051
Iteration 181, loss = 0.01465711
Iteration 182, loss = 0.01451242
Iteration 183, loss = 0.01444759
Iteration 184, loss = 0.01431519
Iteration 185, loss = 0.01417540
Iteration 186, loss = 0.01402733
Iteration 187, loss = 0.01390800
Iteration 188, loss = 0.01382941
Iteration 189, loss = 0.01374850
Iteration 190, loss = 0.01368843
Iteration 191, loss = 0.01354718
Iteration 192, loss = 0.01339338
Iteration 193, loss = 0.01336893
Iteration 194, loss = 0.01344191
Iteration 195, loss = 0.01328393
Iteration 196, loss = 0.01310835
Iteration 197, loss = 0.01291687
Iteration 198, loss = 0.01286415
Iteration 199, loss = 0.01267374
Iteration 200, loss = 0.01256097
Iteration 201, loss = 0.01260285
Iteration 202, loss = 0.01273619
Iteration 203, loss = 0.01258204
Iteration 204, loss = 0.01244584
Iteration 205, loss = 0.01240058
Iteration 206, loss = 0.01239903
Iteration 207, loss = 0.01231915
Iteration 208, loss = 0.01237285
Iteration 209, loss = 0.01239616
Iteration 210, loss = 0.01232404
Iteration 211, loss = 0.01204597
Iteration 212, loss = 0.01169320
Iteration 213, loss = 0.01150677
Iteration 214, loss = 0.01142606
Iteration 215, loss = 0.01155594
Iteration 216, loss = 0.01162745
Iteration 217, loss = 0.01147829
Iteration 218, loss = 0.01125930
Iteration 219, loss = 0.01110506
Iteration 220, loss = 0.01094868
Iteration 221, loss = 0.01086388
Iteration 222, loss = 0.01088295
Iteration 223, loss = 0.01089157
Iteration 224, loss = 0.01081413
Iteration 225, loss = 0.01065712
Iteration 226, loss = 0.01051989
Iteration 227, loss = 0.01045571
Iteration 228, loss = 0.01036804
Iteration 229, loss = 0.01052414
Iteration 230, loss = 0.01068038
Iteration 231, loss = 0.01033048
Iteration 232, loss = 0.01010097
Iteration 233, loss = 0.01009156
Iteration 234, loss = 0.01007552
Iteration 235, loss = 0.01006862
Iteration 236, loss = 0.00995354
Iteration 237, loss = 0.00980702
Iteration 238, loss = 0.00986828
Iteration 239, loss = 0.00990737
Iteration 240, loss = 0.00974631
Iteration 241, loss = 0.00960406
Iteration 242, loss = 0.00951229
Iteration 243, loss = 0.00955950
Iteration 244, loss = 0.00943279
Iteration 245, loss = 0.00939253
Iteration 246, loss = 0.00932901
Iteration 247, loss = 0.00929386
Iteration 248, loss = 0.00927081
Iteration 249, loss = 0.00924163
Iteration 250, loss = 0.00915897
Iteration 251, loss = 0.00903608
Iteration 252, loss = 0.00899708
Iteration 253, loss = 0.00900187
Iteration 254, loss = 0.00890341
Iteration 255, loss = 0.00893569
Iteration 256, loss = 0.00882698
Iteration 257, loss = 0.00882195
Iteration 258, loss = 0.00872711
Iteration 259, loss = 0.00868936
Iteration 260, loss = 0.00863092
Iteration 261, loss = 0.00862229
Iteration 262, loss = 0.00857515
Iteration 263, loss = 0.00853356
Iteration 264, loss = 0.00851972
Iteration 265, loss = 0.00851692
Iteration 266, loss = 0.00850355
Iteration 267, loss = 0.00845070
Iteration 268, loss = 0.00838006
Iteration 269, loss = 0.00832186
Iteration 270, loss = 0.00818160
Iteration 271, loss = 0.00816005
Iteration 272, loss = 0.00819407
Iteration 273, loss = 0.00824927
Iteration 274, loss = 0.00816989
Iteration 275, loss = 0.00808992
Iteration 276, loss = 0.00800352
Iteration 277, loss = 0.00795666
Iteration 278, loss = 0.00791583
Iteration 279, loss = 0.00788081
Iteration 280, loss = 0.00785130
Iteration 281, loss = 0.00780083
Iteration 282, loss = 0.00775928
Iteration 283, loss = 0.00770020
Iteration 284, loss = 0.00770108
Iteration 285, loss = 0.00769624
Iteration 286, loss = 0.00762247
Iteration 287, loss = 0.00754027
Iteration 288, loss = 0.00751211
Iteration 289, loss = 0.00756706
Iteration 290, loss = 0.00762662
Iteration 291, loss = 0.00762439
Iteration 292, loss = 0.00757795
Iteration 293, loss = 0.00734921
Iteration 294, loss = 0.00735734
Iteration 295, loss = 0.00729656
Iteration 296, loss = 0.00729816
Iteration 297, loss = 0.00728675
Iteration 298, loss = 0.00722076
Iteration 299, loss = 0.00719196
Iteration 300, loss = 0.00709342
PARAMETROS DE INICIALIZACAO DA REDE
NUMERO DE NEURONIOS 
Camada de Entrada: 34
Camada Escondida: 3
Camada de Saida: 1

--- PARAMETROS DE CONFIGURACAO DA REDE ---
Numero de Epocas: 300
Taxa de Aprendizado: adaptive
Taxa de Aprendizado Inicial: 0.6
METRICAS

RESULTADOS:

[0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 1 1 1 1 1
 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 1 1 1
 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 1 0 0 1 1 0 1 1 1 0 1 1 1]
ACURACIA: 0.9056603773584906

