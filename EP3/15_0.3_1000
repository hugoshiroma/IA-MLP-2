Pesos Camada de Entrada: 
[[ 2.99221771e-02  8.57977062e-02 -2.32127595e-02  1.46586319e-01
  -2.00235831e-01 -1.46055886e-01  9.59362919e-02  1.83273470e-01
   3.18693102e-02 -1.37037354e-01 -1.34668684e-01  1.31157758e-01
  -2.95430071e-02  5.99013784e-02 -1.71973633e-01]
 [ 1.17640574e-01  1.16813759e-01 -1.24121266e-01  1.60532312e-01
  -1.61416297e-01  5.02624231e-02  9.11517264e-02 -1.61260232e-01
   1.86923325e-01 -1.55598968e-01 -4.71151977e-02 -2.01824767e-01
   6.63840196e-02 -1.26438304e-01 -1.06179624e-01]
 [ 1.50895923e-01 -1.26571994e-01 -6.93548533e-03  4.82759784e-02
  -3.64544361e-02 -9.98156462e-02  1.53707879e-01  1.29098211e-01
  -1.10991343e-01  1.13107376e-01 -1.28546748e-01 -1.74293011e-01
   1.36805671e-01 -3.48488637e-02  1.89652225e-01]
 [ 3.57258720e-02 -1.85781217e-01 -1.78379355e-01  7.35367743e-02
  -1.97103894e-01  2.00665438e-01 -8.14253234e-02  1.82169003e-02
   8.29990779e-02 -1.31673772e-01  1.39088383e-01  3.83099156e-02
  -1.69033012e-01  1.83979755e-01 -9.59225586e-02]
 [ 3.58379250e-02  9.99329179e-03 -1.57288467e-01  9.61790376e-02
   1.59652125e-01 -4.08045505e-02  5.95176328e-02 -6.23965496e-02
  -1.40258044e-01  9.56964419e-02 -1.71069275e-01 -1.41359498e-01
   1.51297635e-01  8.03126255e-02  7.06998278e-02]
 [-1.22303886e-01 -1.98110614e-01  9.37557544e-02  1.51902319e-01
  -1.51355290e-01 -8.36245029e-02  2.66378204e-02 -7.49071537e-02
   4.85214051e-02 -1.47258473e-01  1.48669068e-01 -1.70062866e-01
  -7.34874455e-02  1.63676354e-01  1.15409709e-01]
 [ 1.36157731e-01 -1.48411132e-01 -1.34497809e-01 -1.26993040e-01
   1.24713121e-01  6.27773128e-03  8.27279918e-02 -7.50077364e-02
  -1.70353204e-01  7.32709059e-02 -9.91267797e-02 -1.66145922e-01
  -4.58726084e-02 -3.07077714e-02 -1.19089433e-01]
 [ 1.85238676e-01 -1.98195598e-02  1.55827894e-01 -1.84711134e-01
   1.74803630e-01 -1.22191117e-01 -9.90751245e-02  7.13761508e-02
   1.42914221e-02 -1.37469917e-01  1.25917066e-02 -4.60037836e-03
  -6.15523754e-03  1.52530187e-01  1.28544834e-01]
 [-1.92219623e-01  4.72538026e-02 -1.62940305e-01 -8.26022722e-02
   1.72286793e-01  1.76115525e-01 -1.10720432e-01  9.35343335e-02
  -1.30552327e-01  1.65782896e-01 -5.85635482e-02 -1.48293099e-01
  -4.82815012e-02 -9.41017822e-02  6.73651488e-02]
 [ 1.87852870e-01 -1.79850260e-01  4.59095881e-02 -1.81771781e-01
  -1.40499149e-01  1.71341068e-01  2.01386281e-01 -5.98610775e-03
  -1.87775522e-01 -1.55938872e-03 -8.24913304e-02 -7.02245418e-02
   1.66852710e-01 -1.79129364e-01  1.63484610e-01]
 [-6.69777675e-02  1.35451471e-01 -1.05414220e-01 -9.47084561e-02
  -3.56928939e-02 -1.09567400e-01  1.62667233e-01 -1.11452498e-01
   1.18563678e-01 -3.70417465e-03 -3.19504756e-02  6.45881769e-02
   6.62278059e-02 -9.44125634e-02 -1.42009562e-01]
 [ 1.94007724e-01 -9.27792770e-02  9.53432664e-02  1.01433745e-01
  -1.31774641e-01 -4.35520871e-02  2.01381592e-01  1.70931110e-01
  -9.80962751e-02  1.26755981e-01  3.77145407e-02  4.88029181e-02
  -4.54135037e-02  4.91132881e-02 -7.49192728e-02]
 [ 1.11919902e-01  1.97020880e-01 -1.06343154e-01 -1.94875338e-01
   1.20191688e-01  8.45247982e-02 -1.72329223e-01  1.87111706e-02
  -7.84693005e-02 -1.14326156e-01 -1.21954036e-01 -4.44016409e-02
   9.02941449e-03  1.31483608e-01  1.84789524e-01]
 [ 1.89442956e-01 -1.29016179e-01 -4.51702551e-02 -4.03943957e-02
  -6.75834471e-02  1.35058120e-01 -1.43908373e-02  3.57556508e-02
   6.12388577e-02 -1.34939155e-01 -1.05792296e-01  4.37415547e-02
  -1.28863369e-01 -5.35696584e-02  1.87384543e-01]
 [-5.98750595e-02 -3.50466444e-02  1.98300913e-01 -7.54962606e-02
   4.65609680e-02  1.21207892e-01 -1.04594859e-01  2.73052586e-02
  -7.72199195e-02 -1.16321472e-01 -1.00210970e-01 -7.81813811e-02
   1.02241882e-01 -1.71006534e-01 -9.79842758e-02]
 [-1.52986315e-01  1.79313126e-01  1.49905797e-01 -6.16262930e-02
   7.97977166e-02 -6.78773382e-02  9.49507741e-02  1.09881228e-01
  -1.76477766e-02 -4.19679114e-02  1.44221182e-01  8.05546729e-02
  -1.86658316e-01 -4.92701091e-02 -1.16232241e-01]
 [ 1.41610225e-01 -1.69179549e-01 -1.06083413e-01 -1.17904676e-01
  -3.43269632e-02  4.54407149e-03  3.97881042e-02  1.76819096e-02
   1.66766378e-01  8.83390400e-02  1.26522796e-01 -9.93066505e-02
   1.93201233e-01 -1.37979453e-01 -1.78761561e-01]
 [-1.95376913e-01  8.26800777e-02  2.35843201e-02  1.73193597e-01
  -1.17353089e-01 -1.20951394e-01 -2.12842324e-02  1.48625849e-02
   4.91034538e-02  4.11714801e-02 -1.61540994e-01  1.94176634e-01
  -3.88201036e-02 -8.35561015e-02  7.97470754e-02]
 [ 1.44368866e-03  4.66314894e-02  4.87015315e-03  1.01857024e-01
  -1.23061140e-01 -2.68600576e-02 -6.49582532e-02  1.80626767e-01
  -6.85708848e-03  2.49457761e-02 -1.98347165e-01  6.09606938e-02
  -4.01167264e-02  4.38256023e-02  2.06662031e-02]
 [-1.04162078e-01 -1.52094677e-01  4.18469752e-03 -7.12145517e-02
  -1.53623942e-01  2.10021759e-02  9.80350822e-02 -1.95923436e-01
   1.30081897e-02 -9.51789326e-02  1.96755936e-01  1.38039820e-01
   1.35942423e-01  1.57736083e-01  1.72061135e-01]
 [ 1.27338606e-01  1.67198038e-01  9.26752129e-02 -1.93133672e-01
  -4.81395491e-02 -2.82576067e-02  4.90090684e-02 -1.33144949e-01
  -1.28445359e-01  6.32710530e-02  1.26925586e-01  7.14950920e-03
   1.99658376e-01  6.25626119e-02  1.87633959e-01]
 [-7.51117741e-02 -1.12007946e-01  1.86806949e-01  4.29159566e-03
  -9.48073399e-02  1.18752010e-01 -2.73778455e-02 -4.23377847e-02
   1.14361101e-01 -1.20735366e-01  9.93891775e-02 -3.33150015e-02
  -1.36835585e-01 -1.74283582e-02  1.23937856e-01]
 [ 6.50809603e-02 -7.96765152e-02  1.89106401e-01  1.18311652e-01
  -2.03848137e-03 -1.96080105e-01 -7.69434833e-02 -6.55980166e-02
   1.61726916e-01 -4.37192141e-03 -1.01998444e-02  1.67059515e-02
   1.50626027e-01 -2.87351381e-02 -1.20194461e-01]
 [ 7.21612935e-02  1.47343877e-01 -1.89147129e-01 -2.47670761e-02
   1.70366050e-02 -4.94308006e-02 -6.51507968e-02 -3.41398291e-02
  -7.85956968e-02 -1.83306327e-01 -1.56846224e-01 -1.92152352e-01
   1.22770899e-01  9.59603949e-02  5.36495177e-02]
 [-1.12728070e-01 -7.43292138e-02  1.38882182e-01 -1.62485244e-01
  -7.13252111e-02  1.96097532e-01 -1.14871461e-01  1.18600286e-01
  -4.91300892e-02  1.95847496e-01 -8.37410977e-02  1.41348932e-01
  -7.19659056e-02  1.18027443e-01 -2.14805713e-02]
 [-2.77944006e-02 -1.46490619e-01 -1.29117842e-01 -1.32353244e-01
   1.61477079e-02  1.89806173e-01 -1.30747240e-01  7.25382410e-02
  -4.58637534e-02 -5.44347344e-02 -2.33865818e-03  1.66282388e-01
   1.48151097e-01  1.60358572e-01  3.29724887e-02]
 [-2.71343231e-02  4.50006070e-02 -1.18123286e-01 -8.54351459e-02
  -1.16465973e-01  1.09017476e-01  1.86514701e-01  1.14085461e-01
  -6.40838040e-02 -1.92978442e-01 -1.06481724e-02 -1.71884296e-01
   4.98243423e-02 -1.54046218e-01 -1.84135381e-01]
 [-1.87550859e-01 -1.74878216e-01  7.89553374e-02 -7.25112118e-02
  -3.75178374e-02  5.30048159e-02 -1.43371235e-01  1.75728354e-01
  -9.52423941e-02  1.42711259e-01 -1.73664237e-02  1.43172888e-01
   1.20066981e-01  1.48624105e-01  7.20548455e-02]
 [ 7.22200433e-02  1.79118791e-01  2.70879024e-02 -4.24806235e-02
   1.19924355e-01  1.96667475e-01  6.48558685e-03  9.74426749e-02
   1.92987826e-01 -1.51526898e-01 -2.53436573e-02 -2.58188126e-02
   1.83060299e-01  1.17019175e-01  1.23661989e-01]
 [ 7.71163395e-02  2.63881817e-02  8.98781575e-02 -1.72297941e-02
   8.65057639e-02 -2.17100535e-02  1.20324522e-01 -1.32524764e-01
   1.67536516e-01  3.73980419e-02  2.13788003e-02 -9.49156724e-03
   1.32269065e-01  9.07162500e-02  5.85647085e-02]
 [-8.94555149e-02 -2.51905646e-02 -1.02368338e-01  1.49674387e-01
  -1.46044502e-01  6.69958150e-02  4.86953295e-02 -4.60230512e-02
   1.71262266e-01  1.76617160e-01  1.75628048e-01 -1.30337986e-01
  -3.51456101e-02  5.13863037e-02  1.50149963e-01]
 [ 1.19938335e-02 -4.54664656e-02 -1.63166519e-01 -1.29459662e-02
  -5.84060387e-02  1.05438014e-01  5.82785103e-02 -6.91287501e-02
   5.60272330e-02 -1.26796972e-02 -1.28400465e-01  3.38008512e-02
   1.37761114e-01 -1.95296485e-01 -1.33480199e-01]
 [-1.86301062e-01  1.89381447e-01 -5.90509775e-02  1.69670514e-01
  -1.26748843e-01 -8.63225648e-02  1.50043959e-02  1.98176648e-01
  -1.48484469e-01 -1.42641556e-01  6.20084734e-02 -1.60179944e-01
  -1.38466471e-01  4.45095802e-02 -1.08977584e-01]
 [ 3.00612315e-02 -1.14236925e-01 -1.28063999e-01  1.11227694e-01
   1.40115210e-01  1.42924420e-01 -1.57123087e-01 -1.87544148e-01
  -1.86425804e-01  3.17964117e-02 -9.85691084e-02  1.54231575e-04
   1.04993314e-01  7.05650754e-02  4.57993673e-02]]
Bias Camada de Entrada: 
[ 0.15069658 -0.06102163  0.05745329  0.07682207  0.11079859  0.05790515
 -0.12410222 -0.06471542  0.05361112 -0.10270936  0.00983203 -0.00582327
  0.19745512  0.09599658 -0.13134927]
Pesos Camada Escondida: 
[[-0.34423325]
 [-0.0270447 ]
 [ 0.02732172]
 [-0.06808523]
 [-0.24722285]
 [-0.01581741]
 [-0.26032552]
 [-0.05149084]
 [ 0.10316757]
 [ 0.03252964]
 [-0.10019217]
 [-0.13853632]
 [ 0.08253067]
 [ 0.16413984]
 [-0.08911107]]
Bias Camada Escondida: 
[-0.1884204]
Iteration 1, loss = 0.80800154
Iteration 2, loss = 0.65966624
Iteration 3, loss = 0.69079704
Iteration 4, loss = 0.64395693
Iteration 5, loss = 0.62804328
Iteration 6, loss = 0.61974360
Iteration 7, loss = 0.60180663
Iteration 8, loss = 0.58568595
Iteration 9, loss = 0.57112883
Iteration 10, loss = 0.53466341
Iteration 11, loss = 0.50930486
Iteration 12, loss = 0.47689837
Iteration 13, loss = 0.45094810
Iteration 14, loss = 0.42549405
Iteration 15, loss = 0.40766440
Iteration 16, loss = 0.38055097
Iteration 17, loss = 0.35817391
Iteration 18, loss = 0.34463869
Iteration 19, loss = 0.33131521
Iteration 20, loss = 0.31980306
Iteration 21, loss = 0.31256819
Iteration 22, loss = 0.30367369
Iteration 23, loss = 0.29953419
Iteration 24, loss = 0.29822093
Iteration 25, loss = 0.29041114
Iteration 26, loss = 0.28179610
Iteration 27, loss = 0.27555112
Iteration 28, loss = 0.26977325
Iteration 29, loss = 0.26423455
Iteration 30, loss = 0.25884410
Iteration 31, loss = 0.25416048
Iteration 32, loss = 0.24963367
Iteration 33, loss = 0.24756442
Iteration 34, loss = 0.24862957
Iteration 35, loss = 0.24233198
Iteration 36, loss = 0.23274263
Iteration 37, loss = 0.22942790
Iteration 38, loss = 0.22578974
Iteration 39, loss = 0.22135131
Iteration 40, loss = 0.21766830
Iteration 41, loss = 0.21141061
Iteration 42, loss = 0.21786600
Iteration 43, loss = 0.20446407
Iteration 44, loss = 0.20630482
Iteration 45, loss = 0.19581879
Iteration 46, loss = 0.18952058
Iteration 47, loss = 0.18517590
Iteration 48, loss = 0.18182834
Iteration 49, loss = 0.17694696
Iteration 50, loss = 0.17435965
Iteration 51, loss = 0.17327003
Iteration 52, loss = 0.16679501
Iteration 53, loss = 0.16508187
Iteration 54, loss = 0.16138871
Iteration 55, loss = 0.15665613
Iteration 56, loss = 0.15404585
Iteration 57, loss = 0.15050408
Iteration 58, loss = 0.14796049
Iteration 59, loss = 0.14563624
Iteration 60, loss = 0.14296285
Iteration 61, loss = 0.14095213
Iteration 62, loss = 0.13913201
Iteration 63, loss = 0.13551473
Iteration 64, loss = 0.13351000
Iteration 65, loss = 0.13189645
Iteration 66, loss = 0.13004118
Iteration 67, loss = 0.12934218
Iteration 68, loss = 0.12699515
Iteration 69, loss = 0.12719228
Iteration 70, loss = 0.12891443
Iteration 71, loss = 0.12382303
Iteration 72, loss = 0.11955651
Iteration 73, loss = 0.11825473
Iteration 74, loss = 0.11632681
Iteration 75, loss = 0.11589259
Iteration 76, loss = 0.11908000
Iteration 77, loss = 0.11333194
Iteration 78, loss = 0.11075368
Iteration 79, loss = 0.10974680
Iteration 80, loss = 0.10872934
Iteration 81, loss = 0.10686204
Iteration 82, loss = 0.10571330
Iteration 83, loss = 0.10468962
Iteration 84, loss = 0.10462084
Iteration 85, loss = 0.10220065
Iteration 86, loss = 0.10049568
Iteration 87, loss = 0.09979720
Iteration 88, loss = 0.09873273
Iteration 89, loss = 0.09762840
Iteration 90, loss = 0.09767750
Iteration 91, loss = 0.09665301
Iteration 92, loss = 0.09433628
Iteration 93, loss = 0.09283709
Iteration 94, loss = 0.09233426
Iteration 95, loss = 0.09091988
Iteration 96, loss = 0.08990077
Iteration 97, loss = 0.08880934
Iteration 98, loss = 0.08889615
Iteration 99, loss = 0.08835771
Iteration 100, loss = 0.08933869
Iteration 101, loss = 0.08523034
Iteration 102, loss = 0.08579542
Iteration 103, loss = 0.08434988
Iteration 104, loss = 0.08365601
Iteration 105, loss = 0.08257443
Iteration 106, loss = 0.08099773
Iteration 107, loss = 0.07981466
Iteration 108, loss = 0.07871131
Iteration 109, loss = 0.07835628
Iteration 110, loss = 0.07802392
Iteration 111, loss = 0.07688962
Iteration 112, loss = 0.07616611
Iteration 113, loss = 0.07580800
Iteration 114, loss = 0.07499513
Iteration 115, loss = 0.07316445
Iteration 116, loss = 0.07273596
Iteration 117, loss = 0.07270386
Iteration 118, loss = 0.07287538
Iteration 119, loss = 0.07325043
Iteration 120, loss = 0.07199363
Iteration 121, loss = 0.07072954
Iteration 122, loss = 0.06955097
Iteration 123, loss = 0.06779176
Iteration 124, loss = 0.06725740
Iteration 125, loss = 0.06735594
Iteration 126, loss = 0.06795534
Iteration 127, loss = 0.06774463
Iteration 128, loss = 0.06538202
Iteration 129, loss = 0.06469776
Iteration 130, loss = 0.06466828
Iteration 131, loss = 0.06353202
Iteration 132, loss = 0.06444580
Iteration 133, loss = 0.06279764
Iteration 134, loss = 0.06173415
Iteration 135, loss = 0.06102047
Iteration 136, loss = 0.06004287
Iteration 137, loss = 0.05949416
Iteration 138, loss = 0.05878392
Iteration 139, loss = 0.05888512
Iteration 140, loss = 0.05804778
Iteration 141, loss = 0.05782106
Iteration 142, loss = 0.05732393
Iteration 143, loss = 0.05636393
Iteration 144, loss = 0.05562823
Iteration 145, loss = 0.05510471
Iteration 146, loss = 0.05503628
Iteration 147, loss = 0.05514618
Iteration 148, loss = 0.05483373
Iteration 149, loss = 0.05403643
Iteration 150, loss = 0.05323378
Iteration 151, loss = 0.05201585
Iteration 152, loss = 0.05183764
Iteration 153, loss = 0.05227288
Iteration 154, loss = 0.05086875
Iteration 155, loss = 0.05047423
Iteration 156, loss = 0.05005613
Iteration 157, loss = 0.04955442
Iteration 158, loss = 0.04970825
Iteration 159, loss = 0.04946736
Iteration 160, loss = 0.04898713
Iteration 161, loss = 0.04841945
Iteration 162, loss = 0.04769477
Iteration 163, loss = 0.04705597
Iteration 164, loss = 0.04687109
Iteration 165, loss = 0.04647549
Iteration 166, loss = 0.04632523
Iteration 167, loss = 0.04648925
Iteration 168, loss = 0.04555616
Iteration 169, loss = 0.04468624
Iteration 170, loss = 0.04424011
Iteration 171, loss = 0.04463185
Iteration 172, loss = 0.04430964
Iteration 173, loss = 0.04297959
Iteration 174, loss = 0.04280320
Iteration 175, loss = 0.04223976
Iteration 176, loss = 0.04127976
Iteration 177, loss = 0.04113555
Iteration 178, loss = 0.04035128
Iteration 179, loss = 0.03993233
Iteration 180, loss = 0.03996869
Iteration 181, loss = 0.03959922
Iteration 182, loss = 0.03923457
Iteration 183, loss = 0.03895416
Iteration 184, loss = 0.03843955
Iteration 185, loss = 0.03840554
Iteration 186, loss = 0.03822381
Iteration 187, loss = 0.03830879
Iteration 188, loss = 0.03693381
Iteration 189, loss = 0.03685302
Iteration 190, loss = 0.03682057
Iteration 191, loss = 0.03604262
Iteration 192, loss = 0.03541110
Iteration 193, loss = 0.03496562
Iteration 194, loss = 0.03462944
Iteration 195, loss = 0.03442448
Iteration 196, loss = 0.03417611
Iteration 197, loss = 0.03365419
Iteration 198, loss = 0.03335994
Iteration 199, loss = 0.03347044
Iteration 200, loss = 0.03304196
Iteration 201, loss = 0.03277634
Iteration 202, loss = 0.03277123
Iteration 203, loss = 0.03240794
Iteration 204, loss = 0.03287299
Iteration 205, loss = 0.03138343
Iteration 206, loss = 0.03228915
Iteration 207, loss = 0.03143505
Iteration 208, loss = 0.03117881
Iteration 209, loss = 0.03030548
Iteration 210, loss = 0.03032232
Iteration 211, loss = 0.02969773
Iteration 212, loss = 0.02924586
Iteration 213, loss = 0.02893580
Iteration 214, loss = 0.02874306
Iteration 215, loss = 0.02851390
Iteration 216, loss = 0.02821671
Iteration 217, loss = 0.02802627
Iteration 218, loss = 0.02781477
Iteration 219, loss = 0.02757572
Iteration 220, loss = 0.02744931
Iteration 221, loss = 0.02733429
Iteration 222, loss = 0.02691369
Iteration 223, loss = 0.02651156
Iteration 224, loss = 0.02630508
Iteration 225, loss = 0.02649054
Iteration 226, loss = 0.02591586
Iteration 227, loss = 0.02588615
Iteration 228, loss = 0.02641877
Iteration 229, loss = 0.02540891
Iteration 230, loss = 0.02533230
Iteration 231, loss = 0.02503434
Iteration 232, loss = 0.02476943
Iteration 233, loss = 0.02561687
Iteration 234, loss = 0.02493607
Iteration 235, loss = 0.02393547
Iteration 236, loss = 0.02400290
Iteration 237, loss = 0.02370749
Iteration 238, loss = 0.02359985
Iteration 239, loss = 0.02343996
Iteration 240, loss = 0.02320087
Iteration 241, loss = 0.02295875
Iteration 242, loss = 0.02272668
Iteration 243, loss = 0.02259158
Iteration 244, loss = 0.02218308
Iteration 245, loss = 0.02212386
Iteration 246, loss = 0.02210866
Iteration 247, loss = 0.02184177
Iteration 248, loss = 0.02164253
Iteration 249, loss = 0.02160783
Iteration 250, loss = 0.02180155
Iteration 251, loss = 0.02217929
Iteration 252, loss = 0.02216211
Iteration 253, loss = 0.02177229
Iteration 254, loss = 0.02132744
Iteration 255, loss = 0.02097983
Iteration 256, loss = 0.02085951
Iteration 257, loss = 0.02073292
Iteration 258, loss = 0.02049108
Iteration 259, loss = 0.02000685
Iteration 260, loss = 0.01968645
Iteration 261, loss = 0.02010294
Iteration 262, loss = 0.02003603
Iteration 263, loss = 0.01959872
Iteration 264, loss = 0.01914366
Iteration 265, loss = 0.01906406
Iteration 266, loss = 0.01913416
Iteration 267, loss = 0.01923139
Iteration 268, loss = 0.01870013
Iteration 269, loss = 0.01846309
Iteration 270, loss = 0.01830843
Iteration 271, loss = 0.01828135
Iteration 272, loss = 0.01819521
Iteration 273, loss = 0.01820109
Iteration 274, loss = 0.01805683
Iteration 275, loss = 0.01795725
Iteration 276, loss = 0.01774808
Iteration 277, loss = 0.01756992
Iteration 278, loss = 0.01755318
Iteration 279, loss = 0.01733114
Iteration 280, loss = 0.01732326
Iteration 281, loss = 0.01720723
Iteration 282, loss = 0.01707505
Iteration 283, loss = 0.01699121
Iteration 284, loss = 0.01707776
Iteration 285, loss = 0.01707116
Iteration 286, loss = 0.01683675
Iteration 287, loss = 0.01647568
Iteration 288, loss = 0.01630982
Iteration 289, loss = 0.01612812
Iteration 290, loss = 0.01606718
Iteration 291, loss = 0.01598411
Iteration 292, loss = 0.01569148
Iteration 293, loss = 0.01574439
Iteration 294, loss = 0.01576581
Iteration 295, loss = 0.01565670
Iteration 296, loss = 0.01552726
Iteration 297, loss = 0.01539498
Iteration 298, loss = 0.01524170
Iteration 299, loss = 0.01506271
Iteration 300, loss = 0.01501329
Iteration 301, loss = 0.01516314
Iteration 302, loss = 0.01543380
Iteration 303, loss = 0.01546461
Iteration 304, loss = 0.01492741
Iteration 305, loss = 0.01484779
Iteration 306, loss = 0.01457630
Iteration 307, loss = 0.01450802
Iteration 308, loss = 0.01427683
Iteration 309, loss = 0.01418419
Iteration 310, loss = 0.01411027
Iteration 311, loss = 0.01399496
Iteration 312, loss = 0.01391374
Iteration 313, loss = 0.01386972
Iteration 314, loss = 0.01373098
Iteration 315, loss = 0.01377143
Iteration 316, loss = 0.01379943
Iteration 317, loss = 0.01380530
Iteration 318, loss = 0.01369532
Iteration 319, loss = 0.01340700
Iteration 320, loss = 0.01333266
Iteration 321, loss = 0.01340852
Iteration 322, loss = 0.01343535
Iteration 323, loss = 0.01347605
Iteration 324, loss = 0.01313577
Iteration 325, loss = 0.01296023
Iteration 326, loss = 0.01292600
Iteration 327, loss = 0.01288977
Iteration 328, loss = 0.01283364
Iteration 329, loss = 0.01289426
Iteration 330, loss = 0.01267772
Iteration 331, loss = 0.01262983
Iteration 332, loss = 0.01248255
Iteration 333, loss = 0.01236123
Iteration 334, loss = 0.01233781
Iteration 335, loss = 0.01229542
Iteration 336, loss = 0.01219650
Iteration 337, loss = 0.01212194
Iteration 338, loss = 0.01201674
Iteration 339, loss = 0.01198011
Iteration 340, loss = 0.01196190
Iteration 341, loss = 0.01184044
Iteration 342, loss = 0.01175485
Iteration 343, loss = 0.01175731
Iteration 344, loss = 0.01161945
Iteration 345, loss = 0.01157878
Iteration 346, loss = 0.01158686
Iteration 347, loss = 0.01157301
Iteration 348, loss = 0.01150887
Iteration 349, loss = 0.01143399
Iteration 350, loss = 0.01138214
Iteration 351, loss = 0.01133545
Iteration 352, loss = 0.01125787
Iteration 353, loss = 0.01118931
Iteration 354, loss = 0.01104468
Iteration 355, loss = 0.01101060
Iteration 356, loss = 0.01095731
Iteration 357, loss = 0.01089161
Iteration 358, loss = 0.01088372
Iteration 359, loss = 0.01082141
Iteration 360, loss = 0.01082923
Iteration 361, loss = 0.01079544
Iteration 362, loss = 0.01067704
Iteration 363, loss = 0.01056518
Iteration 364, loss = 0.01048489
Iteration 365, loss = 0.01042495
Iteration 366, loss = 0.01038513
Iteration 367, loss = 0.01042466
Iteration 368, loss = 0.01042656
Iteration 369, loss = 0.01038927
Iteration 370, loss = 0.01037040
Iteration 371, loss = 0.01022754
Iteration 372, loss = 0.01008217
Iteration 373, loss = 0.01006561
Iteration 374, loss = 0.01000394
Iteration 375, loss = 0.00997145
Iteration 376, loss = 0.00990767
Iteration 377, loss = 0.00987326
Iteration 378, loss = 0.00981440
Iteration 379, loss = 0.00987025
Iteration 380, loss = 0.00999412
Iteration 381, loss = 0.00995351
Iteration 382, loss = 0.00984627
Iteration 383, loss = 0.00974413
Iteration 384, loss = 0.00962959
Iteration 385, loss = 0.00957744
Iteration 386, loss = 0.00950477
Iteration 387, loss = 0.00948839
Iteration 388, loss = 0.00941975
Iteration 389, loss = 0.00941229
Iteration 390, loss = 0.00944613
Iteration 391, loss = 0.00948366
Iteration 392, loss = 0.00932522
Iteration 393, loss = 0.00923914
Iteration 394, loss = 0.00920399
Iteration 395, loss = 0.00917398
Iteration 396, loss = 0.00917443
Iteration 397, loss = 0.00916314
Iteration 398, loss = 0.00911082
Iteration 399, loss = 0.00904693
Iteration 400, loss = 0.00898430
Iteration 401, loss = 0.00895405
Iteration 402, loss = 0.00897881
Iteration 403, loss = 0.00908007
Iteration 404, loss = 0.00912991
Iteration 405, loss = 0.00896193
Iteration 406, loss = 0.00884942
Iteration 407, loss = 0.00876492
Iteration 408, loss = 0.00871059
Iteration 409, loss = 0.00867195
Iteration 410, loss = 0.00864048
Iteration 411, loss = 0.00858751
Iteration 412, loss = 0.00854453
Iteration 413, loss = 0.00850659
Iteration 414, loss = 0.00851231
Iteration 415, loss = 0.00847313
Iteration 416, loss = 0.00845434
Iteration 417, loss = 0.00843083
Iteration 418, loss = 0.00840614
Iteration 419, loss = 0.00838623
Iteration 420, loss = 0.00842306
Iteration 421, loss = 0.00823738
Iteration 422, loss = 0.00823031
Iteration 423, loss = 0.00820125
Iteration 424, loss = 0.00813000
Iteration 425, loss = 0.00808883
Iteration 426, loss = 0.00805705
Iteration 427, loss = 0.00804012
Iteration 428, loss = 0.00800797
Iteration 429, loss = 0.00795781
Iteration 430, loss = 0.00792021
Iteration 431, loss = 0.00794212
Iteration 432, loss = 0.00783935
Iteration 433, loss = 0.00781853
Iteration 434, loss = 0.00780145
Iteration 435, loss = 0.00773072
Iteration 436, loss = 0.00769481
Iteration 437, loss = 0.00769752
Iteration 438, loss = 0.00771320
Iteration 439, loss = 0.00773886
Iteration 440, loss = 0.00764786
Iteration 441, loss = 0.00759755
Iteration 442, loss = 0.00757224
Iteration 443, loss = 0.00754789
Iteration 444, loss = 0.00753249
Iteration 445, loss = 0.00754844
Iteration 446, loss = 0.00746849
Iteration 447, loss = 0.00741405
Iteration 448, loss = 0.00738192
Iteration 449, loss = 0.00736082
Iteration 450, loss = 0.00734351
Iteration 451, loss = 0.00735199
Iteration 452, loss = 0.00725503
Iteration 453, loss = 0.00720177
Iteration 454, loss = 0.00724961
Iteration 455, loss = 0.00732935
Iteration 456, loss = 0.00743776
Iteration 457, loss = 0.00745367
Iteration 458, loss = 0.00731407
Iteration 459, loss = 0.00716183
Iteration 460, loss = 0.00701808
Iteration 461, loss = 0.00698659
Iteration 462, loss = 0.00701132
Iteration 463, loss = 0.00704311
Iteration 464, loss = 0.00709140
Iteration 465, loss = 0.00705969
Iteration 466, loss = 0.00701907
Iteration 467, loss = 0.00698457
Iteration 468, loss = 0.00692969
Iteration 469, loss = 0.00684948
Iteration 470, loss = 0.00686364
Iteration 471, loss = 0.00674952
Iteration 472, loss = 0.00674507
Iteration 473, loss = 0.00674013
Iteration 474, loss = 0.00672087
Iteration 475, loss = 0.00666286
Iteration 476, loss = 0.00662982
Iteration 477, loss = 0.00664585
Iteration 478, loss = 0.00656466
Iteration 479, loss = 0.00654707
Iteration 480, loss = 0.00653678
Iteration 481, loss = 0.00652346
Iteration 482, loss = 0.00648378
Iteration 483, loss = 0.00645073
Iteration 484, loss = 0.00640487
Iteration 485, loss = 0.00638064
Iteration 486, loss = 0.00642263
Iteration 487, loss = 0.00633751
Iteration 488, loss = 0.00639045
Iteration 489, loss = 0.00637744
Iteration 490, loss = 0.00635476
Iteration 491, loss = 0.00631259
Iteration 492, loss = 0.00624524
Iteration 493, loss = 0.00622431
Iteration 494, loss = 0.00627166
Iteration 495, loss = 0.00621429
Iteration 496, loss = 0.00619310
Iteration 497, loss = 0.00616597
Iteration 498, loss = 0.00616575
Iteration 499, loss = 0.00610855
Iteration 500, loss = 0.00607765
Iteration 501, loss = 0.00604953
Iteration 502, loss = 0.00605517
Iteration 503, loss = 0.00601114
Iteration 504, loss = 0.00599151
Iteration 505, loss = 0.00598746
Iteration 506, loss = 0.00595765
Iteration 507, loss = 0.00595786
Iteration 508, loss = 0.00595864
Iteration 509, loss = 0.00593582
Iteration 510, loss = 0.00592753
Iteration 511, loss = 0.00587133
Iteration 512, loss = 0.00582872
Iteration 513, loss = 0.00581952
Iteration 514, loss = 0.00582165
Iteration 515, loss = 0.00589928
Iteration 516, loss = 0.00579853
Iteration 517, loss = 0.00575541
Iteration 518, loss = 0.00572303
Iteration 519, loss = 0.00573712
Iteration 520, loss = 0.00572222
Iteration 521, loss = 0.00573845
Iteration 522, loss = 0.00575828
Iteration 523, loss = 0.00575311
Iteration 524, loss = 0.00570418
Iteration 525, loss = 0.00566759
Iteration 526, loss = 0.00565943
Iteration 527, loss = 0.00561842
Iteration 528, loss = 0.00560194
Iteration 529, loss = 0.00558495
Iteration 530, loss = 0.00556341
Iteration 531, loss = 0.00552449
Iteration 532, loss = 0.00549807
Iteration 533, loss = 0.00547485
Iteration 534, loss = 0.00546611
Iteration 535, loss = 0.00546921
Iteration 536, loss = 0.00546685
Iteration 537, loss = 0.00546691
Iteration 538, loss = 0.00544524
Iteration 539, loss = 0.00540085
Iteration 540, loss = 0.00537918
Iteration 541, loss = 0.00538337
Iteration 542, loss = 0.00534860
Iteration 543, loss = 0.00535490
Iteration 544, loss = 0.00534702
Iteration 545, loss = 0.00529507
Iteration 546, loss = 0.00524826
Iteration 547, loss = 0.00524260
Iteration 548, loss = 0.00526688
Iteration 549, loss = 0.00530460
Iteration 550, loss = 0.00532004
Iteration 551, loss = 0.00527536
Iteration 552, loss = 0.00523495
Iteration 553, loss = 0.00519831
Iteration 554, loss = 0.00515900
Iteration 555, loss = 0.00512292
Iteration 556, loss = 0.00512261
Iteration 557, loss = 0.00508956
Iteration 558, loss = 0.00508142
Iteration 559, loss = 0.00507252
Iteration 560, loss = 0.00506530
Iteration 561, loss = 0.00505798
Iteration 562, loss = 0.00504156
Iteration 563, loss = 0.00502596
Iteration 564, loss = 0.00501434
Iteration 565, loss = 0.00499308
Iteration 566, loss = 0.00499675
Iteration 567, loss = 0.00498697
Iteration 568, loss = 0.00497597
Iteration 569, loss = 0.00494691
Iteration 570, loss = 0.00493369
Iteration 571, loss = 0.00490982
Iteration 572, loss = 0.00489631
Iteration 573, loss = 0.00488250
Iteration 574, loss = 0.00488299
Iteration 575, loss = 0.00484690
Iteration 576, loss = 0.00483511
Iteration 577, loss = 0.00482190
Iteration 578, loss = 0.00482227
Iteration 579, loss = 0.00479435
Iteration 580, loss = 0.00477262
Iteration 581, loss = 0.00478163
Iteration 582, loss = 0.00478359
Iteration 583, loss = 0.00478024
Iteration 584, loss = 0.00474569
Iteration 585, loss = 0.00471281
Iteration 586, loss = 0.00470045
Iteration 587, loss = 0.00470653
Iteration 588, loss = 0.00471979
Iteration 589, loss = 0.00472564
Iteration 590, loss = 0.00470477
Iteration 591, loss = 0.00467629
Iteration 592, loss = 0.00464991
Iteration 593, loss = 0.00464323
Iteration 594, loss = 0.00462199
Iteration 595, loss = 0.00459869
Iteration 596, loss = 0.00458989
Iteration 597, loss = 0.00457346
Iteration 598, loss = 0.00456012
Iteration 599, loss = 0.00455757
Iteration 600, loss = 0.00453780
Iteration 601, loss = 0.00452595
Iteration 602, loss = 0.00452612
Iteration 603, loss = 0.00452732
Iteration 604, loss = 0.00450653
Iteration 605, loss = 0.00449414
Iteration 606, loss = 0.00447660
Iteration 607, loss = 0.00446707
Iteration 608, loss = 0.00445330
Iteration 609, loss = 0.00443507
Iteration 610, loss = 0.00442417
Iteration 611, loss = 0.00441212
Iteration 612, loss = 0.00439542
Iteration 613, loss = 0.00438711
Iteration 614, loss = 0.00438156
Iteration 615, loss = 0.00437346
Iteration 616, loss = 0.00436415
Iteration 617, loss = 0.00435334
Iteration 618, loss = 0.00437179
Iteration 619, loss = 0.00433398
Iteration 620, loss = 0.00430309
Iteration 621, loss = 0.00427956
Iteration 622, loss = 0.00432170
Iteration 623, loss = 0.00430823
Iteration 624, loss = 0.00430562
Iteration 625, loss = 0.00431098
Iteration 626, loss = 0.00429011
Iteration 627, loss = 0.00430275
Iteration 628, loss = 0.00425727
Iteration 629, loss = 0.00421985
Iteration 630, loss = 0.00419571
Iteration 631, loss = 0.00419197
Iteration 632, loss = 0.00420389
Iteration 633, loss = 0.00420033
Iteration 634, loss = 0.00420653
Iteration 635, loss = 0.00416547
Iteration 636, loss = 0.00413137
Iteration 637, loss = 0.00412035
Iteration 638, loss = 0.00412068
Iteration 639, loss = 0.00410481
Iteration 640, loss = 0.00408595
Iteration 641, loss = 0.00408230
Iteration 642, loss = 0.00409458
Iteration 643, loss = 0.00406429
Iteration 644, loss = 0.00405292
Iteration 645, loss = 0.00403936
Iteration 646, loss = 0.00403025
Iteration 647, loss = 0.00402310
Iteration 648, loss = 0.00401576
Iteration 649, loss = 0.00400559
Iteration 650, loss = 0.00399503
Iteration 651, loss = 0.00398590
Iteration 652, loss = 0.00397635
Iteration 653, loss = 0.00399515
Iteration 654, loss = 0.00396809
Iteration 655, loss = 0.00395642
Iteration 656, loss = 0.00394556
Iteration 657, loss = 0.00394349
Iteration 658, loss = 0.00393137
Iteration 659, loss = 0.00394642
Iteration 660, loss = 0.00391709
Iteration 661, loss = 0.00390430
Iteration 662, loss = 0.00389825
Iteration 663, loss = 0.00388281
Iteration 664, loss = 0.00387297
Iteration 665, loss = 0.00386271
Iteration 666, loss = 0.00385977
Iteration 667, loss = 0.00384301
Iteration 668, loss = 0.00383609
Iteration 669, loss = 0.00382566
Iteration 670, loss = 0.00382544
Iteration 671, loss = 0.00380939
Iteration 672, loss = 0.00381513
Iteration 673, loss = 0.00379894
Iteration 674, loss = 0.00379956
Iteration 675, loss = 0.00378736
Iteration 676, loss = 0.00378083
Iteration 677, loss = 0.00375983
Iteration 678, loss = 0.00375633
Iteration 679, loss = 0.00375776
Iteration 680, loss = 0.00376310
Iteration 681, loss = 0.00376368
Iteration 682, loss = 0.00374288
Iteration 683, loss = 0.00372571
Iteration 684, loss = 0.00370967
Iteration 685, loss = 0.00370468
Iteration 686, loss = 0.00368871
Iteration 687, loss = 0.00367988
Iteration 688, loss = 0.00367077
Iteration 689, loss = 0.00367270
Iteration 690, loss = 0.00366290
Iteration 691, loss = 0.00366067
Iteration 692, loss = 0.00365972
Iteration 693, loss = 0.00366357
Iteration 694, loss = 0.00366440
Iteration 695, loss = 0.00365247
Iteration 696, loss = 0.00362676
Iteration 697, loss = 0.00359567
Iteration 698, loss = 0.00357868
Iteration 699, loss = 0.00359418
Iteration 700, loss = 0.00359985
Iteration 701, loss = 0.00358917
Iteration 702, loss = 0.00357800
Iteration 703, loss = 0.00356447
Iteration 704, loss = 0.00354197
Iteration 705, loss = 0.00353278
Iteration 706, loss = 0.00352399
Iteration 707, loss = 0.00352191
Iteration 708, loss = 0.00351202
Iteration 709, loss = 0.00349989
Iteration 710, loss = 0.00349024
Iteration 711, loss = 0.00348237
Iteration 712, loss = 0.00347515
Iteration 713, loss = 0.00347192
Iteration 714, loss = 0.00345841
Iteration 715, loss = 0.00344962
Iteration 716, loss = 0.00345090
Iteration 717, loss = 0.00345527
Iteration 718, loss = 0.00344528
Iteration 719, loss = 0.00343592
Iteration 720, loss = 0.00343435
Iteration 721, loss = 0.00341327
Iteration 722, loss = 0.00340911
Iteration 723, loss = 0.00340455
Iteration 724, loss = 0.00339852
Iteration 725, loss = 0.00339238
Iteration 726, loss = 0.00338615
Iteration 727, loss = 0.00337369
Iteration 728, loss = 0.00336133
Iteration 729, loss = 0.00335421
Iteration 730, loss = 0.00334175
Iteration 731, loss = 0.00334675
Iteration 732, loss = 0.00334331
Iteration 733, loss = 0.00333586
Iteration 734, loss = 0.00332478
Iteration 735, loss = 0.00331893
Iteration 736, loss = 0.00331228
Iteration 737, loss = 0.00330657
Iteration 738, loss = 0.00329725
Iteration 739, loss = 0.00328784
Iteration 740, loss = 0.00328012
Iteration 741, loss = 0.00328160
Iteration 742, loss = 0.00326546
Iteration 743, loss = 0.00326637
Iteration 744, loss = 0.00326628
Iteration 745, loss = 0.00326097
Iteration 746, loss = 0.00325435
Iteration 747, loss = 0.00324606
Iteration 748, loss = 0.00323855
Iteration 749, loss = 0.00323354
Iteration 750, loss = 0.00322210
Iteration 751, loss = 0.00321199
Iteration 752, loss = 0.00320292
Iteration 753, loss = 0.00319507
Iteration 754, loss = 0.00319278
Iteration 755, loss = 0.00318523
Iteration 756, loss = 0.00317346
Iteration 757, loss = 0.00317173
Iteration 758, loss = 0.00317156
Iteration 759, loss = 0.00319154
Iteration 760, loss = 0.00318621
Iteration 761, loss = 0.00316523
Iteration 762, loss = 0.00314892
Iteration 763, loss = 0.00313884
Iteration 764, loss = 0.00313787
Iteration 765, loss = 0.00312138
Iteration 766, loss = 0.00311065
Iteration 767, loss = 0.00310713
Iteration 768, loss = 0.00310230
Iteration 769, loss = 0.00310346
Iteration 770, loss = 0.00309065
Iteration 771, loss = 0.00308300
Iteration 772, loss = 0.00307431
Iteration 773, loss = 0.00306655
Iteration 774, loss = 0.00306152
Iteration 775, loss = 0.00305494
Iteration 776, loss = 0.00305081
Iteration 777, loss = 0.00304023
Iteration 778, loss = 0.00303053
Iteration 779, loss = 0.00302080
Iteration 780, loss = 0.00301737
Iteration 781, loss = 0.00306290
Iteration 782, loss = 0.00303142
Iteration 783, loss = 0.00303023
Iteration 784, loss = 0.00300376
Iteration 785, loss = 0.00298900
Iteration 786, loss = 0.00298685
Iteration 787, loss = 0.00297947
Iteration 788, loss = 0.00298152
Iteration 789, loss = 0.00298733
Iteration 790, loss = 0.00299285
Iteration 791, loss = 0.00299596
Iteration 792, loss = 0.00299450
Iteration 793, loss = 0.00297655
Iteration 794, loss = 0.00295470
Iteration 795, loss = 0.00294089
Iteration 796, loss = 0.00293107
Iteration 797, loss = 0.00292112
Iteration 798, loss = 0.00291801
Iteration 799, loss = 0.00292088
Iteration 800, loss = 0.00293537
Iteration 801, loss = 0.00294415
Iteration 802, loss = 0.00295481
Iteration 803, loss = 0.00294807
Iteration 804, loss = 0.00293678
Iteration 805, loss = 0.00291961
Iteration 806, loss = 0.00290622
Iteration 807, loss = 0.00289376
Iteration 808, loss = 0.00287816
Iteration 809, loss = 0.00286704
Iteration 810, loss = 0.00285674
Iteration 811, loss = 0.00285499
Iteration 812, loss = 0.00284888
Iteration 813, loss = 0.00283909
Iteration 814, loss = 0.00283349
Iteration 815, loss = 0.00283075
Iteration 816, loss = 0.00282907
Iteration 817, loss = 0.00282551
Iteration 818, loss = 0.00282271
Iteration 819, loss = 0.00281032
Iteration 820, loss = 0.00279936
Iteration 821, loss = 0.00279352
Iteration 822, loss = 0.00278861
Iteration 823, loss = 0.00278765
Iteration 824, loss = 0.00279087
Iteration 825, loss = 0.00278431
Iteration 826, loss = 0.00277601
Iteration 827, loss = 0.00276903
Iteration 828, loss = 0.00276466
Iteration 829, loss = 0.00275713
Iteration 830, loss = 0.00275418
Iteration 831, loss = 0.00274673
Iteration 832, loss = 0.00274372
Iteration 833, loss = 0.00274443
Iteration 834, loss = 0.00274439
Iteration 835, loss = 0.00273498
Iteration 836, loss = 0.00273039
Iteration 837, loss = 0.00272603
Iteration 838, loss = 0.00272626
Iteration 839, loss = 0.00273095
Iteration 840, loss = 0.00272933
Iteration 841, loss = 0.00272684
Iteration 842, loss = 0.00272195
Iteration 843, loss = 0.00271676
Iteration 844, loss = 0.00269910
Iteration 845, loss = 0.00269801
Iteration 846, loss = 0.00268238
Iteration 847, loss = 0.00267692
Iteration 848, loss = 0.00267423
Iteration 849, loss = 0.00267182
Iteration 850, loss = 0.00266657
Iteration 851, loss = 0.00266407
Iteration 852, loss = 0.00265512
Iteration 853, loss = 0.00265110
Iteration 854, loss = 0.00264403
Iteration 855, loss = 0.00264574
Iteration 856, loss = 0.00263984
Iteration 857, loss = 0.00263548
Iteration 858, loss = 0.00262991
Iteration 859, loss = 0.00262239
Iteration 860, loss = 0.00261656
Iteration 861, loss = 0.00261787
Iteration 862, loss = 0.00260481
Iteration 863, loss = 0.00260407
Iteration 864, loss = 0.00259877
Iteration 865, loss = 0.00259664
Iteration 866, loss = 0.00258882
Iteration 867, loss = 0.00258425
Iteration 868, loss = 0.00258088
Iteration 869, loss = 0.00257286
Iteration 870, loss = 0.00256648
Iteration 871, loss = 0.00256194
Iteration 872, loss = 0.00255708
Iteration 873, loss = 0.00255494
Iteration 874, loss = 0.00255642
Iteration 875, loss = 0.00254521
Iteration 876, loss = 0.00254526
Iteration 877, loss = 0.00253768
Iteration 878, loss = 0.00253583
Iteration 879, loss = 0.00253426
Iteration 880, loss = 0.00253054
Iteration 881, loss = 0.00253012
Iteration 882, loss = 0.00252454
Iteration 883, loss = 0.00252878
Iteration 884, loss = 0.00251902
Iteration 885, loss = 0.00251359
Iteration 886, loss = 0.00250384
Iteration 887, loss = 0.00250019
Iteration 888, loss = 0.00249238
Iteration 889, loss = 0.00248673
Iteration 890, loss = 0.00248178
Iteration 891, loss = 0.00247691
Iteration 892, loss = 0.00247879
Iteration 893, loss = 0.00248571
Iteration 894, loss = 0.00248881
Iteration 895, loss = 0.00249174
Iteration 896, loss = 0.00248651
Iteration 897, loss = 0.00247347
Iteration 898, loss = 0.00246502
Iteration 899, loss = 0.00245447
Iteration 900, loss = 0.00245057
Iteration 901, loss = 0.00245154
Iteration 902, loss = 0.00244765
Iteration 903, loss = 0.00244068
Iteration 904, loss = 0.00243846
Iteration 905, loss = 0.00242417
Iteration 906, loss = 0.00241917
Iteration 907, loss = 0.00241556
Iteration 908, loss = 0.00240554
Iteration 909, loss = 0.00240603
Iteration 910, loss = 0.00239878
Iteration 911, loss = 0.00239513
Iteration 912, loss = 0.00239199
Iteration 913, loss = 0.00239068
Iteration 914, loss = 0.00238530
Iteration 915, loss = 0.00238487
Iteration 916, loss = 0.00236805
Iteration 917, loss = 0.00236604
Iteration 918, loss = 0.00236780
Iteration 919, loss = 0.00239120
Iteration 920, loss = 0.00241665
Iteration 921, loss = 0.00242086
Iteration 922, loss = 0.00241881
Iteration 923, loss = 0.00240845
Iteration 924, loss = 0.00238565
Iteration 925, loss = 0.00237258
Iteration 926, loss = 0.00234968
Iteration 927, loss = 0.00233876
Iteration 928, loss = 0.00232963
Iteration 929, loss = 0.00235031
Iteration 930, loss = 0.00232750
Iteration 931, loss = 0.00232136
Iteration 932, loss = 0.00231618
Iteration 933, loss = 0.00230939
Iteration 934, loss = 0.00230318
Iteration 935, loss = 0.00230716
Iteration 936, loss = 0.00229885
Iteration 937, loss = 0.00229510
Iteration 938, loss = 0.00229666
Iteration 939, loss = 0.00228997
Iteration 940, loss = 0.00228721
Iteration 941, loss = 0.00228748
Iteration 942, loss = 0.00228010
Iteration 943, loss = 0.00227579
Iteration 944, loss = 0.00227223
Iteration 945, loss = 0.00226602
Iteration 946, loss = 0.00226072
Iteration 947, loss = 0.00225989
Iteration 948, loss = 0.00225721
Iteration 949, loss = 0.00225431
Iteration 950, loss = 0.00225144
Iteration 951, loss = 0.00225086
Iteration 952, loss = 0.00224999
Iteration 953, loss = 0.00226167
Iteration 954, loss = 0.00224121
Iteration 955, loss = 0.00223622
Iteration 956, loss = 0.00222909
Iteration 957, loss = 0.00222802
Iteration 958, loss = 0.00222369
Iteration 959, loss = 0.00221884
Iteration 960, loss = 0.00221390
Iteration 961, loss = 0.00221114
Iteration 962, loss = 0.00220760
Iteration 963, loss = 0.00220441
Iteration 964, loss = 0.00220149
Iteration 965, loss = 0.00219830
Iteration 966, loss = 0.00219632
Iteration 967, loss = 0.00219306
Iteration 968, loss = 0.00219130
Iteration 969, loss = 0.00218859
Iteration 970, loss = 0.00218650
Iteration 971, loss = 0.00218587
Iteration 972, loss = 0.00217866
Iteration 973, loss = 0.00217597
Iteration 974, loss = 0.00216937
Iteration 975, loss = 0.00216940
Iteration 976, loss = 0.00216240
Iteration 977, loss = 0.00215993
Iteration 978, loss = 0.00215572
Iteration 979, loss = 0.00215403
Iteration 980, loss = 0.00215024
Iteration 981, loss = 0.00214862
Iteration 982, loss = 0.00214646
Iteration 983, loss = 0.00214145
Iteration 984, loss = 0.00214126
Iteration 985, loss = 0.00213626
Iteration 986, loss = 0.00213147
Iteration 987, loss = 0.00212901
Iteration 988, loss = 0.00212679
Iteration 989, loss = 0.00212369
Iteration 990, loss = 0.00212132
Iteration 991, loss = 0.00211685
Iteration 992, loss = 0.00211514
Iteration 993, loss = 0.00211499
Iteration 994, loss = 0.00211149
Iteration 995, loss = 0.00211026
Iteration 996, loss = 0.00210639
Iteration 997, loss = 0.00210401
Iteration 998, loss = 0.00210148
Iteration 999, loss = 0.00210018
Iteration 1000, loss = 0.00210185
PARAMETROS DE INICIALIZACAO DA REDE
NUMERO DE NEURONIOS 
Camada de Entrada: 34
Camada Escondida: 15
Camada de Saida: 1

--- PARAMETROS DE CONFIGURACAO DA REDE ---
Numero de Epocas: 1000
Taxa de Aprendizado: adaptive
Taxa de Aprendizado Inicial: 0.3
METRICAS

RESULTADOS:

[0 0 1 0 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1 1 1 1 1 1 1
 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 1 1 1
 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 1 0]
ACURACIA: 0.9339622641509434

