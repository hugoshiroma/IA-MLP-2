Pesos Camada de Entrada: 
[[-1.21956491e-01 -1.68100093e-01  3.85635526e-02 -1.68411373e-01
   6.54886450e-02  1.18754888e-01 -8.06229735e-02 -1.23814106e-01
   8.09330186e-02 -1.49408725e-01]
 [-1.55351468e-01  7.27082331e-02  2.07465269e-01  7.06374246e-02
   1.92773728e-01  1.68520067e-01 -1.92860252e-01  1.46415804e-01
   1.66762017e-01 -1.86878327e-01]
 [-1.06180171e-01 -5.96927261e-02 -2.76048633e-02  7.30655051e-02
  -1.19132758e-01  1.99319596e-01 -6.01389244e-02 -1.28034492e-01
  -8.86360630e-02 -1.47475807e-01]
 [ 5.24686221e-02  1.68392762e-01  1.66369228e-01 -2.44587029e-02
   9.08918933e-03 -1.08094906e-02  4.32076211e-02  7.37815117e-02
   6.87319610e-03  1.13178796e-01]
 [ 9.41434762e-02  8.75766283e-02 -7.68766338e-02  1.56971362e-02
  -5.11722807e-02  1.99113288e-01 -2.12765895e-01 -1.17558950e-01
  -2.71927423e-02 -8.43125733e-02]
 [-1.65617682e-01  7.99114048e-02  1.95883456e-01 -1.53169616e-01
  -3.35236687e-02  2.02334930e-01 -1.47962262e-01  1.18057016e-01
   4.25125075e-02  5.16654573e-02]
 [-1.01286155e-01  4.32841058e-02 -5.44892358e-04 -8.80373175e-02
   5.53840365e-02 -1.87622891e-01  2.04578215e-01  4.02783025e-02
  -2.08514622e-01 -1.31955764e-01]
 [ 5.64646395e-02  1.70265885e-01  1.73681458e-01 -1.64714377e-01
  -9.04312126e-02 -8.81355613e-02 -1.83463092e-01 -2.12187380e-01
  -6.42532308e-02 -6.61133523e-02]
 [ 2.05556845e-01  1.32432669e-01 -1.80867929e-01 -2.71317507e-02
  -1.45911213e-01  1.91081839e-01  5.17089069e-02 -2.11845847e-01
   3.50264570e-02  1.80870067e-01]
 [-1.01017254e-01  1.54086624e-01 -1.49742435e-01  1.54150204e-01
   8.89806516e-02  6.67736739e-02  1.35363248e-01 -3.28309565e-02
   1.39603874e-01  1.92732465e-01]
 [ 1.88467463e-01  3.69653278e-02 -6.30932923e-03  1.69639164e-02
  -1.94910999e-01 -1.28466003e-01 -1.91637754e-01 -1.69680784e-01
   8.74977142e-02  8.51493922e-03]
 [-1.92147788e-02  1.20989718e-01  8.25146325e-02  1.27115368e-01
   1.50578556e-01 -1.22370821e-01 -1.95022667e-01 -7.40735112e-02
  -1.90435133e-01  7.59818066e-02]
 [ 1.53432874e-03  1.65833675e-01 -1.61442506e-01  1.81028762e-01
  -1.36704286e-01  1.12360743e-01 -1.40908458e-01 -1.67690895e-01
  -2.31803658e-02 -3.60792606e-02]
 [-1.47045381e-01  1.99148706e-01 -6.05980485e-02 -9.26903173e-02
  -1.71392011e-01 -3.63115101e-02 -1.64007442e-01 -1.85061715e-01
  -1.33367750e-01 -3.17142758e-02]
 [ 5.75536673e-02  1.18522737e-01  8.80587116e-02 -1.14731043e-01
  -1.24364714e-01  1.72872718e-01  1.27304265e-01 -1.40171031e-01
   1.52707041e-01  2.04062166e-01]
 [ 2.05287278e-01 -1.17528414e-01  4.10856183e-02  1.49796116e-01
  -3.56733354e-02  5.69017421e-02 -8.67265754e-02 -1.87112989e-01
  -1.11479074e-02  3.76049400e-02]
 [ 1.25610291e-01  7.27594390e-02 -1.31321479e-01 -1.58899214e-01
  -1.65264610e-01 -2.30654732e-02 -8.77324770e-02 -1.17861769e-01
  -8.14224979e-02 -1.72704279e-01]
 [ 5.50961766e-02 -9.60204849e-02 -8.07286109e-02 -1.39568859e-01
   1.30941227e-01 -1.93236151e-01  1.16661963e-01  1.33337279e-01
   3.66531347e-02 -1.15202442e-01]
 [-1.97205291e-01  6.85641481e-02 -5.20859780e-02  2.10904481e-01
   2.12603715e-01  6.79467879e-02 -8.12106322e-02 -1.46936855e-02
   3.51892282e-02 -3.23833035e-02]
 [ 2.11494446e-01  1.79809022e-01  6.81746283e-02  1.46683090e-01
   7.09016378e-03 -1.24911460e-01 -8.05948735e-02  1.95346020e-01
   1.10028632e-02  1.54857968e-01]
 [ 1.52652078e-01  9.18795594e-02  3.65265363e-02 -1.06323470e-01
  -1.76463736e-01 -2.37517090e-02 -1.73674019e-01  4.76706940e-02
   3.50764224e-03  1.52133139e-01]
 [ 1.92643154e-01  1.73123528e-01 -7.46266487e-02 -1.13047100e-01
   1.01504272e-01 -8.70998714e-02  1.67796261e-01  1.29732332e-01
   1.64576108e-04  7.02395221e-03]
 [ 6.89578899e-02 -2.01755721e-01 -9.39739980e-02  1.42809586e-01
   1.71556479e-01 -1.08809289e-01  2.50851093e-02  6.72125270e-02
  -3.87380929e-02 -2.05545573e-01]
 [ 1.54600178e-01  7.88183950e-02 -3.04101717e-02  4.63314158e-02
   1.86654235e-01  1.62379932e-01  1.30098618e-01 -4.48626000e-02
  -4.94409793e-02  2.69293718e-02]
 [-1.52096767e-01 -8.27323972e-02 -1.77850925e-01 -1.03846843e-01
   1.77871167e-01 -2.10218585e-01  2.00893847e-01  8.65217243e-02
  -1.95496632e-01  2.04530925e-01]
 [ 1.40378810e-03 -1.44654246e-01 -2.03667080e-01  4.58800159e-02
  -1.73674700e-01  6.33708632e-03 -1.50796182e-01  1.26688310e-01
  -1.82213121e-01  9.11022669e-02]
 [ 7.47909652e-02 -1.84414825e-01  1.12795565e-01  1.13496883e-01
   1.93554968e-01  2.05150223e-01 -2.79716236e-02  6.81963384e-02
   1.06071662e-01 -1.05055355e-01]
 [-1.73863259e-01 -4.74784101e-02  1.67708674e-01 -1.22792688e-01
  -1.22335992e-01 -5.17720944e-02 -8.66139767e-03 -1.03773722e-01
  -1.83795045e-01 -5.97204708e-02]
 [-3.21025759e-02  1.00450870e-01  8.94950533e-02  1.04294801e-01
  -5.67747631e-02 -1.95765752e-01 -9.51684922e-02 -1.08634227e-01
   7.05037704e-02  1.33295738e-01]
 [ 1.14332478e-01  1.33625278e-01  1.71474565e-01  1.41424005e-01
  -2.04408097e-01 -1.81649918e-01 -8.92014619e-02 -9.37870463e-02
  -8.19155865e-02  1.47695080e-01]
 [ 1.02705126e-01 -1.41313348e-01 -5.80825905e-02  1.67000135e-01
   1.03763263e-02 -1.70395340e-01  8.52645193e-02  4.04679967e-02
   5.58095820e-02 -2.72627037e-05]
 [-1.92675293e-01 -1.86080238e-01 -3.02589765e-02 -3.57918587e-03
  -8.29233026e-02 -1.02611634e-01  1.55910250e-01  1.89972537e-01
  -1.91908784e-01  1.43790456e-01]
 [-1.14341256e-02 -7.51027837e-02  2.06887825e-02  2.00865581e-01
   1.83647468e-01 -1.81470149e-01 -1.96610113e-01  1.60241392e-01
  -1.26563016e-01 -6.85959412e-03]
 [ 6.80826041e-02  1.85087980e-01  3.78880459e-02 -1.82176133e-01
  -1.98935445e-01  5.71969462e-02 -1.69499096e-01  7.47280599e-02
   3.88680734e-02 -1.77269285e-01]]
Bias Camada de Entrada: 
[-0.04850358 -0.04348677 -0.04300737 -0.20883446  0.02054941 -0.200885
 -0.04077348  0.10443769 -0.04376135  0.10734682]
Pesos Camada Escondida: 
[[-0.08860921]
 [-0.01467248]
 [-0.10057763]
 [-0.30060223]
 [-0.07039607]
 [ 0.3079282 ]
 [ 0.36580251]
 [ 0.05627714]
 [ 0.20823081]
 [-0.08316681]]
Bias Camada Escondida: 
[-0.3903181]
Iteration 1, loss = 0.74622504
Iteration 2, loss = 0.66451616
Iteration 3, loss = 0.65250248
Iteration 4, loss = 0.63847229
Iteration 5, loss = 0.62366979
Iteration 6, loss = 0.60901084
Iteration 7, loss = 0.59421260
Iteration 8, loss = 0.57690128
Iteration 9, loss = 0.55257183
Iteration 10, loss = 0.52599018
Iteration 11, loss = 0.50117595
Iteration 12, loss = 0.47309315
Iteration 13, loss = 0.44704689
Iteration 14, loss = 0.42264264
Iteration 15, loss = 0.40437650
Iteration 16, loss = 0.37887657
Iteration 17, loss = 0.36176060
Iteration 18, loss = 0.34646327
Iteration 19, loss = 0.33414053
Iteration 20, loss = 0.32189374
Iteration 21, loss = 0.31464193
Iteration 22, loss = 0.30610331
Iteration 23, loss = 0.30010289
Iteration 24, loss = 0.29209904
Iteration 25, loss = 0.28494677
Iteration 26, loss = 0.27836952
Iteration 27, loss = 0.27147298
Iteration 28, loss = 0.26673853
Iteration 29, loss = 0.26321409
Iteration 30, loss = 0.25635121
Iteration 31, loss = 0.25592512
Iteration 32, loss = 0.24973143
Iteration 33, loss = 0.24363920
Iteration 34, loss = 0.23948951
Iteration 35, loss = 0.23611694
Iteration 36, loss = 0.23133889
Iteration 37, loss = 0.23361119
Iteration 38, loss = 0.22785577
Iteration 39, loss = 0.21986575
Iteration 40, loss = 0.21954783
Iteration 41, loss = 0.21537543
Iteration 42, loss = 0.20758644
Iteration 43, loss = 0.20188384
Iteration 44, loss = 0.19812073
Iteration 45, loss = 0.19410213
Iteration 46, loss = 0.19360235
Iteration 47, loss = 0.18591155
Iteration 48, loss = 0.18132032
Iteration 49, loss = 0.17811417
Iteration 50, loss = 0.17624889
Iteration 51, loss = 0.17112621
Iteration 52, loss = 0.16749300
Iteration 53, loss = 0.16542314
Iteration 54, loss = 0.16393923
Iteration 55, loss = 0.16095144
Iteration 56, loss = 0.15643478
Iteration 57, loss = 0.15433495
Iteration 58, loss = 0.15132461
Iteration 59, loss = 0.14879844
Iteration 60, loss = 0.14627230
Iteration 61, loss = 0.14426638
Iteration 62, loss = 0.14238956
Iteration 63, loss = 0.14193478
Iteration 64, loss = 0.14054641
Iteration 65, loss = 0.13353054
Iteration 66, loss = 0.13610768
Iteration 67, loss = 0.13425791
Iteration 68, loss = 0.13224441
Iteration 69, loss = 0.12681100
Iteration 70, loss = 0.12521466
Iteration 71, loss = 0.12460188
Iteration 72, loss = 0.12373380
Iteration 73, loss = 0.11979933
Iteration 74, loss = 0.11692439
Iteration 75, loss = 0.11488590
Iteration 76, loss = 0.11403788
Iteration 77, loss = 0.11386810
Iteration 78, loss = 0.11169930
Iteration 79, loss = 0.10997175
Iteration 80, loss = 0.10904112
Iteration 81, loss = 0.10997323
Iteration 82, loss = 0.10737293
Iteration 83, loss = 0.10454015
Iteration 84, loss = 0.10180190
Iteration 85, loss = 0.10036178
Iteration 86, loss = 0.09905073
Iteration 87, loss = 0.09741861
Iteration 88, loss = 0.09638986
Iteration 89, loss = 0.09539709
Iteration 90, loss = 0.09441711
Iteration 91, loss = 0.09401081
Iteration 92, loss = 0.09298162
Iteration 93, loss = 0.09193082
Iteration 94, loss = 0.09056109
Iteration 95, loss = 0.08894861
Iteration 96, loss = 0.08779897
Iteration 97, loss = 0.08656786
Iteration 98, loss = 0.08514999
Iteration 99, loss = 0.08417195
Iteration 100, loss = 0.08299342
Iteration 101, loss = 0.08223921
Iteration 102, loss = 0.08150859
Iteration 103, loss = 0.08068863
Iteration 104, loss = 0.08120143
Iteration 105, loss = 0.07972932
Iteration 106, loss = 0.07860044
Iteration 107, loss = 0.07758645
Iteration 108, loss = 0.07705378
Iteration 109, loss = 0.07659225
Iteration 110, loss = 0.07630572
Iteration 111, loss = 0.07441368
Iteration 112, loss = 0.07425817
Iteration 113, loss = 0.07425327
Iteration 114, loss = 0.07233981
Iteration 115, loss = 0.07116167
Iteration 116, loss = 0.07057884
Iteration 117, loss = 0.06985111
Iteration 118, loss = 0.06919635
Iteration 119, loss = 0.06876113
Iteration 120, loss = 0.06855779
Iteration 121, loss = 0.06741748
Iteration 122, loss = 0.06731122
Iteration 123, loss = 0.06709693
Iteration 124, loss = 0.06649975
Iteration 125, loss = 0.06604269
Iteration 126, loss = 0.06465534
Iteration 127, loss = 0.06403480
Iteration 128, loss = 0.06372321
Iteration 129, loss = 0.06374074
Iteration 130, loss = 0.06274282
Iteration 131, loss = 0.06173619
Iteration 132, loss = 0.06102553
Iteration 133, loss = 0.06093869
Iteration 134, loss = 0.06139792
Iteration 135, loss = 0.06161915
Iteration 136, loss = 0.05913160
Iteration 137, loss = 0.05887883
Iteration 138, loss = 0.05926614
Iteration 139, loss = 0.05851370
Iteration 140, loss = 0.05669312
Iteration 141, loss = 0.05798664
Iteration 142, loss = 0.05781787
Iteration 143, loss = 0.05678568
Iteration 144, loss = 0.05584972
Iteration 145, loss = 0.05514991
Iteration 146, loss = 0.05448494
Iteration 147, loss = 0.05395915
Iteration 148, loss = 0.05358454
Iteration 149, loss = 0.05394729
Iteration 150, loss = 0.05338367
Iteration 151, loss = 0.05276218
Iteration 152, loss = 0.05242663
Iteration 153, loss = 0.05184775
Iteration 154, loss = 0.05118004
Iteration 155, loss = 0.05038214
Iteration 156, loss = 0.05004310
Iteration 157, loss = 0.04988699
Iteration 158, loss = 0.04936974
Iteration 159, loss = 0.04909424
Iteration 160, loss = 0.04847812
Iteration 161, loss = 0.04799278
Iteration 162, loss = 0.04805050
Iteration 163, loss = 0.04833881
Iteration 164, loss = 0.04768049
Iteration 165, loss = 0.04689869
Iteration 166, loss = 0.04640300
Iteration 167, loss = 0.04604280
Iteration 168, loss = 0.04593816
Iteration 169, loss = 0.04626709
Iteration 170, loss = 0.04639528
Iteration 171, loss = 0.04469131
Iteration 172, loss = 0.04446454
Iteration 173, loss = 0.04329840
Iteration 174, loss = 0.04305281
Iteration 175, loss = 0.04337712
Iteration 176, loss = 0.04294358
Iteration 177, loss = 0.04218891
Iteration 178, loss = 0.04156215
Iteration 179, loss = 0.04094382
Iteration 180, loss = 0.04063461
Iteration 181, loss = 0.04025583
Iteration 182, loss = 0.04023857
Iteration 183, loss = 0.03941771
Iteration 184, loss = 0.03867971
Iteration 185, loss = 0.03838593
Iteration 186, loss = 0.03842038
Iteration 187, loss = 0.03820441
Iteration 188, loss = 0.03803105
Iteration 189, loss = 0.03764225
Iteration 190, loss = 0.03738586
Iteration 191, loss = 0.03722801
Iteration 192, loss = 0.03711154
Iteration 193, loss = 0.03680830
Iteration 194, loss = 0.03634032
Iteration 195, loss = 0.03568005
Iteration 196, loss = 0.03523647
Iteration 197, loss = 0.03525537
Iteration 198, loss = 0.03496062
Iteration 199, loss = 0.03451597
Iteration 200, loss = 0.03409056
Iteration 201, loss = 0.03369500
Iteration 202, loss = 0.03334140
Iteration 203, loss = 0.03302792
Iteration 204, loss = 0.03287400
Iteration 205, loss = 0.03258921
Iteration 206, loss = 0.03237154
Iteration 207, loss = 0.03234566
Iteration 208, loss = 0.03174948
Iteration 209, loss = 0.03214110
Iteration 210, loss = 0.03127726
Iteration 211, loss = 0.03101206
Iteration 212, loss = 0.03092333
Iteration 213, loss = 0.03066902
Iteration 214, loss = 0.03035919
Iteration 215, loss = 0.03053370
Iteration 216, loss = 0.02962621
Iteration 217, loss = 0.02920400
Iteration 218, loss = 0.02909234
Iteration 219, loss = 0.02905672
Iteration 220, loss = 0.02912657
Iteration 221, loss = 0.02850408
Iteration 222, loss = 0.02815127
Iteration 223, loss = 0.02846712
Iteration 224, loss = 0.02805371
Iteration 225, loss = 0.02746653
Iteration 226, loss = 0.02704856
Iteration 227, loss = 0.02708534
Iteration 228, loss = 0.02737573
Iteration 229, loss = 0.02747875
Iteration 230, loss = 0.02695064
Iteration 231, loss = 0.02637196
Iteration 232, loss = 0.02578880
Iteration 233, loss = 0.02560966
Iteration 234, loss = 0.02551522
Iteration 235, loss = 0.02544798
Iteration 236, loss = 0.02530763
Iteration 237, loss = 0.02505484
Iteration 238, loss = 0.02485369
Iteration 239, loss = 0.02477728
Iteration 240, loss = 0.02479057
Iteration 241, loss = 0.02430563
Iteration 242, loss = 0.02394117
Iteration 243, loss = 0.02402536
Iteration 244, loss = 0.02352321
Iteration 245, loss = 0.02320449
Iteration 246, loss = 0.02300371
Iteration 247, loss = 0.02280056
Iteration 248, loss = 0.02257380
Iteration 249, loss = 0.02244979
Iteration 250, loss = 0.02231336
Iteration 251, loss = 0.02232139
Iteration 252, loss = 0.02199205
Iteration 253, loss = 0.02216291
Iteration 254, loss = 0.02333374
Iteration 255, loss = 0.02273406
Iteration 256, loss = 0.02171817
Iteration 257, loss = 0.02099989
Iteration 258, loss = 0.02116849
Iteration 259, loss = 0.02071239
Iteration 260, loss = 0.02054829
Iteration 261, loss = 0.02050317
Iteration 262, loss = 0.02048242
Iteration 263, loss = 0.02040219
Iteration 264, loss = 0.02018458
Iteration 265, loss = 0.02011141
Iteration 266, loss = 0.02027806
Iteration 267, loss = 0.02052846
Iteration 268, loss = 0.02001177
Iteration 269, loss = 0.01987022
Iteration 270, loss = 0.01933584
Iteration 271, loss = 0.01891326
Iteration 272, loss = 0.01906942
Iteration 273, loss = 0.01945939
Iteration 274, loss = 0.01890057
Iteration 275, loss = 0.01851150
Iteration 276, loss = 0.01826584
Iteration 277, loss = 0.01816286
Iteration 278, loss = 0.01810412
Iteration 279, loss = 0.01791659
Iteration 280, loss = 0.01772290
Iteration 281, loss = 0.01753966
Iteration 282, loss = 0.01748674
Iteration 283, loss = 0.01758832
Iteration 284, loss = 0.01777627
Iteration 285, loss = 0.01778889
Iteration 286, loss = 0.01763359
Iteration 287, loss = 0.01733306
Iteration 288, loss = 0.01695932
Iteration 289, loss = 0.01680170
Iteration 290, loss = 0.01662370
Iteration 291, loss = 0.01650286
Iteration 292, loss = 0.01640915
Iteration 293, loss = 0.01631415
Iteration 294, loss = 0.01655913
Iteration 295, loss = 0.01633744
Iteration 296, loss = 0.01618379
Iteration 297, loss = 0.01602751
Iteration 298, loss = 0.01591453
Iteration 299, loss = 0.01582631
Iteration 300, loss = 0.01574120
Iteration 301, loss = 0.01563486
Iteration 302, loss = 0.01554424
Iteration 303, loss = 0.01543046
Iteration 304, loss = 0.01535979
Iteration 305, loss = 0.01535258
Iteration 306, loss = 0.01542444
Iteration 307, loss = 0.01547688
Iteration 308, loss = 0.01557772
Iteration 309, loss = 0.01490160
Iteration 310, loss = 0.01488701
Iteration 311, loss = 0.01475019
Iteration 312, loss = 0.01473190
Iteration 313, loss = 0.01469820
Iteration 314, loss = 0.01448813
Iteration 315, loss = 0.01429241
Iteration 316, loss = 0.01420232
Iteration 317, loss = 0.01418367
Iteration 318, loss = 0.01412722
Iteration 319, loss = 0.01416155
Iteration 320, loss = 0.01402081
Iteration 321, loss = 0.01382142
Iteration 322, loss = 0.01366037
Iteration 323, loss = 0.01363386
Iteration 324, loss = 0.01354201
Iteration 325, loss = 0.01346156
Iteration 326, loss = 0.01338427
Iteration 327, loss = 0.01328144
Iteration 328, loss = 0.01323571
Iteration 329, loss = 0.01328701
Iteration 330, loss = 0.01340509
Iteration 331, loss = 0.01361398
Iteration 332, loss = 0.01383119
Iteration 333, loss = 0.01353460
Iteration 334, loss = 0.01302824
Iteration 335, loss = 0.01272949
Iteration 336, loss = 0.01280757
Iteration 337, loss = 0.01275753
Iteration 338, loss = 0.01249152
Iteration 339, loss = 0.01259730
Iteration 340, loss = 0.01276032
Iteration 341, loss = 0.01243182
Iteration 342, loss = 0.01235698
Iteration 343, loss = 0.01233296
Iteration 344, loss = 0.01231153
Iteration 345, loss = 0.01228364
Iteration 346, loss = 0.01226251
Iteration 347, loss = 0.01207263
Iteration 348, loss = 0.01192825
Iteration 349, loss = 0.01177879
Iteration 350, loss = 0.01170471
Iteration 351, loss = 0.01156123
Iteration 352, loss = 0.01148219
Iteration 353, loss = 0.01144002
Iteration 354, loss = 0.01142312
Iteration 355, loss = 0.01143486
Iteration 356, loss = 0.01138297
Iteration 357, loss = 0.01129355
Iteration 358, loss = 0.01118308
Iteration 359, loss = 0.01116324
Iteration 360, loss = 0.01114876
Iteration 361, loss = 0.01107904
Iteration 362, loss = 0.01099201
Iteration 363, loss = 0.01091084
Iteration 364, loss = 0.01083477
Iteration 365, loss = 0.01077049
Iteration 366, loss = 0.01071795
Iteration 367, loss = 0.01069356
Iteration 368, loss = 0.01067556
Iteration 369, loss = 0.01066645
Iteration 370, loss = 0.01063216
Iteration 371, loss = 0.01054172
Iteration 372, loss = 0.01045459
Iteration 373, loss = 0.01054814
Iteration 374, loss = 0.01057523
Iteration 375, loss = 0.01037707
Iteration 376, loss = 0.01021383
Iteration 377, loss = 0.01017262
Iteration 378, loss = 0.01012985
Iteration 379, loss = 0.01012034
Iteration 380, loss = 0.01009965
Iteration 381, loss = 0.01001094
Iteration 382, loss = 0.01002924
Iteration 383, loss = 0.00991058
Iteration 384, loss = 0.00980753
Iteration 385, loss = 0.01000340
Iteration 386, loss = 0.00982974
Iteration 387, loss = 0.00966959
Iteration 388, loss = 0.00961002
Iteration 389, loss = 0.00956523
Iteration 390, loss = 0.00949581
Iteration 391, loss = 0.00943080
Iteration 392, loss = 0.00939539
Iteration 393, loss = 0.00934019
Iteration 394, loss = 0.00929496
Iteration 395, loss = 0.00926749
Iteration 396, loss = 0.00922740
Iteration 397, loss = 0.00921149
Iteration 398, loss = 0.00915658
Iteration 399, loss = 0.00909977
Iteration 400, loss = 0.00907415
Iteration 401, loss = 0.00905830
Iteration 402, loss = 0.00905811
Iteration 403, loss = 0.00902099
Iteration 404, loss = 0.00892794
Iteration 405, loss = 0.00888944
Iteration 406, loss = 0.00886986
Iteration 407, loss = 0.00886080
Iteration 408, loss = 0.00880599
Iteration 409, loss = 0.00877032
Iteration 410, loss = 0.00872766
Iteration 411, loss = 0.00870011
Iteration 412, loss = 0.00866363
Iteration 413, loss = 0.00864768
Iteration 414, loss = 0.00859200
Iteration 415, loss = 0.00853734
Iteration 416, loss = 0.00850782
Iteration 417, loss = 0.00848881
Iteration 418, loss = 0.00846786
Iteration 419, loss = 0.00842535
Iteration 420, loss = 0.00838860
Iteration 421, loss = 0.00837305
Iteration 422, loss = 0.00835149
Iteration 423, loss = 0.00830895
Iteration 424, loss = 0.00828376
Iteration 425, loss = 0.00820010
Iteration 426, loss = 0.00815197
Iteration 427, loss = 0.00811179
Iteration 428, loss = 0.00807988
Iteration 429, loss = 0.00804600
Iteration 430, loss = 0.00803531
Iteration 431, loss = 0.00804985
Iteration 432, loss = 0.00809493
Iteration 433, loss = 0.00802577
Iteration 434, loss = 0.00794217
Iteration 435, loss = 0.00784965
Iteration 436, loss = 0.00784474
Iteration 437, loss = 0.00781160
Iteration 438, loss = 0.00784417
Iteration 439, loss = 0.00792676
Iteration 440, loss = 0.00780238
Iteration 441, loss = 0.00768442
Iteration 442, loss = 0.00762045
Iteration 443, loss = 0.00760502
Iteration 444, loss = 0.00760332
Iteration 445, loss = 0.00760008
Iteration 446, loss = 0.00755595
Iteration 447, loss = 0.00751855
Iteration 448, loss = 0.00746729
Iteration 449, loss = 0.00740849
Iteration 450, loss = 0.00738118
Iteration 451, loss = 0.00734596
Iteration 452, loss = 0.00731563
Iteration 453, loss = 0.00729938
Iteration 454, loss = 0.00726185
Iteration 455, loss = 0.00723243
Iteration 456, loss = 0.00720134
Iteration 457, loss = 0.00717263
Iteration 458, loss = 0.00716307
Iteration 459, loss = 0.00713244
Iteration 460, loss = 0.00709178
Iteration 461, loss = 0.00705734
Iteration 462, loss = 0.00703741
Iteration 463, loss = 0.00703667
Iteration 464, loss = 0.00700613
Iteration 465, loss = 0.00696870
Iteration 466, loss = 0.00694012
Iteration 467, loss = 0.00691891
Iteration 468, loss = 0.00690456
Iteration 469, loss = 0.00692177
Iteration 470, loss = 0.00690811
Iteration 471, loss = 0.00690020
Iteration 472, loss = 0.00689616
Iteration 473, loss = 0.00687831
Iteration 474, loss = 0.00690777
Iteration 475, loss = 0.00677642
Iteration 476, loss = 0.00670184
Iteration 477, loss = 0.00670039
Iteration 478, loss = 0.00668881
Iteration 479, loss = 0.00665759
Iteration 480, loss = 0.00663355
Iteration 481, loss = 0.00660151
Iteration 482, loss = 0.00657721
Iteration 483, loss = 0.00655886
Iteration 484, loss = 0.00652746
Iteration 485, loss = 0.00653512
Iteration 486, loss = 0.00664781
Iteration 487, loss = 0.00658619
Iteration 488, loss = 0.00650974
Iteration 489, loss = 0.00645245
Iteration 490, loss = 0.00640135
Iteration 491, loss = 0.00634730
Iteration 492, loss = 0.00637311
Iteration 493, loss = 0.00632902
Iteration 494, loss = 0.00630769
Iteration 495, loss = 0.00630051
Iteration 496, loss = 0.00627350
Iteration 497, loss = 0.00628997
Iteration 498, loss = 0.00624754
Iteration 499, loss = 0.00621307
Iteration 500, loss = 0.00618028
Iteration 501, loss = 0.00615124
Iteration 502, loss = 0.00612292
Iteration 503, loss = 0.00609818
Iteration 504, loss = 0.00607746
Iteration 505, loss = 0.00605375
Iteration 506, loss = 0.00602808
Iteration 507, loss = 0.00601341
Iteration 508, loss = 0.00599587
Iteration 509, loss = 0.00598148
Iteration 510, loss = 0.00596595
Iteration 511, loss = 0.00594896
Iteration 512, loss = 0.00592875
Iteration 513, loss = 0.00588843
Iteration 514, loss = 0.00588374
Iteration 515, loss = 0.00594684
Iteration 516, loss = 0.00587161
Iteration 517, loss = 0.00583888
Iteration 518, loss = 0.00581521
Iteration 519, loss = 0.00578989
Iteration 520, loss = 0.00579949
Iteration 521, loss = 0.00577403
Iteration 522, loss = 0.00575759
Iteration 523, loss = 0.00574878
Iteration 524, loss = 0.00573323
Iteration 525, loss = 0.00571503
Iteration 526, loss = 0.00569297
Iteration 527, loss = 0.00567583
Iteration 528, loss = 0.00566008
Iteration 529, loss = 0.00566044
Iteration 530, loss = 0.00562019
Iteration 531, loss = 0.00558560
Iteration 532, loss = 0.00557817
Iteration 533, loss = 0.00557192
Iteration 534, loss = 0.00555202
Iteration 535, loss = 0.00552158
Iteration 536, loss = 0.00550379
Iteration 537, loss = 0.00549084
Iteration 538, loss = 0.00547037
Iteration 539, loss = 0.00545898
Iteration 540, loss = 0.00544660
Iteration 541, loss = 0.00543776
Iteration 542, loss = 0.00541193
Iteration 543, loss = 0.00539227
Iteration 544, loss = 0.00537288
Iteration 545, loss = 0.00537936
Iteration 546, loss = 0.00534784
Iteration 547, loss = 0.00537765
Iteration 548, loss = 0.00529888
Iteration 549, loss = 0.00526402
Iteration 550, loss = 0.00525170
Iteration 551, loss = 0.00526879
Iteration 552, loss = 0.00526370
Iteration 553, loss = 0.00526032
Iteration 554, loss = 0.00523403
Iteration 555, loss = 0.00521465
Iteration 556, loss = 0.00517349
Iteration 557, loss = 0.00514218
Iteration 558, loss = 0.00517803
Iteration 559, loss = 0.00513925
Iteration 560, loss = 0.00510023
Iteration 561, loss = 0.00508258
Iteration 562, loss = 0.00506943
Iteration 563, loss = 0.00505771
Iteration 564, loss = 0.00504634
Iteration 565, loss = 0.00503152
Iteration 566, loss = 0.00504002
Iteration 567, loss = 0.00500487
Iteration 568, loss = 0.00498815
Iteration 569, loss = 0.00499208
Iteration 570, loss = 0.00497416
Iteration 571, loss = 0.00494885
Iteration 572, loss = 0.00495319
Iteration 573, loss = 0.00491744
Iteration 574, loss = 0.00494612
Iteration 575, loss = 0.00490054
Iteration 576, loss = 0.00488043
Iteration 577, loss = 0.00486584
Iteration 578, loss = 0.00485451
Iteration 579, loss = 0.00483487
Iteration 580, loss = 0.00482952
Iteration 581, loss = 0.00481441
Iteration 582, loss = 0.00479842
Iteration 583, loss = 0.00480177
Iteration 584, loss = 0.00479186
Iteration 585, loss = 0.00475311
Iteration 586, loss = 0.00474988
Iteration 587, loss = 0.00474109
Iteration 588, loss = 0.00473073
Iteration 589, loss = 0.00471721
Iteration 590, loss = 0.00471503
Iteration 591, loss = 0.00471856
Iteration 592, loss = 0.00471897
Iteration 593, loss = 0.00473386
Iteration 594, loss = 0.00467632
Iteration 595, loss = 0.00463980
Iteration 596, loss = 0.00461202
Iteration 597, loss = 0.00459166
Iteration 598, loss = 0.00459180
Iteration 599, loss = 0.00460906
Iteration 600, loss = 0.00460160
Iteration 601, loss = 0.00458151
Iteration 602, loss = 0.00455471
Iteration 603, loss = 0.00452837
Iteration 604, loss = 0.00450938
Iteration 605, loss = 0.00450128
Iteration 606, loss = 0.00450060
Iteration 607, loss = 0.00450079
Iteration 608, loss = 0.00449792
Iteration 609, loss = 0.00449635
Iteration 610, loss = 0.00447996
Iteration 611, loss = 0.00445330
Iteration 612, loss = 0.00443469
Iteration 613, loss = 0.00441737
Iteration 614, loss = 0.00440487
Iteration 615, loss = 0.00440391
Iteration 616, loss = 0.00439583
Iteration 617, loss = 0.00438511
Iteration 618, loss = 0.00438156
Iteration 619, loss = 0.00439693
Iteration 620, loss = 0.00437032
Iteration 621, loss = 0.00435911
Iteration 622, loss = 0.00434142
Iteration 623, loss = 0.00433927
Iteration 624, loss = 0.00432334
Iteration 625, loss = 0.00429248
Iteration 626, loss = 0.00426707
Iteration 627, loss = 0.00426462
Iteration 628, loss = 0.00427886
Iteration 629, loss = 0.00427222
Iteration 630, loss = 0.00427512
Iteration 631, loss = 0.00427289
Iteration 632, loss = 0.00428922
Iteration 633, loss = 0.00427001
Iteration 634, loss = 0.00424792
Iteration 635, loss = 0.00422464
Iteration 636, loss = 0.00419783
Iteration 637, loss = 0.00416867
Iteration 638, loss = 0.00415177
Iteration 639, loss = 0.00413967
Iteration 640, loss = 0.00414808
Iteration 641, loss = 0.00414211
Iteration 642, loss = 0.00414228
Iteration 643, loss = 0.00413508
Iteration 644, loss = 0.00413039
Iteration 645, loss = 0.00412479
Iteration 646, loss = 0.00411506
Iteration 647, loss = 0.00409946
Iteration 648, loss = 0.00408865
Iteration 649, loss = 0.00407219
Iteration 650, loss = 0.00404982
Iteration 651, loss = 0.00403515
Iteration 652, loss = 0.00401151
Iteration 653, loss = 0.00402734
Iteration 654, loss = 0.00406607
Iteration 655, loss = 0.00409100
Iteration 656, loss = 0.00404876
Iteration 657, loss = 0.00399107
Iteration 658, loss = 0.00398018
Iteration 659, loss = 0.00395075
Iteration 660, loss = 0.00394859
Iteration 661, loss = 0.00396362
Iteration 662, loss = 0.00399256
Iteration 663, loss = 0.00396842
Iteration 664, loss = 0.00395511
Iteration 665, loss = 0.00393661
Iteration 666, loss = 0.00392332
Iteration 667, loss = 0.00389413
Iteration 668, loss = 0.00388313
Iteration 669, loss = 0.00386941
Iteration 670, loss = 0.00386309
Iteration 671, loss = 0.00385487
Iteration 672, loss = 0.00385239
Iteration 673, loss = 0.00383755
Iteration 674, loss = 0.00382899
Iteration 675, loss = 0.00381395
Iteration 676, loss = 0.00380445
Iteration 677, loss = 0.00379636
Iteration 678, loss = 0.00379121
Iteration 679, loss = 0.00378829
Iteration 680, loss = 0.00378370
Iteration 681, loss = 0.00377229
Iteration 682, loss = 0.00376385
Iteration 683, loss = 0.00374547
Iteration 684, loss = 0.00374133
Iteration 685, loss = 0.00374402
Iteration 686, loss = 0.00374843
Iteration 687, loss = 0.00374986
Iteration 688, loss = 0.00374164
Iteration 689, loss = 0.00372987
Iteration 690, loss = 0.00371990
Iteration 691, loss = 0.00369599
Iteration 692, loss = 0.00368138
Iteration 693, loss = 0.00366689
Iteration 694, loss = 0.00365383
Iteration 695, loss = 0.00364414
Iteration 696, loss = 0.00363638
Iteration 697, loss = 0.00362693
Iteration 698, loss = 0.00363062
Iteration 699, loss = 0.00360796
Iteration 700, loss = 0.00360928
Iteration 701, loss = 0.00360464
Iteration 702, loss = 0.00359793
Iteration 703, loss = 0.00360151
Iteration 704, loss = 0.00358765
Iteration 705, loss = 0.00356487
Iteration 706, loss = 0.00357389
Iteration 707, loss = 0.00354935
Iteration 708, loss = 0.00354080
Iteration 709, loss = 0.00353087
Iteration 710, loss = 0.00352393
Iteration 711, loss = 0.00352095
Iteration 712, loss = 0.00351328
Iteration 713, loss = 0.00350890
Iteration 714, loss = 0.00350128
Iteration 715, loss = 0.00349108
Iteration 716, loss = 0.00348570
Iteration 717, loss = 0.00348553
Iteration 718, loss = 0.00349611
Iteration 719, loss = 0.00350645
Iteration 720, loss = 0.00352327
Iteration 721, loss = 0.00355237
Iteration 722, loss = 0.00352534
Iteration 723, loss = 0.00350016
Iteration 724, loss = 0.00346853
Iteration 725, loss = 0.00344625
Iteration 726, loss = 0.00341898
Iteration 727, loss = 0.00341366
Iteration 728, loss = 0.00340864
Iteration 729, loss = 0.00339235
Iteration 730, loss = 0.00338171
Iteration 731, loss = 0.00337568
Iteration 732, loss = 0.00338482
Iteration 733, loss = 0.00337191
Iteration 734, loss = 0.00336779
Iteration 735, loss = 0.00335823
Iteration 736, loss = 0.00335132
Iteration 737, loss = 0.00332219
Iteration 738, loss = 0.00331273
Iteration 739, loss = 0.00331320
Iteration 740, loss = 0.00331423
Iteration 741, loss = 0.00332320
Iteration 742, loss = 0.00329825
Iteration 743, loss = 0.00328426
Iteration 744, loss = 0.00327437
Iteration 745, loss = 0.00327970
Iteration 746, loss = 0.00326726
Iteration 747, loss = 0.00325726
Iteration 748, loss = 0.00325143
Iteration 749, loss = 0.00324674
Iteration 750, loss = 0.00324164
Iteration 751, loss = 0.00323995
Iteration 752, loss = 0.00323125
Iteration 753, loss = 0.00322905
Iteration 754, loss = 0.00322699
Iteration 755, loss = 0.00322770
Iteration 756, loss = 0.00322033
Iteration 757, loss = 0.00321474
Iteration 758, loss = 0.00320816
Iteration 759, loss = 0.00320264
Iteration 760, loss = 0.00319379
Iteration 761, loss = 0.00318318
Iteration 762, loss = 0.00317411
Iteration 763, loss = 0.00316988
Iteration 764, loss = 0.00318560
Iteration 765, loss = 0.00317286
Iteration 766, loss = 0.00315792
Iteration 767, loss = 0.00314215
Iteration 768, loss = 0.00314143
Iteration 769, loss = 0.00312379
Iteration 770, loss = 0.00312000
Iteration 771, loss = 0.00312116
Iteration 772, loss = 0.00311140
Iteration 773, loss = 0.00310122
Iteration 774, loss = 0.00309421
Iteration 775, loss = 0.00308718
Iteration 776, loss = 0.00308035
Iteration 777, loss = 0.00307445
Iteration 778, loss = 0.00306973
Iteration 779, loss = 0.00306457
Iteration 780, loss = 0.00305904
Iteration 781, loss = 0.00305535
Iteration 782, loss = 0.00305152
Iteration 783, loss = 0.00304637
Iteration 784, loss = 0.00304344
Iteration 785, loss = 0.00303957
Iteration 786, loss = 0.00303853
Iteration 787, loss = 0.00304060
Iteration 788, loss = 0.00304219
Iteration 789, loss = 0.00304405
Iteration 790, loss = 0.00304767
Iteration 791, loss = 0.00304862
Iteration 792, loss = 0.00305696
Iteration 793, loss = 0.00304038
Iteration 794, loss = 0.00302198
Iteration 795, loss = 0.00300577
Iteration 796, loss = 0.00298876
Iteration 797, loss = 0.00297826
Iteration 798, loss = 0.00297072
Iteration 799, loss = 0.00296660
Iteration 800, loss = 0.00295985
Iteration 801, loss = 0.00294750
Iteration 802, loss = 0.00293980
Iteration 803, loss = 0.00293524
Iteration 804, loss = 0.00293304
Iteration 805, loss = 0.00291763
Iteration 806, loss = 0.00291349
Iteration 807, loss = 0.00290791
Iteration 808, loss = 0.00290276
Iteration 809, loss = 0.00290602
Iteration 810, loss = 0.00289757
Iteration 811, loss = 0.00289214
Iteration 812, loss = 0.00288594
Iteration 813, loss = 0.00288156
Iteration 814, loss = 0.00287774
Iteration 815, loss = 0.00286898
Iteration 816, loss = 0.00286994
Iteration 817, loss = 0.00285638
Iteration 818, loss = 0.00285171
Iteration 819, loss = 0.00284726
Iteration 820, loss = 0.00285044
Iteration 821, loss = 0.00284017
Iteration 822, loss = 0.00284069
Iteration 823, loss = 0.00283221
Iteration 824, loss = 0.00282762
Iteration 825, loss = 0.00282222
Iteration 826, loss = 0.00281788
Iteration 827, loss = 0.00281287
Iteration 828, loss = 0.00281067
Iteration 829, loss = 0.00280696
Iteration 830, loss = 0.00280025
Iteration 831, loss = 0.00279707
Iteration 832, loss = 0.00279187
Iteration 833, loss = 0.00278675
Iteration 834, loss = 0.00278253
Iteration 835, loss = 0.00277761
Iteration 836, loss = 0.00277405
Iteration 837, loss = 0.00277074
Iteration 838, loss = 0.00276857
Iteration 839, loss = 0.00277328
Iteration 840, loss = 0.00276473
Iteration 841, loss = 0.00275588
Iteration 842, loss = 0.00274257
Iteration 843, loss = 0.00273279
Iteration 844, loss = 0.00272813
Iteration 845, loss = 0.00272339
Iteration 846, loss = 0.00272181
Iteration 847, loss = 0.00271361
Iteration 848, loss = 0.00270887
Iteration 849, loss = 0.00270584
Iteration 850, loss = 0.00270198
Iteration 851, loss = 0.00270102
Iteration 852, loss = 0.00269165
Iteration 853, loss = 0.00268758
Iteration 854, loss = 0.00268292
Iteration 855, loss = 0.00268367
Iteration 856, loss = 0.00266995
Iteration 857, loss = 0.00266249
Iteration 858, loss = 0.00266181
Iteration 859, loss = 0.00265731
Iteration 860, loss = 0.00265765
Iteration 861, loss = 0.00266072
Iteration 862, loss = 0.00266174
Iteration 863, loss = 0.00266215
Iteration 864, loss = 0.00266128
Iteration 865, loss = 0.00265459
Iteration 866, loss = 0.00264997
Iteration 867, loss = 0.00263665
Iteration 868, loss = 0.00262889
Iteration 869, loss = 0.00262091
Iteration 870, loss = 0.00261233
Iteration 871, loss = 0.00260807
Iteration 872, loss = 0.00260066
Iteration 873, loss = 0.00259509
Iteration 874, loss = 0.00259016
Iteration 875, loss = 0.00258556
Iteration 876, loss = 0.00258095
Iteration 877, loss = 0.00258137
Iteration 878, loss = 0.00257940
Iteration 879, loss = 0.00258144
Iteration 880, loss = 0.00258085
Iteration 881, loss = 0.00257770
Iteration 882, loss = 0.00257500
Iteration 883, loss = 0.00256714
Iteration 884, loss = 0.00255660
Iteration 885, loss = 0.00255117
Iteration 886, loss = 0.00254092
Iteration 887, loss = 0.00253424
Iteration 888, loss = 0.00253179
Iteration 889, loss = 0.00252550
Iteration 890, loss = 0.00252112
Iteration 891, loss = 0.00251693
Iteration 892, loss = 0.00251072
Iteration 893, loss = 0.00251137
Iteration 894, loss = 0.00251084
Iteration 895, loss = 0.00251051
Iteration 896, loss = 0.00250476
Iteration 897, loss = 0.00250062
Iteration 898, loss = 0.00249466
Iteration 899, loss = 0.00248450
Iteration 900, loss = 0.00247968
Iteration 901, loss = 0.00247982
Iteration 902, loss = 0.00248367
Iteration 903, loss = 0.00249278
Iteration 904, loss = 0.00250597
Iteration 905, loss = 0.00249552
Iteration 906, loss = 0.00248369
Iteration 907, loss = 0.00247738
Iteration 908, loss = 0.00247152
Iteration 909, loss = 0.00246634
Iteration 910, loss = 0.00246265
Iteration 911, loss = 0.00245502
Iteration 912, loss = 0.00244934
Iteration 913, loss = 0.00244202
Iteration 914, loss = 0.00243387
Iteration 915, loss = 0.00243160
Iteration 916, loss = 0.00242301
Iteration 917, loss = 0.00242149
Iteration 918, loss = 0.00241094
Iteration 919, loss = 0.00240894
Iteration 920, loss = 0.00240187
Iteration 921, loss = 0.00239915
Iteration 922, loss = 0.00239324
Iteration 923, loss = 0.00239236
Iteration 924, loss = 0.00239525
Iteration 925, loss = 0.00238972
Iteration 926, loss = 0.00238932
Iteration 927, loss = 0.00237289
Iteration 928, loss = 0.00236755
Iteration 929, loss = 0.00236502
Iteration 930, loss = 0.00236698
Iteration 931, loss = 0.00236445
Iteration 932, loss = 0.00236387
Iteration 933, loss = 0.00236264
Iteration 934, loss = 0.00236270
Iteration 935, loss = 0.00236650
Iteration 936, loss = 0.00236107
Iteration 937, loss = 0.00235246
Iteration 938, loss = 0.00234386
Iteration 939, loss = 0.00233348
Iteration 940, loss = 0.00233071
Iteration 941, loss = 0.00232929
Iteration 942, loss = 0.00233016
Iteration 943, loss = 0.00233410
Iteration 944, loss = 0.00233220
Iteration 945, loss = 0.00233335
Iteration 946, loss = 0.00232494
Iteration 947, loss = 0.00231744
Iteration 948, loss = 0.00230889
Iteration 949, loss = 0.00230230
Iteration 950, loss = 0.00229720
Iteration 951, loss = 0.00228952
Iteration 952, loss = 0.00228864
Iteration 953, loss = 0.00228153
Iteration 954, loss = 0.00228419
Iteration 955, loss = 0.00227739
Iteration 956, loss = 0.00227030
Iteration 957, loss = 0.00226722
Iteration 958, loss = 0.00226580
Iteration 959, loss = 0.00225887
Iteration 960, loss = 0.00225647
Iteration 961, loss = 0.00225644
Iteration 962, loss = 0.00225030
Iteration 963, loss = 0.00224856
Iteration 964, loss = 0.00224571
Iteration 965, loss = 0.00224090
Iteration 966, loss = 0.00223693
Iteration 967, loss = 0.00223274
Iteration 968, loss = 0.00222918
Iteration 969, loss = 0.00222482
Iteration 970, loss = 0.00222138
Iteration 971, loss = 0.00221841
Iteration 972, loss = 0.00221564
Iteration 973, loss = 0.00221101
Iteration 974, loss = 0.00220720
Iteration 975, loss = 0.00220273
Iteration 976, loss = 0.00220185
Iteration 977, loss = 0.00219717
Iteration 978, loss = 0.00219413
Iteration 979, loss = 0.00219181
Iteration 980, loss = 0.00218936
Iteration 981, loss = 0.00218386
Iteration 982, loss = 0.00219809
Iteration 983, loss = 0.00218766
Iteration 984, loss = 0.00218747
Iteration 985, loss = 0.00218813
Iteration 986, loss = 0.00218595
Iteration 987, loss = 0.00218440
Iteration 988, loss = 0.00218151
Iteration 989, loss = 0.00217846
Iteration 990, loss = 0.00217432
Iteration 991, loss = 0.00216829
Iteration 992, loss = 0.00216254
Iteration 993, loss = 0.00215541
Iteration 994, loss = 0.00215086
Iteration 995, loss = 0.00214621
Iteration 996, loss = 0.00214163
Iteration 997, loss = 0.00213855
Iteration 998, loss = 0.00213747
Iteration 999, loss = 0.00213616
Iteration 1000, loss = 0.00213705
PARAMETROS DE INICIALIZACAO DA REDE
NUMERO DE NEURONIOS 
Camada de Entrada: 34
Camada Escondida: 10
Camada de Saida: 1

--- PARAMETROS DE CONFIGURACAO DA REDE ---
Numero de Epocas: 1000
Taxa de Aprendizado: adaptive
Taxa de Aprendizado Inicial: 0.3
METRICAS

RESULTADOS:

[0 0 1 0 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1 1 1 1 1 1 1
 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 1 1 1
 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 1 0 0 1 1 0 1 1 1 0 1 1 0]
ACURACIA: 0.9245283018867925

