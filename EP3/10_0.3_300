Pesos Camada de Entrada: 
[[-0.02230177  0.20333934 -0.18179152  0.00311934 -0.04077344 -0.12901098
  -0.03542862 -0.17222849 -0.00786332  0.01837845]
 [-0.10074681 -0.09755278  0.00882231  0.16072235  0.19809482  0.17822288
   0.15340916  0.1845043  -0.13051383  0.05577645]
 [ 0.1001139   0.00564101  0.15670176 -0.18729385 -0.0678411   0.05869685
   0.08118946  0.15925412  0.07134954 -0.07674929]
 [ 0.18356665 -0.12376936 -0.02226719  0.0038873   0.16371305 -0.08899625
  -0.02450545  0.12659089  0.0846137  -0.20938084]
 [-0.18453889  0.1096083  -0.0467212   0.04591808  0.000274    0.10696488
   0.09245873  0.08733152 -0.15756477 -0.00489918]
 [-0.2115923  -0.02067085  0.15791685 -0.14484331 -0.15822522 -0.06767604
  -0.09963393 -0.1031058   0.13533546  0.13548849]
 [-0.01368978  0.13204523 -0.14522514 -0.12579148 -0.06876976 -0.04695071
  -0.19384577 -0.21290081  0.17981594 -0.00465605]
 [-0.14629889 -0.05779969  0.19625944  0.0238007   0.19658064  0.11419031
   0.02737755 -0.16036844  0.11200706 -0.06759942]
 [ 0.00059092 -0.20042146 -0.01683449 -0.0442143  -0.09712459 -0.18911741
   0.10470672 -0.0286493  -0.20347062 -0.0539029 ]
 [ 0.12741155  0.17503054 -0.05096805 -0.18172987 -0.04604078  0.05742049
  -0.0440186   0.19836054 -0.00845183  0.02914009]
 [ 0.07539921 -0.00115824  0.1895847   0.14173087  0.16866573  0.18687511
   0.18766296 -0.19450086 -0.18703185  0.01386182]
 [-0.18327962  0.01706853  0.17672763  0.10384627 -0.14891486  0.20651586
   0.05029795  0.10412048  0.15741272 -0.00469083]
 [ 0.04145375  0.09671897 -0.0609506   0.11051317  0.14711401  0.18136404
  -0.12623674 -0.12092608 -0.03378883  0.1479958 ]
 [-0.00820862  0.11785283 -0.01929874  0.11141881  0.02768382  0.0415484
   0.02709585 -0.04639052  0.0649914  -0.17172964]
 [-0.11370746  0.08406085 -0.03899719 -0.18523401 -0.20327945  0.07012325
  -0.03814459  0.03817748 -0.05588204 -0.20780338]
 [ 0.19835058 -0.08354027  0.03808986  0.00143603  0.05571801 -0.05440806
   0.01778221 -0.07647869 -0.06437066  0.00525269]
 [ 0.08974303 -0.00907701 -0.11741365  0.0424091  -0.04293594  0.20315711
   0.00579656  0.0148314   0.20365675  0.17942287]
 [-0.04132264  0.00913696 -0.01495485 -0.13927278 -0.18546625  0.08925025
  -0.10755472  0.03092584  0.10449759  0.06165025]
 [ 0.13230231 -0.03169942 -0.08528484 -0.03012782 -0.15914632 -0.05500812
  -0.03606092 -0.1541878  -0.05653533 -0.02135687]
 [ 0.02263773  0.04736264  0.21071161  0.15608267 -0.12573318 -0.018066
  -0.02471008 -0.06363271 -0.16039157  0.17353018]
 [ 0.1076083   0.04592831 -0.08413963  0.04983197 -0.00510374 -0.10630847
  -0.11434871  0.06761558  0.05774646 -0.13378455]
 [ 0.09129082  0.20337397 -0.11635622  0.20005212  0.13155773 -0.03389588
   0.15201347 -0.07656897  0.21281547 -0.17916755]
 [-0.18760237  0.13000812 -0.09752988  0.07785214  0.17424328 -0.10064186
   0.07342664 -0.20058413  0.02387461  0.07550421]
 [-0.17513574  0.05767455  0.02442649 -0.13981474  0.20179452 -0.10800661
   0.00607482 -0.08560142  0.10583966 -0.10546222]
 [ 0.17549839  0.0748584  -0.14981726  0.10785606 -0.07824014  0.19501892
   0.20741617 -0.12790385 -0.08925531  0.1826178 ]
 [-0.04629227 -0.0771349  -0.03169568 -0.12004938  0.11143916  0.17776728
   0.04761259 -0.20590176  0.17393235  0.10645085]
 [-0.00200578  0.07253429 -0.09042361  0.20595908 -0.12788662  0.14892686
   0.06970416  0.20809289 -0.0383456  -0.10999999]
 [-0.19769089 -0.11428407 -0.15130014  0.20604257  0.04038094 -0.18950689
  -0.06512274  0.12405556 -0.15381328  0.10468821]
 [ 0.04787551 -0.07327313  0.03824335 -0.1544287   0.15789411  0.07494556
   0.08087348 -0.06283071 -0.04575753 -0.20918702]
 [-0.05153422 -0.02304788 -0.07004272  0.19943706  0.00322638 -0.06845589
   0.01191193  0.20636496 -0.00022081 -0.11887988]
 [-0.0635728  -0.04512662  0.11664102  0.08384682 -0.18653349 -0.01128849
   0.05208873 -0.18828069  0.09392911  0.15917279]
 [-0.19381078  0.0184157   0.19193149  0.08926691  0.03760765  0.07187938
  -0.14938064  0.07564074 -0.15734922  0.19646015]
 [ 0.0697454   0.14007148 -0.02974596 -0.15744213  0.13800016  0.20992114
  -0.19541273 -0.20814638  0.03235703 -0.00964478]
 [ 0.03878859 -0.10955461  0.16594542 -0.00438949 -0.10915029 -0.09937633
   0.12268473  0.05996212  0.14802821 -0.00357381]]
Bias Camada de Entrada: 
[ 0.11786606 -0.01882323  0.11871159 -0.13583587 -0.05283677 -0.16836435
  0.03657105  0.18630783 -0.14359022  0.07346327]
Pesos Camada Escondida: 
[[ 0.11857222]
 [ 0.14892308]
 [ 0.08988893]
 [-0.28625931]
 [-0.12364817]
 [-0.18153944]
 [ 0.21400457]
 [ 0.35973054]
 [ 0.04722524]
 [ 0.21628609]]
Bias Camada Escondida: 
[-0.32542432]
Iteration 1, loss = 0.69030344
Iteration 2, loss = 0.64956568
Iteration 3, loss = 0.66180433
Iteration 4, loss = 0.64051356
Iteration 5, loss = 0.62109150
Iteration 6, loss = 0.60595938
Iteration 7, loss = 0.58928470
Iteration 8, loss = 0.57137262
Iteration 9, loss = 0.54968322
Iteration 10, loss = 0.52702752
Iteration 11, loss = 0.50486266
Iteration 12, loss = 0.47804931
Iteration 13, loss = 0.45938717
Iteration 14, loss = 0.42979153
Iteration 15, loss = 0.40443639
Iteration 16, loss = 0.38346146
Iteration 17, loss = 0.36583488
Iteration 18, loss = 0.35043281
Iteration 19, loss = 0.33624103
Iteration 20, loss = 0.32808368
Iteration 21, loss = 0.32836713
Iteration 22, loss = 0.31203882
Iteration 23, loss = 0.30147396
Iteration 24, loss = 0.29512325
Iteration 25, loss = 0.28859485
Iteration 26, loss = 0.28233951
Iteration 27, loss = 0.27961414
Iteration 28, loss = 0.28243699
Iteration 29, loss = 0.26997150
Iteration 30, loss = 0.26128816
Iteration 31, loss = 0.25858332
Iteration 32, loss = 0.25415651
Iteration 33, loss = 0.24922223
Iteration 34, loss = 0.24456959
Iteration 35, loss = 0.23809973
Iteration 36, loss = 0.23319269
Iteration 37, loss = 0.22915716
Iteration 38, loss = 0.22442465
Iteration 39, loss = 0.21976807
Iteration 40, loss = 0.21608565
Iteration 41, loss = 0.21238214
Iteration 42, loss = 0.20822358
Iteration 43, loss = 0.20247237
Iteration 44, loss = 0.20032311
Iteration 45, loss = 0.19557749
Iteration 46, loss = 0.18913774
Iteration 47, loss = 0.18428033
Iteration 48, loss = 0.18622047
Iteration 49, loss = 0.17841635
Iteration 50, loss = 0.17213878
Iteration 51, loss = 0.16961961
Iteration 52, loss = 0.16584404
Iteration 53, loss = 0.16213432
Iteration 54, loss = 0.15946375
Iteration 55, loss = 0.15676471
Iteration 56, loss = 0.15383296
Iteration 57, loss = 0.15190902
Iteration 58, loss = 0.14948852
Iteration 59, loss = 0.14639724
Iteration 60, loss = 0.14378871
Iteration 61, loss = 0.14169347
Iteration 62, loss = 0.13986867
Iteration 63, loss = 0.13854074
Iteration 64, loss = 0.13522018
Iteration 65, loss = 0.13272817
Iteration 66, loss = 0.13171498
Iteration 67, loss = 0.12875221
Iteration 68, loss = 0.12719826
Iteration 69, loss = 0.12528628
Iteration 70, loss = 0.12387437
Iteration 71, loss = 0.12344317
Iteration 72, loss = 0.11967155
Iteration 73, loss = 0.11790010
Iteration 74, loss = 0.11682072
Iteration 75, loss = 0.11516403
Iteration 76, loss = 0.11348152
Iteration 77, loss = 0.11468927
Iteration 78, loss = 0.11824679
Iteration 79, loss = 0.11423702
Iteration 80, loss = 0.10875197
Iteration 81, loss = 0.10714826
Iteration 82, loss = 0.10534306
Iteration 83, loss = 0.10385937
Iteration 84, loss = 0.10226331
Iteration 85, loss = 0.10123227
Iteration 86, loss = 0.10064569
Iteration 87, loss = 0.09975878
Iteration 88, loss = 0.09782468
Iteration 89, loss = 0.09628260
Iteration 90, loss = 0.09532105
Iteration 91, loss = 0.09425876
Iteration 92, loss = 0.09356470
Iteration 93, loss = 0.09221869
Iteration 94, loss = 0.09075440
Iteration 95, loss = 0.08968542
Iteration 96, loss = 0.08936697
Iteration 97, loss = 0.08831079
Iteration 98, loss = 0.08686141
Iteration 99, loss = 0.08614292
Iteration 100, loss = 0.08519937
Iteration 101, loss = 0.08394837
Iteration 102, loss = 0.08256557
Iteration 103, loss = 0.08154218
Iteration 104, loss = 0.08053964
Iteration 105, loss = 0.07879801
Iteration 106, loss = 0.07966883
Iteration 107, loss = 0.07978810
Iteration 108, loss = 0.07679233
Iteration 109, loss = 0.07485423
Iteration 110, loss = 0.07563796
Iteration 111, loss = 0.07631803
Iteration 112, loss = 0.07450301
Iteration 113, loss = 0.07182086
Iteration 114, loss = 0.07135089
Iteration 115, loss = 0.07137389
Iteration 116, loss = 0.07063371
Iteration 117, loss = 0.06852978
Iteration 118, loss = 0.06926395
Iteration 119, loss = 0.06806118
Iteration 120, loss = 0.06610304
Iteration 121, loss = 0.06581262
Iteration 122, loss = 0.06488331
Iteration 123, loss = 0.06404465
Iteration 124, loss = 0.06298486
Iteration 125, loss = 0.06234555
Iteration 126, loss = 0.06179138
Iteration 127, loss = 0.06149479
Iteration 128, loss = 0.06088872
Iteration 129, loss = 0.05966072
Iteration 130, loss = 0.05889546
Iteration 131, loss = 0.05842554
Iteration 132, loss = 0.05811903
Iteration 133, loss = 0.05739334
Iteration 134, loss = 0.05760567
Iteration 135, loss = 0.05721943
Iteration 136, loss = 0.05676407
Iteration 137, loss = 0.05640214
Iteration 138, loss = 0.05522069
Iteration 139, loss = 0.05419508
Iteration 140, loss = 0.05347368
Iteration 141, loss = 0.05267039
Iteration 142, loss = 0.05264270
Iteration 143, loss = 0.05211316
Iteration 144, loss = 0.05189766
Iteration 145, loss = 0.05111462
Iteration 146, loss = 0.05044994
Iteration 147, loss = 0.05009256
Iteration 148, loss = 0.04931379
Iteration 149, loss = 0.05112377
Iteration 150, loss = 0.04998909
Iteration 151, loss = 0.04807425
Iteration 152, loss = 0.04724639
Iteration 153, loss = 0.04693601
Iteration 154, loss = 0.04682312
Iteration 155, loss = 0.04668039
Iteration 156, loss = 0.04650968
Iteration 157, loss = 0.04633445
Iteration 158, loss = 0.04601021
Iteration 159, loss = 0.04582980
Iteration 160, loss = 0.04549423
Iteration 161, loss = 0.04385973
Iteration 162, loss = 0.04365573
Iteration 163, loss = 0.04271989
Iteration 164, loss = 0.04261115
Iteration 165, loss = 0.04214489
Iteration 166, loss = 0.04170421
Iteration 167, loss = 0.04110481
Iteration 168, loss = 0.04169735
Iteration 169, loss = 0.04069752
Iteration 170, loss = 0.04005624
Iteration 171, loss = 0.03964427
Iteration 172, loss = 0.03938111
Iteration 173, loss = 0.03920127
Iteration 174, loss = 0.03919757
Iteration 175, loss = 0.03932976
Iteration 176, loss = 0.03989148
Iteration 177, loss = 0.04046217
Iteration 178, loss = 0.03924847
Iteration 179, loss = 0.03775958
Iteration 180, loss = 0.03712848
Iteration 181, loss = 0.03746356
Iteration 182, loss = 0.03790923
Iteration 183, loss = 0.03748146
Iteration 184, loss = 0.03639025
Iteration 185, loss = 0.03596882
Iteration 186, loss = 0.03554955
Iteration 187, loss = 0.03506499
Iteration 188, loss = 0.03461177
Iteration 189, loss = 0.03488739
Iteration 190, loss = 0.03420493
Iteration 191, loss = 0.03378938
Iteration 192, loss = 0.03389273
Iteration 193, loss = 0.03427891
Iteration 194, loss = 0.03342255
Iteration 195, loss = 0.03297422
Iteration 196, loss = 0.03281492
Iteration 197, loss = 0.03256479
Iteration 198, loss = 0.03241917
Iteration 199, loss = 0.03161256
Iteration 200, loss = 0.03214913
Iteration 201, loss = 0.03167896
Iteration 202, loss = 0.03105126
Iteration 203, loss = 0.03068186
Iteration 204, loss = 0.03043747
Iteration 205, loss = 0.03034030
Iteration 206, loss = 0.03024544
Iteration 207, loss = 0.02941857
Iteration 208, loss = 0.02969287
Iteration 209, loss = 0.03052535
Iteration 210, loss = 0.03141829
Iteration 211, loss = 0.03145125
Iteration 212, loss = 0.03046877
Iteration 213, loss = 0.02939733
Iteration 214, loss = 0.02852676
Iteration 215, loss = 0.02812788
Iteration 216, loss = 0.02810881
Iteration 217, loss = 0.02800461
Iteration 218, loss = 0.02763944
Iteration 219, loss = 0.02720961
Iteration 220, loss = 0.02687047
Iteration 221, loss = 0.02656849
Iteration 222, loss = 0.02636662
Iteration 223, loss = 0.02637539
Iteration 224, loss = 0.02637754
Iteration 225, loss = 0.02606935
Iteration 226, loss = 0.02577210
Iteration 227, loss = 0.02555714
Iteration 228, loss = 0.02534858
Iteration 229, loss = 0.02504169
Iteration 230, loss = 0.02490583
Iteration 231, loss = 0.02499046
Iteration 232, loss = 0.02471869
Iteration 233, loss = 0.02449723
Iteration 234, loss = 0.02426440
Iteration 235, loss = 0.02413901
Iteration 236, loss = 0.02409940
Iteration 237, loss = 0.02424286
Iteration 238, loss = 0.02392406
Iteration 239, loss = 0.02373178
Iteration 240, loss = 0.02342842
Iteration 241, loss = 0.02318624
Iteration 242, loss = 0.02304553
Iteration 243, loss = 0.02285530
Iteration 244, loss = 0.02278336
Iteration 245, loss = 0.02267779
Iteration 246, loss = 0.02242325
Iteration 247, loss = 0.02238016
Iteration 248, loss = 0.02238643
Iteration 249, loss = 0.02227781
Iteration 250, loss = 0.02224137
Iteration 251, loss = 0.02205491
Iteration 252, loss = 0.02209103
Iteration 253, loss = 0.02177210
Iteration 254, loss = 0.02159545
Iteration 255, loss = 0.02137385
Iteration 256, loss = 0.02107296
Iteration 257, loss = 0.02084116
Iteration 258, loss = 0.02060757
Iteration 259, loss = 0.02054490
Iteration 260, loss = 0.02036370
Iteration 261, loss = 0.02019971
Iteration 262, loss = 0.02015630
Iteration 263, loss = 0.02003392
Iteration 264, loss = 0.01996709
Iteration 265, loss = 0.01963375
Iteration 266, loss = 0.01992218
Iteration 267, loss = 0.01995386
Iteration 268, loss = 0.01949873
Iteration 269, loss = 0.01935890
Iteration 270, loss = 0.01934740
Iteration 271, loss = 0.01921281
Iteration 272, loss = 0.01897915
Iteration 273, loss = 0.01882472
Iteration 274, loss = 0.01858877
Iteration 275, loss = 0.01857919
Iteration 276, loss = 0.01852409
Iteration 277, loss = 0.01854904
Iteration 278, loss = 0.01840639
Iteration 279, loss = 0.01795053
Iteration 280, loss = 0.01775704
Iteration 281, loss = 0.01783124
Iteration 282, loss = 0.01813192
Iteration 283, loss = 0.01782319
Iteration 284, loss = 0.01746937
Iteration 285, loss = 0.01727929
Iteration 286, loss = 0.01737797
Iteration 287, loss = 0.01727892
Iteration 288, loss = 0.01714853
Iteration 289, loss = 0.01693667
Iteration 290, loss = 0.01664471
Iteration 291, loss = 0.01657875
Iteration 292, loss = 0.01652427
Iteration 293, loss = 0.01670127
Iteration 294, loss = 0.01616856
Iteration 295, loss = 0.01653279
Iteration 296, loss = 0.01649309
Iteration 297, loss = 0.01611312
Iteration 298, loss = 0.01586687
Iteration 299, loss = 0.01582614
Iteration 300, loss = 0.01585932
PARAMETROS DE INICIALIZACAO DA REDE
NUMERO DE NEURONIOS 
Camada de Entrada: 34
Camada Escondida: 10
Camada de Saida: 1

--- PARAMETROS DE CONFIGURACAO DA REDE ---
Numero de Epocas: 300
Taxa de Aprendizado: adaptive
Taxa de Aprendizado Inicial: 0.3
METRICAS

RESULTADOS:

[0 0 1 0 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 1 1
 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 1 1 1
 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 1 0]
ACURACIA: 0.9245283018867925

