Pesos Camada de Entrada: 
[[ 8.84583812e-02 -1.79374782e-01 -9.31096036e-03 -2.04417442e-01
  -1.89792526e-01  1.98520336e-01  1.21230707e-01 -4.84826582e-03
  -8.58325831e-02  9.74114832e-03]
 [-3.15818889e-02 -1.27384544e-01  7.98252872e-02 -4.80881370e-02
  -2.06860477e-01  1.08002831e-01  5.14311978e-02 -1.32240466e-01
  -3.42814539e-02 -2.00557363e-01]
 [-6.92079181e-02 -1.13879763e-01  7.75598140e-02  8.02657630e-02
  -1.46422218e-01  1.88206431e-01  1.15582377e-01  1.76526674e-01
  -3.45283103e-02  8.70218295e-02]
 [-3.07325102e-02  8.49158823e-02  3.41199533e-02  1.24405347e-01
  -5.57727759e-02  1.79353397e-01  1.52034211e-01 -2.12830514e-01
  -4.49498150e-02 -1.49889735e-01]
 [ 7.06917661e-02  3.60732419e-02 -8.87806196e-02 -1.05700366e-01
   9.25590922e-02  2.04889630e-01 -1.43602398e-01 -9.69836170e-02
  -1.25655449e-01  1.48893495e-01]
 [-1.49335178e-05  1.48336456e-01  1.23298574e-01  4.67139798e-02
   3.28660423e-02  4.00919943e-02  1.07445985e-01 -1.21665474e-02
  -1.13376531e-01 -1.06724394e-01]
 [-6.90289661e-02 -1.37351604e-01 -6.02297406e-02  1.76260054e-01
  -3.32648830e-02  1.59545287e-01 -1.00792066e-01  8.17355439e-02
  -1.29620904e-01 -1.27446839e-01]
 [ 1.74450121e-01 -2.11499822e-01  2.44356385e-02  1.90335969e-01
  -4.29453346e-02  1.93378305e-01  1.96427944e-01 -1.93641735e-01
  -1.26842687e-01 -1.18331098e-03]
 [-5.63745379e-02 -1.37227159e-01 -1.92205565e-01  1.46918277e-01
   1.33570189e-01  1.68504319e-01  1.80184614e-01  4.90536201e-02
  -1.74926972e-01  3.25311240e-02]
 [ 7.83847173e-02  1.95267237e-01 -5.07825241e-02  7.12125527e-02
  -7.72309876e-02 -1.85511738e-01  1.75113637e-01  7.46322223e-02
  -5.04015281e-02 -1.49275903e-01]
 [ 9.17206418e-02  1.92985972e-01 -4.87342262e-02 -1.62490140e-01
   9.26759941e-02 -1.19202468e-01  1.26459180e-01 -5.17368223e-03
   4.20219469e-02  1.89963883e-01]
 [ 1.19525355e-01  1.85967843e-02  1.83923552e-01 -1.64820690e-01
  -4.63890251e-02 -1.28777250e-01 -1.31019250e-01  2.25842020e-02
  -1.80349656e-01 -1.70591343e-01]
 [-2.03658427e-01 -1.28677183e-01  7.57061956e-02 -9.17211550e-02
  -4.17871472e-02 -9.10801804e-03 -1.87902035e-01  1.86676848e-01
  -1.74883152e-01  1.69856881e-01]
 [ 7.03689473e-02  1.26243064e-01 -4.29540730e-02  1.59110886e-01
  -1.61026227e-01  6.63267709e-02 -6.09014406e-02  7.61066636e-02
   1.02244102e-01 -6.41675156e-02]
 [-5.92072317e-02  5.87097572e-02 -1.95415527e-01 -1.05904025e-01
  -1.79757084e-01  1.41086300e-01  1.56395105e-01 -1.15696187e-01
   1.44937196e-01 -1.87982776e-01]
 [ 1.90179034e-01 -2.08930159e-01 -9.60956097e-02  1.34820423e-01
   2.41104200e-02 -4.39197894e-04  1.58273233e-01 -2.10091862e-01
  -1.28873625e-01 -1.18828134e-01]
 [ 1.76918977e-01 -1.76566990e-01  8.09892193e-02 -1.95229114e-01
   2.03484498e-01 -1.15784509e-01 -8.10695085e-04  1.32436701e-01
  -1.88502773e-01  1.50150562e-01]
 [ 5.16204919e-02 -1.24956972e-01 -9.14196299e-02  1.61977750e-01
   1.14075134e-01 -8.57285629e-02  9.40789300e-02  1.62091034e-01
   7.30777412e-02 -4.01151142e-02]
 [-2.99698584e-02  1.82984237e-01 -9.47923916e-02 -2.09875045e-01
   1.06645335e-01  1.72216863e-01 -7.55565818e-03  1.01419709e-01
  -8.06340993e-02  5.14544654e-02]
 [ 9.13985136e-02  6.90104268e-02 -1.80896647e-01 -1.07451329e-01
  -1.25413323e-01 -1.60480905e-01  3.57230168e-02 -6.20706519e-02
  -2.76472169e-02 -1.37846232e-01]
 [-1.89395094e-01  7.84947274e-02 -1.70740988e-01  6.07548662e-02
  -1.20522736e-01  1.46095358e-01 -1.60051431e-01 -1.54867336e-01
   2.02855832e-01  9.26858535e-02]
 [-3.56865952e-02  9.01431640e-02 -6.69565231e-02 -6.70854384e-02
   4.01827315e-02  4.48673521e-02  4.48623141e-02 -1.75673649e-01
  -8.19014596e-02  7.29288528e-02]
 [ 1.03842258e-01  1.27679760e-01  5.32689365e-02 -1.87157211e-01
  -1.11363606e-01  7.66039568e-02 -5.85364223e-02 -1.17972509e-02
  -4.57028344e-02  1.84180326e-01]
 [ 8.30384377e-02 -5.86682720e-02  2.68886957e-02  4.78009933e-02
   7.68629588e-02 -1.83222356e-01  7.02621590e-02  5.93874615e-02
   2.05864088e-01  2.55782261e-03]
 [ 2.73406367e-02  7.88293445e-02 -2.10972868e-01 -1.14710435e-01
   5.82119857e-02 -1.04189827e-02 -1.57082225e-01  1.70356491e-01
  -6.05201230e-02  8.80387133e-02]
 [-7.12992570e-03 -9.08725404e-02  1.50129642e-01 -9.27665883e-02
  -5.47083725e-02 -1.49760327e-01 -5.44670263e-02  1.54793694e-01
  -1.21220848e-01  3.49280250e-02]
 [ 2.72891763e-02  1.53647639e-01 -1.48625585e-01  1.33785113e-01
   1.17608857e-01 -2.02776317e-01 -4.15476731e-02  1.62635657e-01
  -1.03884860e-01 -1.96475628e-01]
 [-9.78494704e-03  6.87649049e-02  1.02268976e-01 -1.44763998e-01
   1.59203895e-03  1.97408958e-01  1.43700036e-01 -5.17187967e-02
  -2.50381001e-02 -2.12357381e-01]
 [-1.34445523e-01 -8.13987092e-02 -2.08064062e-01 -2.10170766e-01
   1.10841109e-01  1.99612745e-01 -1.42694400e-01 -6.99038531e-02
  -3.80290574e-02 -1.65931538e-01]
 [ 5.13090655e-03 -9.12296519e-02 -1.40407525e-01  2.12543587e-01
   1.24179744e-01 -1.25765003e-01  6.38065949e-03  6.51451909e-02
   1.68122523e-01  1.23903621e-01]
 [ 1.26831046e-02  1.23977986e-01 -1.65489608e-03  4.84087112e-02
  -5.77735175e-02 -1.32134116e-01  9.38688401e-02 -3.24555947e-02
  -3.56487318e-02 -7.73783128e-02]
 [-1.92657356e-01  1.19175407e-01  2.77808336e-02  2.86911067e-02
  -7.06281038e-02 -2.11331182e-01 -2.23785451e-02  1.95763391e-01
  -6.55560012e-02  1.09041618e-01]
 [-1.44304226e-01  9.33715634e-02  3.01931470e-02  8.32755247e-02
  -1.83796718e-01  1.73190665e-01 -1.71816736e-01  1.33775493e-01
  -1.23472606e-01  7.12900474e-02]
 [-4.48004686e-02  1.76736274e-01  1.42709854e-01 -9.78824903e-02
  -1.55121280e-01 -1.90986581e-01  2.07072121e-01 -1.83514412e-01
   1.35694217e-01 -2.88915155e-02]]
Bias Camada de Entrada: 
[ 0.18892192 -0.05591661 -0.19904342  0.03477258 -0.18502775 -0.02958254
 -0.05830375  0.05531141  0.17834873 -0.16369628]
Pesos Camada Escondida: 
[[-0.40826329]
 [-0.28525347]
 [-0.09154342]
 [ 0.08249613]
 [ 0.37402211]
 [-0.26012759]
 [ 0.06209925]
 [-0.05977704]
 [-0.17240367]
 [ 0.04586564]]
Bias Camada Escondida: 
[0.0356904]
Iteration 1, loss = 0.74913914
Iteration 2, loss = 0.66391434
Iteration 3, loss = 0.64371812
Iteration 4, loss = 0.60696965
Iteration 5, loss = 0.57723590
Iteration 6, loss = 0.53927613
Iteration 7, loss = 0.49758928
Iteration 8, loss = 0.45747538
Iteration 9, loss = 0.40175027
Iteration 10, loss = 0.36698757
Iteration 11, loss = 0.34042246
Iteration 12, loss = 0.32372335
Iteration 13, loss = 0.32511297
Iteration 14, loss = 0.30089151
Iteration 15, loss = 0.29675948
Iteration 16, loss = 0.28208211
Iteration 17, loss = 0.27282142
Iteration 18, loss = 0.26262723
Iteration 19, loss = 0.25445536
Iteration 20, loss = 0.25160679
Iteration 21, loss = 0.24357583
Iteration 22, loss = 0.22917458
Iteration 23, loss = 0.22314556
Iteration 24, loss = 0.22034065
Iteration 25, loss = 0.20770554
Iteration 26, loss = 0.19960027
Iteration 27, loss = 0.19219332
Iteration 28, loss = 0.19091436
Iteration 29, loss = 0.18061031
Iteration 30, loss = 0.17846299
Iteration 31, loss = 0.17648367
Iteration 32, loss = 0.16444048
Iteration 33, loss = 0.16214275
Iteration 34, loss = 0.15492739
Iteration 35, loss = 0.14759966
Iteration 36, loss = 0.14617540
Iteration 37, loss = 0.14064168
Iteration 38, loss = 0.13668695
Iteration 39, loss = 0.13838178
Iteration 40, loss = 0.13106331
Iteration 41, loss = 0.12631803
Iteration 42, loss = 0.12528249
Iteration 43, loss = 0.12329033
Iteration 44, loss = 0.11854748
Iteration 45, loss = 0.11740217
Iteration 46, loss = 0.11434657
Iteration 47, loss = 0.11248918
Iteration 48, loss = 0.11038511
Iteration 49, loss = 0.10870085
Iteration 50, loss = 0.10671549
Iteration 51, loss = 0.10435734
Iteration 52, loss = 0.10639149
Iteration 53, loss = 0.11836610
Iteration 54, loss = 0.10595772
Iteration 55, loss = 0.09917841
Iteration 56, loss = 0.09662616
Iteration 57, loss = 0.09459898
Iteration 58, loss = 0.09386031
Iteration 59, loss = 0.09240087
Iteration 60, loss = 0.09057155
Iteration 61, loss = 0.08695705
Iteration 62, loss = 0.08667897
Iteration 63, loss = 0.08785183
Iteration 64, loss = 0.08405337
Iteration 65, loss = 0.08039331
Iteration 66, loss = 0.07879375
Iteration 67, loss = 0.07926324
Iteration 68, loss = 0.08030072
Iteration 69, loss = 0.07778205
Iteration 70, loss = 0.07761417
Iteration 71, loss = 0.07202658
Iteration 72, loss = 0.07029290
Iteration 73, loss = 0.07139184
Iteration 74, loss = 0.07066273
Iteration 75, loss = 0.06704542
Iteration 76, loss = 0.06627606
Iteration 77, loss = 0.06620011
Iteration 78, loss = 0.06395709
Iteration 79, loss = 0.06215574
Iteration 80, loss = 0.06280202
Iteration 81, loss = 0.06116066
Iteration 82, loss = 0.05927043
Iteration 83, loss = 0.05920469
Iteration 84, loss = 0.05862502
Iteration 85, loss = 0.05643723
Iteration 86, loss = 0.05657549
Iteration 87, loss = 0.05552435
Iteration 88, loss = 0.05362648
Iteration 89, loss = 0.05354792
Iteration 90, loss = 0.05130100
Iteration 91, loss = 0.05163835
Iteration 92, loss = 0.04909292
Iteration 93, loss = 0.04827664
Iteration 94, loss = 0.04748051
Iteration 95, loss = 0.04744041
Iteration 96, loss = 0.04476209
Iteration 97, loss = 0.04535203
Iteration 98, loss = 0.04540656
Iteration 99, loss = 0.04275721
Iteration 100, loss = 0.04242851
Iteration 101, loss = 0.04272960
Iteration 102, loss = 0.04372356
Iteration 103, loss = 0.03937276
Iteration 104, loss = 0.03860081
Iteration 105, loss = 0.03700916
Iteration 106, loss = 0.03928107
Iteration 107, loss = 0.04297746
Iteration 108, loss = 0.03679680
Iteration 109, loss = 0.03411058
Iteration 110, loss = 0.03592018
Iteration 111, loss = 0.03415218
Iteration 112, loss = 0.03120837
Iteration 113, loss = 0.03291795
Iteration 114, loss = 0.03333536
Iteration 115, loss = 0.02985023
Iteration 116, loss = 0.02914927
Iteration 117, loss = 0.02892707
Iteration 118, loss = 0.02817368
Iteration 119, loss = 0.02766037
Iteration 120, loss = 0.02760097
Iteration 121, loss = 0.02816250
Iteration 122, loss = 0.02772716
Iteration 123, loss = 0.02569717
Iteration 124, loss = 0.02501033
Iteration 125, loss = 0.02429886
Iteration 126, loss = 0.02349416
Iteration 127, loss = 0.02333912
Iteration 128, loss = 0.02294888
Iteration 129, loss = 0.02260923
Iteration 130, loss = 0.02262866
Iteration 131, loss = 0.02283395
Iteration 132, loss = 0.02234412
Iteration 133, loss = 0.02131991
Iteration 134, loss = 0.02165480
Iteration 135, loss = 0.02095898
Iteration 136, loss = 0.02021381
Iteration 137, loss = 0.02016867
Iteration 138, loss = 0.02045487
Iteration 139, loss = 0.01957011
Iteration 140, loss = 0.01959316
Iteration 141, loss = 0.01945714
Iteration 142, loss = 0.01900872
Iteration 143, loss = 0.01876184
Iteration 144, loss = 0.01820934
Iteration 145, loss = 0.01778752
Iteration 146, loss = 0.01766822
Iteration 147, loss = 0.01743083
Iteration 148, loss = 0.01698233
Iteration 149, loss = 0.01651466
Iteration 150, loss = 0.01614633
Iteration 151, loss = 0.01588490
Iteration 152, loss = 0.01576810
Iteration 153, loss = 0.01577430
Iteration 154, loss = 0.01603101
Iteration 155, loss = 0.01590368
Iteration 156, loss = 0.01516233
Iteration 157, loss = 0.01479702
Iteration 158, loss = 0.01461418
Iteration 159, loss = 0.01439838
Iteration 160, loss = 0.01429255
Iteration 161, loss = 0.01414184
Iteration 162, loss = 0.01403549
Iteration 163, loss = 0.01379080
Iteration 164, loss = 0.01374977
Iteration 165, loss = 0.01354687
Iteration 166, loss = 0.01337010
Iteration 167, loss = 0.01317946
Iteration 168, loss = 0.01300647
Iteration 169, loss = 0.01292665
Iteration 170, loss = 0.01272641
Iteration 171, loss = 0.01263897
Iteration 172, loss = 0.01247801
Iteration 173, loss = 0.01251042
Iteration 174, loss = 0.01270018
Iteration 175, loss = 0.01207610
Iteration 176, loss = 0.01194404
Iteration 177, loss = 0.01181599
Iteration 178, loss = 0.01165161
Iteration 179, loss = 0.01155196
Iteration 180, loss = 0.01146017
Iteration 181, loss = 0.01163325
Iteration 182, loss = 0.01141104
Iteration 183, loss = 0.01121031
Iteration 184, loss = 0.01089015
Iteration 185, loss = 0.01091155
Iteration 186, loss = 0.01067701
Iteration 187, loss = 0.01056549
Iteration 188, loss = 0.01039117
Iteration 189, loss = 0.01036911
Iteration 190, loss = 0.01016258
Iteration 191, loss = 0.01026603
Iteration 192, loss = 0.01007893
Iteration 193, loss = 0.01000443
Iteration 194, loss = 0.00994040
Iteration 195, loss = 0.00990334
Iteration 196, loss = 0.00984215
Iteration 197, loss = 0.00987204
Iteration 198, loss = 0.00957122
Iteration 199, loss = 0.00936110
Iteration 200, loss = 0.00921979
Iteration 201, loss = 0.00918476
Iteration 202, loss = 0.00919931
Iteration 203, loss = 0.00915998
Iteration 204, loss = 0.00907039
Iteration 205, loss = 0.00896708
Iteration 206, loss = 0.00881638
Iteration 207, loss = 0.00867504
Iteration 208, loss = 0.00861789
Iteration 209, loss = 0.00848750
Iteration 210, loss = 0.00843746
Iteration 211, loss = 0.00838759
Iteration 212, loss = 0.00832612
Iteration 213, loss = 0.00823861
Iteration 214, loss = 0.00815512
Iteration 215, loss = 0.00806620
Iteration 216, loss = 0.00800309
Iteration 217, loss = 0.00800623
Iteration 218, loss = 0.00802271
Iteration 219, loss = 0.00823146
Iteration 220, loss = 0.00777378
Iteration 221, loss = 0.00792148
Iteration 222, loss = 0.00776932
Iteration 223, loss = 0.00767932
Iteration 224, loss = 0.00758359
Iteration 225, loss = 0.00751816
Iteration 226, loss = 0.00748346
Iteration 227, loss = 0.00740760
Iteration 228, loss = 0.00734610
Iteration 229, loss = 0.00740883
Iteration 230, loss = 0.00717535
Iteration 231, loss = 0.00708219
Iteration 232, loss = 0.00706407
Iteration 233, loss = 0.00710667
Iteration 234, loss = 0.00704924
Iteration 235, loss = 0.00681089
Iteration 236, loss = 0.00695353
Iteration 237, loss = 0.00700503
Iteration 238, loss = 0.00686880
Iteration 239, loss = 0.00675863
Iteration 240, loss = 0.00662616
Iteration 241, loss = 0.00656158
Iteration 242, loss = 0.00652186
Iteration 243, loss = 0.00649747
Iteration 244, loss = 0.00649198
Iteration 245, loss = 0.00648919
Iteration 246, loss = 0.00645151
Iteration 247, loss = 0.00637336
Iteration 248, loss = 0.00631334
Iteration 249, loss = 0.00627209
Iteration 250, loss = 0.00623823
Iteration 251, loss = 0.00621764
Iteration 252, loss = 0.00622127
Iteration 253, loss = 0.00612867
Iteration 254, loss = 0.00603541
Iteration 255, loss = 0.00597510
Iteration 256, loss = 0.00595225
Iteration 257, loss = 0.00594190
Iteration 258, loss = 0.00586758
Iteration 259, loss = 0.00585160
Iteration 260, loss = 0.00585912
Iteration 261, loss = 0.00585763
Iteration 262, loss = 0.00582421
Iteration 263, loss = 0.00579411
Iteration 264, loss = 0.00576095
Iteration 265, loss = 0.00574665
Iteration 266, loss = 0.00571454
Iteration 267, loss = 0.00565790
Iteration 268, loss = 0.00559987
Iteration 269, loss = 0.00557168
Iteration 270, loss = 0.00558953
Iteration 271, loss = 0.00559502
Iteration 272, loss = 0.00553997
Iteration 273, loss = 0.00546433
Iteration 274, loss = 0.00541433
Iteration 275, loss = 0.00533048
Iteration 276, loss = 0.00527931
Iteration 277, loss = 0.00530717
Iteration 278, loss = 0.00526855
Iteration 279, loss = 0.00523461
Iteration 280, loss = 0.00518466
Iteration 281, loss = 0.00515882
Iteration 282, loss = 0.00510542
Iteration 283, loss = 0.00508368
Iteration 284, loss = 0.00509067
Iteration 285, loss = 0.00506166
Iteration 286, loss = 0.00502705
Iteration 287, loss = 0.00500034
Iteration 288, loss = 0.00496789
Iteration 289, loss = 0.00492954
Iteration 290, loss = 0.00491620
Iteration 291, loss = 0.00487779
Iteration 292, loss = 0.00485490
Iteration 293, loss = 0.00484289
Iteration 294, loss = 0.00476663
Iteration 295, loss = 0.00480767
Iteration 296, loss = 0.00476083
Iteration 297, loss = 0.00471131
Iteration 298, loss = 0.00466372
Iteration 299, loss = 0.00462053
Iteration 300, loss = 0.00460044
PARAMETROS DE INICIALIZACAO DA REDE
NUMERO DE NEURONIOS 
Camada de Entrada: 34
Camada Escondida: 10
Camada de Saida: 1

--- PARAMETROS DE CONFIGURACAO DA REDE ---
Numero de Epocas: 300
Taxa de Aprendizado: adaptive
Taxa de Aprendizado Inicial: 0.6
METRICAS

RESULTADOS:

[0 0 1 0 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1 1 1 1 1 1 1
 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 1 1 1
 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 1 0]
ACURACIA: 0.9339622641509434

