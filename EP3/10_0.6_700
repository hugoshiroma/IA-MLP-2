Pesos Camada de Entrada: 
[[-0.08327884  0.17660702  0.05363813  0.00527337 -0.05039983  0.19437266
  -0.11190321 -0.13665028  0.10004734  0.10114417]
 [-0.06281283  0.03580852  0.11876408 -0.06844202 -0.15398099  0.04289859
  -0.08586675 -0.03367464 -0.14446026 -0.05826129]
 [ 0.1671309   0.00537538 -0.1657773   0.00691208  0.19239247 -0.04839556
  -0.04236769  0.1375251   0.09487258 -0.00042973]
 [ 0.06614469 -0.08902785 -0.20395305 -0.09927995  0.0900262   0.03360659
   0.03517789 -0.02261962 -0.06516017  0.1701288 ]
 [ 0.2095129   0.04122611 -0.13779554  0.11908447 -0.0832991   0.10659221
   0.003341    0.15060369  0.06973526  0.15385807]
 [ 0.08749796 -0.05910939  0.0237415   0.10282345  0.03848162 -0.18815468
   0.179295    0.16074752 -0.16804764 -0.01583432]
 [-0.10962302 -0.12852172  0.19556415 -0.0770639   0.07496616  0.17290832
   0.00296056  0.01199877  0.01412462 -0.20989704]
 [-0.18214224 -0.04454552  0.10849198  0.17353306  0.20442239 -0.10115417
   0.10191261 -0.16771455 -0.02967407 -0.09828032]
 [ 0.03912895 -0.19492881  0.20627691 -0.18668597 -0.1021577  -0.06478794
  -0.02939488 -0.13223833  0.17802097  0.05490592]
 [ 0.07258421  0.07101384 -0.13386658  0.07191744  0.17180603 -0.1469518
  -0.11570364  0.04613793 -0.03458254 -0.2056019 ]
 [-0.19188819 -0.0445445   0.14919325 -0.14192278 -0.15456882  0.11720265
   0.14987021 -0.20085693 -0.04161995  0.1780207 ]
 [-0.06652159  0.18911286  0.05326428 -0.06642551 -0.18377526 -0.06117161
  -0.08467531  0.11827754  0.08317912 -0.15684503]
 [ 0.17941453  0.04187632 -0.09073995  0.18315526  0.1102689   0.19195859
  -0.146735    0.09818913 -0.19059204  0.03428729]
 [-0.06331266 -0.1776977  -0.20500138  0.05863764  0.08624061  0.13507116
   0.12919076 -0.04833055  0.17712102 -0.21078972]
 [-0.18482095 -0.03426119  0.12734573  0.16218039 -0.1243678   0.20539699
   0.18936893 -0.0061165   0.08257008  0.08606529]
 [-0.03748447 -0.16651018 -0.17966225 -0.0142256   0.10709936  0.0745345
  -0.00564334  0.00621539 -0.07264399  0.07120263]
 [-0.01627095  0.16913373 -0.11342578 -0.17205745  0.08540394 -0.01758628
  -0.20544567 -0.02660695 -0.12078117  0.08893332]
 [-0.14055293  0.06666478  0.19980735 -0.11238188 -0.05917175  0.09183387
   0.13041165 -0.07997223  0.07798956  0.18706828]
 [-0.06653529 -0.05926581 -0.01516772  0.11115253 -0.08772601 -0.19901153
  -0.19697899 -0.09898422 -0.06338917 -0.03293436]
 [ 0.18382037 -0.04031271  0.05123023  0.06588434  0.19871313  0.16125616
  -0.20577275 -0.20679259  0.14288559  0.19773315]
 [ 0.04777787 -0.04126245 -0.00681468 -0.18207068 -0.20015092  0.20844093
  -0.15318284 -0.19635237 -0.12797582 -0.12650697]
 [-0.15327245 -0.0672677   0.13910359 -0.17627026  0.1988136  -0.01256592
   0.13082376  0.021721   -0.10630178 -0.18910404]
 [ 0.03685681  0.01651101  0.09491431  0.20366506  0.18216015 -0.19382885
   0.1473465   0.00488787  0.19896761  0.14481368]
 [ 0.04701299  0.13122915 -0.19060242 -0.13925895 -0.15720148 -0.0198952
  -0.1929782  -0.1032599  -0.10939072 -0.15351343]
 [ 0.17722598 -0.09280511 -0.20808316  0.16439191  0.19983527  0.18101377
   0.04665595  0.12452409  0.09100683  0.00883107]
 [ 0.07311671  0.16312517  0.06760055  0.04478348  0.17678823 -0.10179376
  -0.12383157 -0.16825792  0.0041673   0.05697999]
 [ 0.10626707 -0.11881371 -0.03169897 -0.05666196 -0.13036097 -0.16707187
   0.02500402  0.14945226 -0.11531203 -0.10690846]
 [-0.03385014  0.06228912  0.05827205 -0.03811716  0.1025497  -0.19210891
  -0.06651925 -0.02640766  0.08861893  0.00414598]
 [-0.1757275  -0.05129711 -0.06253142 -0.03139847  0.09072352 -0.0969037
  -0.030976   -0.07923378 -0.17208116  0.18791782]
 [ 0.11197973 -0.16053007  0.20553004  0.12245635  0.02761233  0.1820695
  -0.13627398  0.21243425 -0.04881784  0.1777697 ]
 [ 0.04499982 -0.15927588  0.01771488 -0.04286853 -0.19080485 -0.20420242
   0.05509383 -0.02974151  0.15470345 -0.02435966]
 [-0.0066413   0.14300229  0.14780292  0.08839527  0.18877404 -0.07438916
  -0.0573149   0.16788321  0.21103887  0.1838759 ]
 [-0.02349404  0.1549255  -0.15491239  0.09955469  0.12958765  0.04888153
  -0.03793045  0.03078829  0.16559933 -0.00136045]
 [-0.09787043  0.18137015 -0.01544061 -0.06079249  0.05253342  0.16602947
   0.04457248  0.08255134  0.04629233  0.05689975]]
Bias Camada de Entrada: 
[-0.11925159 -0.05813106 -0.1808388   0.07140263 -0.01616391  0.09954309
 -0.00412821  0.01239277 -0.10915977  0.16302988]
Pesos Camada Escondida: 
[[ 0.05553003]
 [ 0.2208095 ]
 [ 0.42241683]
 [ 0.22423184]
 [-0.1750218 ]
 [-0.39268719]
 [-0.32536231]
 [ 0.21820481]
 [ 0.29516309]
 [-0.39932962]]
Bias Camada Escondida: 
[-0.09294333]
Iteration 1, loss = 0.70548447
Iteration 2, loss = 0.64472283
Iteration 3, loss = 0.64321841
Iteration 4, loss = 0.58876682
Iteration 5, loss = 0.55397742
Iteration 6, loss = 0.51797113
Iteration 7, loss = 0.48250401
Iteration 8, loss = 0.45555762
Iteration 9, loss = 0.40072649
Iteration 10, loss = 0.38941890
Iteration 11, loss = 0.35917446
Iteration 12, loss = 0.33054185
Iteration 13, loss = 0.33827094
Iteration 14, loss = 0.30690144
Iteration 15, loss = 0.28791982
Iteration 16, loss = 0.27856206
Iteration 17, loss = 0.27027338
Iteration 18, loss = 0.26273052
Iteration 19, loss = 0.25306388
Iteration 20, loss = 0.24576904
Iteration 21, loss = 0.23831616
Iteration 22, loss = 0.22528412
Iteration 23, loss = 0.21413531
Iteration 24, loss = 0.20618863
Iteration 25, loss = 0.20432993
Iteration 26, loss = 0.18992160
Iteration 27, loss = 0.18169082
Iteration 28, loss = 0.17519578
Iteration 29, loss = 0.16822589
Iteration 30, loss = 0.16153824
Iteration 31, loss = 0.15764636
Iteration 32, loss = 0.15203036
Iteration 33, loss = 0.14632971
Iteration 34, loss = 0.13981117
Iteration 35, loss = 0.14114759
Iteration 36, loss = 0.14402951
Iteration 37, loss = 0.13126923
Iteration 38, loss = 0.12571264
Iteration 39, loss = 0.12627551
Iteration 40, loss = 0.12416089
Iteration 41, loss = 0.11493457
Iteration 42, loss = 0.11647946
Iteration 43, loss = 0.11166120
Iteration 44, loss = 0.10661523
Iteration 45, loss = 0.10596236
Iteration 46, loss = 0.10148324
Iteration 47, loss = 0.10163939
Iteration 48, loss = 0.09656197
Iteration 49, loss = 0.09822676
Iteration 50, loss = 0.09523073
Iteration 51, loss = 0.09336452
Iteration 52, loss = 0.08928765
Iteration 53, loss = 0.08688343
Iteration 54, loss = 0.08642205
Iteration 55, loss = 0.08484423
Iteration 56, loss = 0.08254237
Iteration 57, loss = 0.08883383
Iteration 58, loss = 0.08282743
Iteration 59, loss = 0.08051997
Iteration 60, loss = 0.07521177
Iteration 61, loss = 0.07445304
Iteration 62, loss = 0.07228028
Iteration 63, loss = 0.07013924
Iteration 64, loss = 0.07082958
Iteration 65, loss = 0.06816494
Iteration 66, loss = 0.06491072
Iteration 67, loss = 0.06201936
Iteration 68, loss = 0.06110550
Iteration 69, loss = 0.05899369
Iteration 70, loss = 0.05935607
Iteration 71, loss = 0.05817716
Iteration 72, loss = 0.05597513
Iteration 73, loss = 0.05500748
Iteration 74, loss = 0.05551356
Iteration 75, loss = 0.05473037
Iteration 76, loss = 0.05223911
Iteration 77, loss = 0.04958569
Iteration 78, loss = 0.04879135
Iteration 79, loss = 0.04747218
Iteration 80, loss = 0.04744994
Iteration 81, loss = 0.04750970
Iteration 82, loss = 0.04604543
Iteration 83, loss = 0.04351480
Iteration 84, loss = 0.04310576
Iteration 85, loss = 0.04313939
Iteration 86, loss = 0.04190972
Iteration 87, loss = 0.04057003
Iteration 88, loss = 0.03978425
Iteration 89, loss = 0.03916690
Iteration 90, loss = 0.03821319
Iteration 91, loss = 0.03756833
Iteration 92, loss = 0.03736370
Iteration 93, loss = 0.03728946
Iteration 94, loss = 0.03718758
Iteration 95, loss = 0.03651248
Iteration 96, loss = 0.03482557
Iteration 97, loss = 0.03409363
Iteration 98, loss = 0.03364718
Iteration 99, loss = 0.03299567
Iteration 100, loss = 0.03236484
Iteration 101, loss = 0.03139640
Iteration 102, loss = 0.03158590
Iteration 103, loss = 0.03041122
Iteration 104, loss = 0.03075307
Iteration 105, loss = 0.03020776
Iteration 106, loss = 0.02891843
Iteration 107, loss = 0.02831130
Iteration 108, loss = 0.02827464
Iteration 109, loss = 0.02760863
Iteration 110, loss = 0.02702624
Iteration 111, loss = 0.02636266
Iteration 112, loss = 0.02595486
Iteration 113, loss = 0.02622703
Iteration 114, loss = 0.02712928
Iteration 115, loss = 0.02626860
Iteration 116, loss = 0.02440926
Iteration 117, loss = 0.02503247
Iteration 118, loss = 0.02344382
Iteration 119, loss = 0.02354269
Iteration 120, loss = 0.02280086
Iteration 121, loss = 0.02264097
Iteration 122, loss = 0.02269021
Iteration 123, loss = 0.02238069
Iteration 124, loss = 0.02171909
Iteration 125, loss = 0.02110759
Iteration 126, loss = 0.02078743
Iteration 127, loss = 0.02039075
Iteration 128, loss = 0.02014936
Iteration 129, loss = 0.01992023
Iteration 130, loss = 0.01960945
Iteration 131, loss = 0.01941756
Iteration 132, loss = 0.01926830
Iteration 133, loss = 0.01910631
Iteration 134, loss = 0.01882910
Iteration 135, loss = 0.01872533
Iteration 136, loss = 0.01834304
Iteration 137, loss = 0.01801408
Iteration 138, loss = 0.01770865
Iteration 139, loss = 0.01754812
Iteration 140, loss = 0.01730727
Iteration 141, loss = 0.01705672
Iteration 142, loss = 0.01689783
Iteration 143, loss = 0.01676428
Iteration 144, loss = 0.01652452
Iteration 145, loss = 0.01679174
Iteration 146, loss = 0.01696666
Iteration 147, loss = 0.01658189
Iteration 148, loss = 0.01560918
Iteration 149, loss = 0.01600028
Iteration 150, loss = 0.01625555
Iteration 151, loss = 0.01590995
Iteration 152, loss = 0.01545206
Iteration 153, loss = 0.01530799
Iteration 154, loss = 0.01543965
Iteration 155, loss = 0.01446061
Iteration 156, loss = 0.01492925
Iteration 157, loss = 0.01468786
Iteration 158, loss = 0.01450316
Iteration 159, loss = 0.01405906
Iteration 160, loss = 0.01395841
Iteration 161, loss = 0.01377445
Iteration 162, loss = 0.01347359
Iteration 163, loss = 0.01333957
Iteration 164, loss = 0.01326280
Iteration 165, loss = 0.01322113
Iteration 166, loss = 0.01302982
Iteration 167, loss = 0.01307949
Iteration 168, loss = 0.01276372
Iteration 169, loss = 0.01247999
Iteration 170, loss = 0.01265321
Iteration 171, loss = 0.01240727
Iteration 172, loss = 0.01218105
Iteration 173, loss = 0.01212852
Iteration 174, loss = 0.01196165
Iteration 175, loss = 0.01182643
Iteration 176, loss = 0.01185598
Iteration 177, loss = 0.01156901
Iteration 178, loss = 0.01137481
Iteration 179, loss = 0.01132173
Iteration 180, loss = 0.01138722
Iteration 181, loss = 0.01127594
Iteration 182, loss = 0.01128059
Iteration 183, loss = 0.01107382
Iteration 184, loss = 0.01102964
Iteration 185, loss = 0.01069710
Iteration 186, loss = 0.01078623
Iteration 187, loss = 0.01085279
Iteration 188, loss = 0.01089519
Iteration 189, loss = 0.01084117
Iteration 190, loss = 0.01063002
Iteration 191, loss = 0.01045101
Iteration 192, loss = 0.01019233
Iteration 193, loss = 0.01000898
Iteration 194, loss = 0.00998867
Iteration 195, loss = 0.00999022
Iteration 196, loss = 0.00999979
Iteration 197, loss = 0.00990522
Iteration 198, loss = 0.00973595
Iteration 199, loss = 0.00966591
Iteration 200, loss = 0.00967573
Iteration 201, loss = 0.00966149
Iteration 202, loss = 0.00949205
Iteration 203, loss = 0.00931746
Iteration 204, loss = 0.00929344
Iteration 205, loss = 0.00920428
Iteration 206, loss = 0.00908836
Iteration 207, loss = 0.00900241
Iteration 208, loss = 0.00927942
Iteration 209, loss = 0.00942906
Iteration 210, loss = 0.00896441
Iteration 211, loss = 0.00866454
Iteration 212, loss = 0.00878835
Iteration 213, loss = 0.00904089
Iteration 214, loss = 0.00909835
Iteration 215, loss = 0.00873650
Iteration 216, loss = 0.00850467
Iteration 217, loss = 0.00835328
Iteration 218, loss = 0.00828297
Iteration 219, loss = 0.00824529
Iteration 220, loss = 0.00820687
Iteration 221, loss = 0.00817381
Iteration 222, loss = 0.00818173
Iteration 223, loss = 0.00802107
Iteration 224, loss = 0.00794402
Iteration 225, loss = 0.00789174
Iteration 226, loss = 0.00792495
Iteration 227, loss = 0.00799442
Iteration 228, loss = 0.00798996
Iteration 229, loss = 0.00787440
Iteration 230, loss = 0.00777335
Iteration 231, loss = 0.00764803
Iteration 232, loss = 0.00758891
Iteration 233, loss = 0.00762485
Iteration 234, loss = 0.00758122
Iteration 235, loss = 0.00755597
Iteration 236, loss = 0.00752011
Iteration 237, loss = 0.00742588
Iteration 238, loss = 0.00730701
Iteration 239, loss = 0.00723262
Iteration 240, loss = 0.00715162
Iteration 241, loss = 0.00708241
Iteration 242, loss = 0.00700298
Iteration 243, loss = 0.00701279
Iteration 244, loss = 0.00691206
Iteration 245, loss = 0.00686876
Iteration 246, loss = 0.00684257
Iteration 247, loss = 0.00684681
Iteration 248, loss = 0.00692907
Iteration 249, loss = 0.00684251
Iteration 250, loss = 0.00673299
Iteration 251, loss = 0.00664334
Iteration 252, loss = 0.00655816
Iteration 253, loss = 0.00649768
Iteration 254, loss = 0.00647537
Iteration 255, loss = 0.00661695
Iteration 256, loss = 0.00651383
Iteration 257, loss = 0.00648096
Iteration 258, loss = 0.00638884
Iteration 259, loss = 0.00632899
Iteration 260, loss = 0.00629439
Iteration 261, loss = 0.00631372
Iteration 262, loss = 0.00622382
Iteration 263, loss = 0.00622062
Iteration 264, loss = 0.00620549
Iteration 265, loss = 0.00624426
Iteration 266, loss = 0.00627694
Iteration 267, loss = 0.00618805
Iteration 268, loss = 0.00606364
Iteration 269, loss = 0.00596112
Iteration 270, loss = 0.00592054
Iteration 271, loss = 0.00590967
Iteration 272, loss = 0.00592716
Iteration 273, loss = 0.00590679
Iteration 274, loss = 0.00582507
Iteration 275, loss = 0.00575482
Iteration 276, loss = 0.00570130
Iteration 277, loss = 0.00565326
Iteration 278, loss = 0.00563377
Iteration 279, loss = 0.00561507
Iteration 280, loss = 0.00557061
Iteration 281, loss = 0.00554882
Iteration 282, loss = 0.00551528
Iteration 283, loss = 0.00549750
Iteration 284, loss = 0.00544426
Iteration 285, loss = 0.00542564
Iteration 286, loss = 0.00546420
Iteration 287, loss = 0.00543571
Iteration 288, loss = 0.00540118
Iteration 289, loss = 0.00535380
Iteration 290, loss = 0.00529877
Iteration 291, loss = 0.00525407
Iteration 292, loss = 0.00521719
Iteration 293, loss = 0.00519210
Iteration 294, loss = 0.00517313
Iteration 295, loss = 0.00515678
Iteration 296, loss = 0.00515941
Iteration 297, loss = 0.00512498
Iteration 298, loss = 0.00514990
Iteration 299, loss = 0.00507534
Iteration 300, loss = 0.00503675
Iteration 301, loss = 0.00498354
Iteration 302, loss = 0.00495630
Iteration 303, loss = 0.00493745
Iteration 304, loss = 0.00490289
Iteration 305, loss = 0.00488286
Iteration 306, loss = 0.00486852
Iteration 307, loss = 0.00485792
Iteration 308, loss = 0.00482372
Iteration 309, loss = 0.00481234
Iteration 310, loss = 0.00478963
Iteration 311, loss = 0.00477398
Iteration 312, loss = 0.00473679
Iteration 313, loss = 0.00470954
Iteration 314, loss = 0.00467804
Iteration 315, loss = 0.00466919
Iteration 316, loss = 0.00464892
Iteration 317, loss = 0.00463712
Iteration 318, loss = 0.00462887
Iteration 319, loss = 0.00463492
Iteration 320, loss = 0.00461610
Iteration 321, loss = 0.00458513
Iteration 322, loss = 0.00454036
Iteration 323, loss = 0.00448790
Iteration 324, loss = 0.00447360
Iteration 325, loss = 0.00445081
Iteration 326, loss = 0.00442581
Iteration 327, loss = 0.00440437
Iteration 328, loss = 0.00438210
Iteration 329, loss = 0.00436675
Iteration 330, loss = 0.00434222
Iteration 331, loss = 0.00432510
Iteration 332, loss = 0.00431081
Iteration 333, loss = 0.00429536
Iteration 334, loss = 0.00428569
Iteration 335, loss = 0.00424923
Iteration 336, loss = 0.00422650
Iteration 337, loss = 0.00420750
Iteration 338, loss = 0.00419893
Iteration 339, loss = 0.00418659
Iteration 340, loss = 0.00416155
Iteration 341, loss = 0.00414237
Iteration 342, loss = 0.00411751
Iteration 343, loss = 0.00411037
Iteration 344, loss = 0.00408021
Iteration 345, loss = 0.00410859
Iteration 346, loss = 0.00404905
Iteration 347, loss = 0.00404059
Iteration 348, loss = 0.00401397
Iteration 349, loss = 0.00399524
Iteration 350, loss = 0.00398369
Iteration 351, loss = 0.00397725
Iteration 352, loss = 0.00396215
Iteration 353, loss = 0.00395508
Iteration 354, loss = 0.00394771
Iteration 355, loss = 0.00393485
Iteration 356, loss = 0.00389132
Iteration 357, loss = 0.00387663
Iteration 358, loss = 0.00385690
Iteration 359, loss = 0.00385183
Iteration 360, loss = 0.00382468
Iteration 361, loss = 0.00381896
Iteration 362, loss = 0.00381410
Iteration 363, loss = 0.00383260
Iteration 364, loss = 0.00383256
Iteration 365, loss = 0.00377370
Iteration 366, loss = 0.00374310
Iteration 367, loss = 0.00372274
Iteration 368, loss = 0.00370958
Iteration 369, loss = 0.00372372
Iteration 370, loss = 0.00372806
Iteration 371, loss = 0.00372444
Iteration 372, loss = 0.00371220
Iteration 373, loss = 0.00369472
Iteration 374, loss = 0.00366817
Iteration 375, loss = 0.00364673
Iteration 376, loss = 0.00363248
Iteration 377, loss = 0.00362550
Iteration 378, loss = 0.00366799
Iteration 379, loss = 0.00363224
Iteration 380, loss = 0.00359870
Iteration 381, loss = 0.00355468
Iteration 382, loss = 0.00352958
Iteration 383, loss = 0.00350133
Iteration 384, loss = 0.00351934
Iteration 385, loss = 0.00354489
Iteration 386, loss = 0.00354603
Iteration 387, loss = 0.00351662
Iteration 388, loss = 0.00347451
Iteration 389, loss = 0.00344645
Iteration 390, loss = 0.00341259
Iteration 391, loss = 0.00338883
Iteration 392, loss = 0.00337012
Iteration 393, loss = 0.00335661
Iteration 394, loss = 0.00334588
Iteration 395, loss = 0.00333421
Iteration 396, loss = 0.00333777
Iteration 397, loss = 0.00330825
Iteration 398, loss = 0.00329065
Iteration 399, loss = 0.00329038
Iteration 400, loss = 0.00328595
Iteration 401, loss = 0.00330340
Iteration 402, loss = 0.00328336
Iteration 403, loss = 0.00325987
Iteration 404, loss = 0.00323873
Iteration 405, loss = 0.00322327
Iteration 406, loss = 0.00321111
Iteration 407, loss = 0.00320824
Iteration 408, loss = 0.00318583
Iteration 409, loss = 0.00317426
Iteration 410, loss = 0.00316069
Iteration 411, loss = 0.00315150
Iteration 412, loss = 0.00313522
Iteration 413, loss = 0.00312761
Iteration 414, loss = 0.00312471
Iteration 415, loss = 0.00312757
Iteration 416, loss = 0.00313414
Iteration 417, loss = 0.00312483
Iteration 418, loss = 0.00311156
Iteration 419, loss = 0.00308277
Iteration 420, loss = 0.00305694
Iteration 421, loss = 0.00303648
Iteration 422, loss = 0.00304306
Iteration 423, loss = 0.00303820
Iteration 424, loss = 0.00304498
Iteration 425, loss = 0.00302820
Iteration 426, loss = 0.00301391
Iteration 427, loss = 0.00299309
Iteration 428, loss = 0.00297751
Iteration 429, loss = 0.00296392
Iteration 430, loss = 0.00295389
Iteration 431, loss = 0.00294257
Iteration 432, loss = 0.00294210
Iteration 433, loss = 0.00292644
Iteration 434, loss = 0.00291901
Iteration 435, loss = 0.00291314
Iteration 436, loss = 0.00291077
Iteration 437, loss = 0.00290504
Iteration 438, loss = 0.00289139
Iteration 439, loss = 0.00287692
Iteration 440, loss = 0.00286392
Iteration 441, loss = 0.00285665
Iteration 442, loss = 0.00285649
Iteration 443, loss = 0.00284426
Iteration 444, loss = 0.00283683
Iteration 445, loss = 0.00282981
Iteration 446, loss = 0.00282268
Iteration 447, loss = 0.00281330
Iteration 448, loss = 0.00280309
Iteration 449, loss = 0.00279152
Iteration 450, loss = 0.00278272
Iteration 451, loss = 0.00277145
Iteration 452, loss = 0.00276040
Iteration 453, loss = 0.00275529
Iteration 454, loss = 0.00274516
Iteration 455, loss = 0.00274816
Iteration 456, loss = 0.00272751
Iteration 457, loss = 0.00271174
Iteration 458, loss = 0.00271041
Iteration 459, loss = 0.00269300
Iteration 460, loss = 0.00269396
Iteration 461, loss = 0.00268563
Iteration 462, loss = 0.00267413
Iteration 463, loss = 0.00266299
Iteration 464, loss = 0.00264598
Iteration 465, loss = 0.00264279
Iteration 466, loss = 0.00263534
Iteration 467, loss = 0.00265832
Iteration 468, loss = 0.00262886
Iteration 469, loss = 0.00262585
Iteration 470, loss = 0.00261446
Iteration 471, loss = 0.00260693
Iteration 472, loss = 0.00259951
Iteration 473, loss = 0.00259102
Iteration 474, loss = 0.00259217
Iteration 475, loss = 0.00257360
Iteration 476, loss = 0.00256216
Iteration 477, loss = 0.00255502
Iteration 478, loss = 0.00254327
Iteration 479, loss = 0.00254040
Iteration 480, loss = 0.00252884
Iteration 481, loss = 0.00252006
Iteration 482, loss = 0.00252390
Iteration 483, loss = 0.00250591
Iteration 484, loss = 0.00249740
Iteration 485, loss = 0.00248827
Iteration 486, loss = 0.00248315
Iteration 487, loss = 0.00247667
Iteration 488, loss = 0.00247246
Iteration 489, loss = 0.00246491
Iteration 490, loss = 0.00246017
Iteration 491, loss = 0.00244855
Iteration 492, loss = 0.00243997
Iteration 493, loss = 0.00243807
Iteration 494, loss = 0.00242859
Iteration 495, loss = 0.00242204
Iteration 496, loss = 0.00241502
Iteration 497, loss = 0.00240837
Iteration 498, loss = 0.00240530
Iteration 499, loss = 0.00239309
Iteration 500, loss = 0.00238786
Iteration 501, loss = 0.00237714
Iteration 502, loss = 0.00237382
Iteration 503, loss = 0.00237276
Iteration 504, loss = 0.00235905
Iteration 505, loss = 0.00235184
Iteration 506, loss = 0.00234998
Iteration 507, loss = 0.00234108
Iteration 508, loss = 0.00233252
Iteration 509, loss = 0.00232497
Iteration 510, loss = 0.00231830
Iteration 511, loss = 0.00231112
Iteration 512, loss = 0.00230581
Iteration 513, loss = 0.00230127
Iteration 514, loss = 0.00229305
Iteration 515, loss = 0.00229011
Iteration 516, loss = 0.00228515
Iteration 517, loss = 0.00228325
Iteration 518, loss = 0.00227439
Iteration 519, loss = 0.00226311
Iteration 520, loss = 0.00225725
Iteration 521, loss = 0.00224998
Iteration 522, loss = 0.00224476
Iteration 523, loss = 0.00224035
Iteration 524, loss = 0.00223553
Iteration 525, loss = 0.00223307
Iteration 526, loss = 0.00222828
Iteration 527, loss = 0.00222273
Iteration 528, loss = 0.00221899
Iteration 529, loss = 0.00221256
Iteration 530, loss = 0.00220725
Iteration 531, loss = 0.00220103
Iteration 532, loss = 0.00220171
Iteration 533, loss = 0.00218839
Iteration 534, loss = 0.00218087
Iteration 535, loss = 0.00217575
Iteration 536, loss = 0.00216644
Iteration 537, loss = 0.00216453
Iteration 538, loss = 0.00215869
Iteration 539, loss = 0.00215493
Iteration 540, loss = 0.00215198
Iteration 541, loss = 0.00215820
Iteration 542, loss = 0.00214492
Iteration 543, loss = 0.00213900
Iteration 544, loss = 0.00212787
Iteration 545, loss = 0.00211907
Iteration 546, loss = 0.00211389
Iteration 547, loss = 0.00210781
Iteration 548, loss = 0.00210580
Iteration 549, loss = 0.00210258
Iteration 550, loss = 0.00210101
Iteration 551, loss = 0.00210042
Iteration 552, loss = 0.00209970
Iteration 553, loss = 0.00209952
Iteration 554, loss = 0.00208552
Iteration 555, loss = 0.00208407
Iteration 556, loss = 0.00208221
Iteration 557, loss = 0.00208684
Iteration 558, loss = 0.00208532
Iteration 559, loss = 0.00208893
Iteration 560, loss = 0.00207972
Iteration 561, loss = 0.00206766
Iteration 562, loss = 0.00205441
Iteration 563, loss = 0.00203824
Iteration 564, loss = 0.00202143
Iteration 565, loss = 0.00200726
Iteration 566, loss = 0.00200474
Iteration 567, loss = 0.00199803
Iteration 568, loss = 0.00200167
Iteration 569, loss = 0.00203594
Iteration 570, loss = 0.00201801
Iteration 571, loss = 0.00200435
Iteration 572, loss = 0.00199095
Iteration 573, loss = 0.00197846
Iteration 574, loss = 0.00196948
Iteration 575, loss = 0.00196473
Iteration 576, loss = 0.00195850
Iteration 577, loss = 0.00195309
Iteration 578, loss = 0.00195137
Iteration 579, loss = 0.00194990
Iteration 580, loss = 0.00194419
Iteration 581, loss = 0.00193899
Iteration 582, loss = 0.00193390
Iteration 583, loss = 0.00192888
Iteration 584, loss = 0.00192411
Iteration 585, loss = 0.00191957
Iteration 586, loss = 0.00191613
Iteration 587, loss = 0.00191563
Iteration 588, loss = 0.00190469
Iteration 589, loss = 0.00189721
Iteration 590, loss = 0.00189469
Iteration 591, loss = 0.00188872
Iteration 592, loss = 0.00188612
Iteration 593, loss = 0.00187988
Iteration 594, loss = 0.00187605
Iteration 595, loss = 0.00187057
Iteration 596, loss = 0.00186867
Iteration 597, loss = 0.00186704
Iteration 598, loss = 0.00186115
Iteration 599, loss = 0.00185826
Iteration 600, loss = 0.00185147
Iteration 601, loss = 0.00184622
Iteration 602, loss = 0.00184337
Iteration 603, loss = 0.00184093
Iteration 604, loss = 0.00183772
Iteration 605, loss = 0.00183528
Iteration 606, loss = 0.00182826
Iteration 607, loss = 0.00182204
Iteration 608, loss = 0.00181693
Iteration 609, loss = 0.00181867
Iteration 610, loss = 0.00180818
Iteration 611, loss = 0.00180722
Iteration 612, loss = 0.00180357
Iteration 613, loss = 0.00179518
Iteration 614, loss = 0.00179376
Iteration 615, loss = 0.00179371
Iteration 616, loss = 0.00179801
Iteration 617, loss = 0.00179769
Iteration 618, loss = 0.00179277
Iteration 619, loss = 0.00178517
Iteration 620, loss = 0.00177773
Iteration 621, loss = 0.00177321
Iteration 622, loss = 0.00176743
Iteration 623, loss = 0.00176077
Iteration 624, loss = 0.00175240
Iteration 625, loss = 0.00174925
Iteration 626, loss = 0.00174512
Iteration 627, loss = 0.00174398
Iteration 628, loss = 0.00174460
Iteration 629, loss = 0.00173912
Iteration 630, loss = 0.00173723
Iteration 631, loss = 0.00173310
Iteration 632, loss = 0.00172963
Iteration 633, loss = 0.00172509
Iteration 634, loss = 0.00172182
Iteration 635, loss = 0.00171804
Iteration 636, loss = 0.00171162
Iteration 637, loss = 0.00170810
Iteration 638, loss = 0.00170101
Iteration 639, loss = 0.00169689
Iteration 640, loss = 0.00169210
Iteration 641, loss = 0.00168983
Iteration 642, loss = 0.00168630
Iteration 643, loss = 0.00168411
Iteration 644, loss = 0.00168102
Iteration 645, loss = 0.00167721
Iteration 646, loss = 0.00167282
Iteration 647, loss = 0.00166982
Iteration 648, loss = 0.00166371
Iteration 649, loss = 0.00166306
Iteration 650, loss = 0.00166763
Iteration 651, loss = 0.00167457
Iteration 652, loss = 0.00166914
Iteration 653, loss = 0.00166574
Iteration 654, loss = 0.00166130
Iteration 655, loss = 0.00165204
Iteration 656, loss = 0.00164453
Iteration 657, loss = 0.00163681
Iteration 658, loss = 0.00163399
Iteration 659, loss = 0.00163040
Iteration 660, loss = 0.00162690
Iteration 661, loss = 0.00162295
Iteration 662, loss = 0.00161961
Iteration 663, loss = 0.00161796
Iteration 664, loss = 0.00161228
Iteration 665, loss = 0.00160904
Iteration 666, loss = 0.00160560
Iteration 667, loss = 0.00160307
Iteration 668, loss = 0.00159928
Iteration 669, loss = 0.00159675
Iteration 670, loss = 0.00159819
Iteration 671, loss = 0.00159913
Iteration 672, loss = 0.00159808
Iteration 673, loss = 0.00159587
Iteration 674, loss = 0.00159317
Iteration 675, loss = 0.00159151
Iteration 676, loss = 0.00158943
Iteration 677, loss = 0.00158815
Iteration 678, loss = 0.00158397
Iteration 679, loss = 0.00157873
Iteration 680, loss = 0.00157247
Iteration 681, loss = 0.00156594
Iteration 682, loss = 0.00156039
Iteration 683, loss = 0.00155548
Iteration 684, loss = 0.00155175
Iteration 685, loss = 0.00154798
Iteration 686, loss = 0.00154897
Iteration 687, loss = 0.00154308
Iteration 688, loss = 0.00154019
Iteration 689, loss = 0.00153749
Iteration 690, loss = 0.00153594
Iteration 691, loss = 0.00153457
Iteration 692, loss = 0.00153091
Iteration 693, loss = 0.00153423
Iteration 694, loss = 0.00153581
Iteration 695, loss = 0.00153385
Iteration 696, loss = 0.00152605
Iteration 697, loss = 0.00151842
Iteration 698, loss = 0.00151269
Iteration 699, loss = 0.00150694
Iteration 700, loss = 0.00150510
PARAMETROS DE INICIALIZACAO DA REDE
NUMERO DE NEURONIOS 
Camada de Entrada: 34
Camada Escondida: 10
Camada de Saida: 1

--- PARAMETROS DE CONFIGURACAO DA REDE ---
Numero de Epocas: 700
Taxa de Aprendizado: adaptive
Taxa de Aprendizado Inicial: 0.6
METRICAS

RESULTADOS:

[0 0 1 0 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1 1 1 1 1 1 1
 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 1 1 1
 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 1 0 0 1 0 0 0 1 1 0 1 1 0]
ACURACIA: 0.9433962264150944

