Pesos Camada de Entrada: 
[[-0.08064656  0.05070578 -0.22728152]
 [-0.22439699  0.08311949 -0.13816047]
 [ 0.04182518 -0.22174106  0.0766336 ]
 [ 0.03380619 -0.1153321  -0.00932604]
 [-0.10974243  0.03006688 -0.11488232]
 [-0.10190796 -0.0421714   0.02508838]
 [-0.09376931 -0.09810333  0.19937012]
 [ 0.2235823  -0.03388052 -0.03211191]
 [-0.08555058  0.21490261 -0.12593235]
 [-0.04148018  0.07221879 -0.19848621]
 [ 0.06837547 -0.20551788 -0.19627742]
 [-0.02192534  0.0773959  -0.13706263]
 [-0.13474243  0.09721934  0.00840055]
 [ 0.1869506  -0.22513266 -0.02637847]
 [-0.2233765   0.09413431 -0.13673229]
 [-0.17939356  0.13096073 -0.06331187]
 [-0.19467588 -0.11726075  0.14435003]
 [ 0.11770976 -0.04352893 -0.2090001 ]
 [ 0.21283305 -0.02014639  0.17309973]
 [-0.13007659 -0.2100907  -0.15423121]
 [-0.11333723  0.0241832   0.17681734]
 [ 0.02003449  0.06964033 -0.00555145]
 [-0.22365305  0.12958808 -0.02104347]
 [-0.10585927  0.07491782  0.17130834]
 [ 0.19430446  0.06170811  0.04842778]
 [-0.0325451   0.11079291  0.13062296]
 [-0.23058068 -0.08612618  0.21632038]
 [ 0.06672245  0.1582272  -0.15841301]
 [-0.05554107  0.09642928  0.19629492]
 [-0.08682144 -0.03837423 -0.07145019]
 [ 0.08822471  0.04621037  0.10947179]
 [ 0.03827971  0.09815639 -0.10849268]
 [ 0.08656942 -0.20344511 -0.08241288]
 [-0.20940505  0.07146651  0.02065549]]
Bias Camada de Entrada: 
[0.18936668 0.04492103 0.09539768]
Pesos Camada Escondida: 
[[-0.64060703]
 [ 0.52004045]
 [ 0.06225357]]
Bias Camada Escondida: 
[-0.20261747]
Iteration 1, loss = 0.72299279
Iteration 2, loss = 0.65824867
Iteration 3, loss = 0.63958385
Iteration 4, loss = 0.63592698
Iteration 5, loss = 0.62878303
Iteration 6, loss = 0.61298579
Iteration 7, loss = 0.59460830
Iteration 8, loss = 0.57885481
Iteration 9, loss = 0.56022826
Iteration 10, loss = 0.53743214
Iteration 11, loss = 0.51663019
Iteration 12, loss = 0.49450317
Iteration 13, loss = 0.47015665
Iteration 14, loss = 0.44693584
Iteration 15, loss = 0.42685431
Iteration 16, loss = 0.40695988
Iteration 17, loss = 0.39088238
Iteration 18, loss = 0.37753696
Iteration 19, loss = 0.36658129
Iteration 20, loss = 0.35370989
Iteration 21, loss = 0.34446100
Iteration 22, loss = 0.33670036
Iteration 23, loss = 0.33134684
Iteration 24, loss = 0.32529842
Iteration 25, loss = 0.32058675
Iteration 26, loss = 0.31248196
Iteration 27, loss = 0.31113277
Iteration 28, loss = 0.30960736
Iteration 29, loss = 0.30168661
Iteration 30, loss = 0.29206537
Iteration 31, loss = 0.28797945
Iteration 32, loss = 0.28474792
Iteration 33, loss = 0.27652290
Iteration 34, loss = 0.26909162
Iteration 35, loss = 0.26561965
Iteration 36, loss = 0.26095616
Iteration 37, loss = 0.25818908
Iteration 38, loss = 0.25480021
Iteration 39, loss = 0.25126373
Iteration 40, loss = 0.24781930
Iteration 41, loss = 0.24422155
Iteration 42, loss = 0.24131760
Iteration 43, loss = 0.23853214
Iteration 44, loss = 0.23588513
Iteration 45, loss = 0.23285715
Iteration 46, loss = 0.22960112
Iteration 47, loss = 0.22646291
Iteration 48, loss = 0.22207534
Iteration 49, loss = 0.22048636
Iteration 50, loss = 0.21603091
Iteration 51, loss = 0.21328834
Iteration 52, loss = 0.21032365
Iteration 53, loss = 0.20682487
Iteration 54, loss = 0.20461081
Iteration 55, loss = 0.20536928
Iteration 56, loss = 0.19827508
Iteration 57, loss = 0.19767864
Iteration 58, loss = 0.19423318
Iteration 59, loss = 0.19206876
Iteration 60, loss = 0.18944822
Iteration 61, loss = 0.18756347
Iteration 62, loss = 0.18480650
Iteration 63, loss = 0.18365937
Iteration 64, loss = 0.18370786
Iteration 65, loss = 0.18124437
Iteration 66, loss = 0.17654107
Iteration 67, loss = 0.17789774
Iteration 68, loss = 0.17407129
Iteration 69, loss = 0.16929391
Iteration 70, loss = 0.16647086
Iteration 71, loss = 0.16286793
Iteration 72, loss = 0.16193615
Iteration 73, loss = 0.15870125
Iteration 74, loss = 0.15506067
Iteration 75, loss = 0.15217883
Iteration 76, loss = 0.14999173
Iteration 77, loss = 0.14706759
Iteration 78, loss = 0.14446367
Iteration 79, loss = 0.14256107
Iteration 80, loss = 0.14075829
Iteration 81, loss = 0.13852311
Iteration 82, loss = 0.13741081
Iteration 83, loss = 0.13677044
Iteration 84, loss = 0.13455441
Iteration 85, loss = 0.13298798
Iteration 86, loss = 0.13241250
Iteration 87, loss = 0.13106952
Iteration 88, loss = 0.12636229
Iteration 89, loss = 0.12260985
Iteration 90, loss = 0.12169237
Iteration 91, loss = 0.11979246
Iteration 92, loss = 0.11793920
Iteration 93, loss = 0.11700153
Iteration 94, loss = 0.11637891
Iteration 95, loss = 0.11463268
Iteration 96, loss = 0.11233680
Iteration 97, loss = 0.10966132
Iteration 98, loss = 0.10741814
Iteration 99, loss = 0.10577495
Iteration 100, loss = 0.10522043
Iteration 101, loss = 0.10273931
Iteration 102, loss = 0.10279976
Iteration 103, loss = 0.10037259
Iteration 104, loss = 0.09859706
Iteration 105, loss = 0.09873554
Iteration 106, loss = 0.09574244
Iteration 107, loss = 0.09455795
Iteration 108, loss = 0.09306430
Iteration 109, loss = 0.09242459
Iteration 110, loss = 0.09124287
Iteration 111, loss = 0.09046471
Iteration 112, loss = 0.08960631
Iteration 113, loss = 0.08895102
Iteration 114, loss = 0.08793568
Iteration 115, loss = 0.08541444
Iteration 116, loss = 0.08639419
Iteration 117, loss = 0.08664193
Iteration 118, loss = 0.08587505
Iteration 119, loss = 0.08320018
Iteration 120, loss = 0.08172761
Iteration 121, loss = 0.08016740
Iteration 122, loss = 0.07905001
Iteration 123, loss = 0.07829137
Iteration 124, loss = 0.07801349
Iteration 125, loss = 0.07823959
Iteration 126, loss = 0.07569644
Iteration 127, loss = 0.07690580
Iteration 128, loss = 0.07732092
Iteration 129, loss = 0.07501295
Iteration 130, loss = 0.07286002
Iteration 131, loss = 0.07479535
Iteration 132, loss = 0.07243558
Iteration 133, loss = 0.07130398
Iteration 134, loss = 0.07326303
Iteration 135, loss = 0.07261534
Iteration 136, loss = 0.07013235
Iteration 137, loss = 0.06957121
Iteration 138, loss = 0.06909192
Iteration 139, loss = 0.06787840
Iteration 140, loss = 0.06750162
Iteration 141, loss = 0.06740107
Iteration 142, loss = 0.06704724
Iteration 143, loss = 0.06516203
Iteration 144, loss = 0.06560696
Iteration 145, loss = 0.06629870
Iteration 146, loss = 0.06451768
Iteration 147, loss = 0.06356921
Iteration 148, loss = 0.06268242
Iteration 149, loss = 0.06208174
Iteration 150, loss = 0.06171278
Iteration 151, loss = 0.06156219
Iteration 152, loss = 0.06159034
Iteration 153, loss = 0.06063492
Iteration 154, loss = 0.05936934
Iteration 155, loss = 0.05883217
Iteration 156, loss = 0.05900676
Iteration 157, loss = 0.05811789
Iteration 158, loss = 0.05758853
Iteration 159, loss = 0.05697638
Iteration 160, loss = 0.05662210
Iteration 161, loss = 0.05703480
Iteration 162, loss = 0.05578755
Iteration 163, loss = 0.05737370
Iteration 164, loss = 0.05730589
Iteration 165, loss = 0.05640728
Iteration 166, loss = 0.05488095
Iteration 167, loss = 0.05334319
Iteration 168, loss = 0.05254400
Iteration 169, loss = 0.05263814
Iteration 170, loss = 0.05182155
Iteration 171, loss = 0.05116411
Iteration 172, loss = 0.05079997
Iteration 173, loss = 0.05066872
Iteration 174, loss = 0.05062732
Iteration 175, loss = 0.04955266
Iteration 176, loss = 0.04976614
Iteration 177, loss = 0.04979929
Iteration 178, loss = 0.04906402
Iteration 179, loss = 0.04845216
Iteration 180, loss = 0.04776040
Iteration 181, loss = 0.04663028
Iteration 182, loss = 0.04660320
Iteration 183, loss = 0.04625975
Iteration 184, loss = 0.04596375
Iteration 185, loss = 0.04561891
Iteration 186, loss = 0.04534486
Iteration 187, loss = 0.04529715
Iteration 188, loss = 0.04488604
Iteration 189, loss = 0.04385459
Iteration 190, loss = 0.04361059
Iteration 191, loss = 0.04302473
Iteration 192, loss = 0.04262548
Iteration 193, loss = 0.04219982
Iteration 194, loss = 0.04204069
Iteration 195, loss = 0.04169529
Iteration 196, loss = 0.04165557
Iteration 197, loss = 0.04097852
Iteration 198, loss = 0.04138746
Iteration 199, loss = 0.04107562
Iteration 200, loss = 0.04034264
Iteration 201, loss = 0.03984665
Iteration 202, loss = 0.03944196
Iteration 203, loss = 0.03920787
Iteration 204, loss = 0.03867920
Iteration 205, loss = 0.03881195
Iteration 206, loss = 0.03838226
Iteration 207, loss = 0.03781423
Iteration 208, loss = 0.03766106
Iteration 209, loss = 0.03702821
Iteration 210, loss = 0.03661786
Iteration 211, loss = 0.03637598
Iteration 212, loss = 0.03622805
Iteration 213, loss = 0.03590615
Iteration 214, loss = 0.03590554
Iteration 215, loss = 0.03490450
Iteration 216, loss = 0.03486073
Iteration 217, loss = 0.03466192
Iteration 218, loss = 0.03451152
Iteration 219, loss = 0.03350843
Iteration 220, loss = 0.03372804
Iteration 221, loss = 0.03331260
Iteration 222, loss = 0.03326591
Iteration 223, loss = 0.03290522
Iteration 224, loss = 0.03270914
Iteration 225, loss = 0.03249285
Iteration 226, loss = 0.03217678
Iteration 227, loss = 0.03188430
Iteration 228, loss = 0.03162538
Iteration 229, loss = 0.03132177
Iteration 230, loss = 0.03160018
Iteration 231, loss = 0.03189312
Iteration 232, loss = 0.03093539
Iteration 233, loss = 0.03044794
Iteration 234, loss = 0.03040740
Iteration 235, loss = 0.03078877
Iteration 236, loss = 0.03073042
Iteration 237, loss = 0.02998704
Iteration 238, loss = 0.03016842
Iteration 239, loss = 0.02991268
Iteration 240, loss = 0.02967948
Iteration 241, loss = 0.02945010
Iteration 242, loss = 0.03004476
Iteration 243, loss = 0.02973011
Iteration 244, loss = 0.02841662
Iteration 245, loss = 0.02802886
Iteration 246, loss = 0.02964471
Iteration 247, loss = 0.02718495
Iteration 248, loss = 0.02877159
Iteration 249, loss = 0.02763638
Iteration 250, loss = 0.02724505
Iteration 251, loss = 0.02712605
Iteration 252, loss = 0.02685945
Iteration 253, loss = 0.02649682
Iteration 254, loss = 0.02638293
Iteration 255, loss = 0.02614860
Iteration 256, loss = 0.02591764
Iteration 257, loss = 0.02580578
Iteration 258, loss = 0.02584199
Iteration 259, loss = 0.02594107
Iteration 260, loss = 0.02565979
Iteration 261, loss = 0.02504329
Iteration 262, loss = 0.02528577
Iteration 263, loss = 0.02501834
Iteration 264, loss = 0.02483122
Iteration 265, loss = 0.02466992
Iteration 266, loss = 0.02447338
Iteration 267, loss = 0.02427015
Iteration 268, loss = 0.02423022
Iteration 269, loss = 0.02424938
Iteration 270, loss = 0.02463627
Iteration 271, loss = 0.02462639
Iteration 272, loss = 0.02421219
Iteration 273, loss = 0.02378170
Iteration 274, loss = 0.02353790
Iteration 275, loss = 0.02342311
Iteration 276, loss = 0.02308362
Iteration 277, loss = 0.02302461
Iteration 278, loss = 0.02308618
Iteration 279, loss = 0.02324302
Iteration 280, loss = 0.02313073
Iteration 281, loss = 0.02230173
Iteration 282, loss = 0.02229375
Iteration 283, loss = 0.02261469
Iteration 284, loss = 0.02274539
Iteration 285, loss = 0.02245951
Iteration 286, loss = 0.02231923
Iteration 287, loss = 0.02234135
Iteration 288, loss = 0.02211541
Iteration 289, loss = 0.02171867
Iteration 290, loss = 0.02134139
Iteration 291, loss = 0.02121087
Iteration 292, loss = 0.02106130
Iteration 293, loss = 0.02105630
Iteration 294, loss = 0.02115724
Iteration 295, loss = 0.02153536
Iteration 296, loss = 0.02102060
Iteration 297, loss = 0.02067909
Iteration 298, loss = 0.02040903
Iteration 299, loss = 0.02031869
Iteration 300, loss = 0.02016995
Iteration 301, loss = 0.02015466
Iteration 302, loss = 0.02034295
Iteration 303, loss = 0.02001689
Iteration 304, loss = 0.01972784
Iteration 305, loss = 0.01959165
Iteration 306, loss = 0.01950143
Iteration 307, loss = 0.01938274
Iteration 308, loss = 0.01932040
Iteration 309, loss = 0.01926540
Iteration 310, loss = 0.01927342
Iteration 311, loss = 0.01901862
Iteration 312, loss = 0.01881940
Iteration 313, loss = 0.01873947
Iteration 314, loss = 0.01871317
Iteration 315, loss = 0.01873171
Iteration 316, loss = 0.01893776
Iteration 317, loss = 0.01874821
Iteration 318, loss = 0.01841207
Iteration 319, loss = 0.01820341
Iteration 320, loss = 0.01822344
Iteration 321, loss = 0.01842671
Iteration 322, loss = 0.01855226
Iteration 323, loss = 0.01823318
Iteration 324, loss = 0.01803487
Iteration 325, loss = 0.01748444
Iteration 326, loss = 0.01742029
Iteration 327, loss = 0.01735526
Iteration 328, loss = 0.01746033
Iteration 329, loss = 0.01755344
Iteration 330, loss = 0.01767160
Iteration 331, loss = 0.01726434
Iteration 332, loss = 0.01712121
Iteration 333, loss = 0.01683734
Iteration 334, loss = 0.01685009
Iteration 335, loss = 0.01662633
Iteration 336, loss = 0.01654693
Iteration 337, loss = 0.01649894
Iteration 338, loss = 0.01642881
Iteration 339, loss = 0.01644341
Iteration 340, loss = 0.01630890
Iteration 341, loss = 0.01689383
Iteration 342, loss = 0.01645061
Iteration 343, loss = 0.01602528
Iteration 344, loss = 0.01588794
Iteration 345, loss = 0.01594057
Iteration 346, loss = 0.01599748
Iteration 347, loss = 0.01591053
Iteration 348, loss = 0.01573296
Iteration 349, loss = 0.01555476
Iteration 350, loss = 0.01559561
Iteration 351, loss = 0.01538220
Iteration 352, loss = 0.01534108
Iteration 353, loss = 0.01529577
Iteration 354, loss = 0.01525902
Iteration 355, loss = 0.01540192
Iteration 356, loss = 0.01528862
Iteration 357, loss = 0.01512486
Iteration 358, loss = 0.01506172
Iteration 359, loss = 0.01497446
Iteration 360, loss = 0.01492671
Iteration 361, loss = 0.01487480
Iteration 362, loss = 0.01480556
Iteration 363, loss = 0.01485305
Iteration 364, loss = 0.01471783
Iteration 365, loss = 0.01458756
Iteration 366, loss = 0.01446177
Iteration 367, loss = 0.01448819
Iteration 368, loss = 0.01448945
Iteration 369, loss = 0.01441130
Iteration 370, loss = 0.01430366
Iteration 371, loss = 0.01426880
Iteration 372, loss = 0.01397904
Iteration 373, loss = 0.01409669
Iteration 374, loss = 0.01457097
Iteration 375, loss = 0.01438071
Iteration 376, loss = 0.01414391
Iteration 377, loss = 0.01408954
Iteration 378, loss = 0.01397617
Iteration 379, loss = 0.01384816
Iteration 380, loss = 0.01372623
Iteration 381, loss = 0.01367275
Iteration 382, loss = 0.01350764
Iteration 383, loss = 0.01340696
Iteration 384, loss = 0.01333505
Iteration 385, loss = 0.01318108
Iteration 386, loss = 0.01320847
Iteration 387, loss = 0.01334263
Iteration 388, loss = 0.01338140
Iteration 389, loss = 0.01337737
Iteration 390, loss = 0.01306045
Iteration 391, loss = 0.01293263
Iteration 392, loss = 0.01279509
Iteration 393, loss = 0.01274909
Iteration 394, loss = 0.01268347
Iteration 395, loss = 0.01262558
Iteration 396, loss = 0.01260825
Iteration 397, loss = 0.01257208
Iteration 398, loss = 0.01253358
Iteration 399, loss = 0.01243555
Iteration 400, loss = 0.01239417
Iteration 401, loss = 0.01234268
Iteration 402, loss = 0.01230615
Iteration 403, loss = 0.01227963
Iteration 404, loss = 0.01226135
Iteration 405, loss = 0.01237300
Iteration 406, loss = 0.01222416
Iteration 407, loss = 0.01215476
Iteration 408, loss = 0.01210611
Iteration 409, loss = 0.01207500
Iteration 410, loss = 0.01201541
Iteration 411, loss = 0.01203568
Iteration 412, loss = 0.01195486
Iteration 413, loss = 0.01189815
Iteration 414, loss = 0.01185539
Iteration 415, loss = 0.01183163
Iteration 416, loss = 0.01177280
Iteration 417, loss = 0.01169224
Iteration 418, loss = 0.01164554
Iteration 419, loss = 0.01162162
Iteration 420, loss = 0.01163007
Iteration 421, loss = 0.01155307
Iteration 422, loss = 0.01149808
Iteration 423, loss = 0.01148351
Iteration 424, loss = 0.01154477
Iteration 425, loss = 0.01147740
Iteration 426, loss = 0.01141165
Iteration 427, loss = 0.01138576
Iteration 428, loss = 0.01141647
Iteration 429, loss = 0.01141086
Iteration 430, loss = 0.01138982
Iteration 431, loss = 0.01130860
Iteration 432, loss = 0.01130332
Iteration 433, loss = 0.01103419
Iteration 434, loss = 0.01110094
Iteration 435, loss = 0.01126652
Iteration 436, loss = 0.01132485
Iteration 437, loss = 0.01128792
Iteration 438, loss = 0.01104306
Iteration 439, loss = 0.01075917
Iteration 440, loss = 0.01077017
Iteration 441, loss = 0.01071112
Iteration 442, loss = 0.01073005
Iteration 443, loss = 0.01065708
Iteration 444, loss = 0.01064505
Iteration 445, loss = 0.01061190
Iteration 446, loss = 0.01074154
Iteration 447, loss = 0.01074777
Iteration 448, loss = 0.01072154
Iteration 449, loss = 0.01063097
Iteration 450, loss = 0.01063100
Iteration 451, loss = 0.01056273
Iteration 452, loss = 0.01034287
Iteration 453, loss = 0.01016627
Iteration 454, loss = 0.01012917
Iteration 455, loss = 0.01017740
Iteration 456, loss = 0.01023837
Iteration 457, loss = 0.01028740
Iteration 458, loss = 0.01032788
Iteration 459, loss = 0.01029588
Iteration 460, loss = 0.01016570
Iteration 461, loss = 0.00996279
Iteration 462, loss = 0.00989035
Iteration 463, loss = 0.00985988
Iteration 464, loss = 0.00992049
Iteration 465, loss = 0.00981406
Iteration 466, loss = 0.00986979
Iteration 467, loss = 0.00971087
Iteration 468, loss = 0.00967387
Iteration 469, loss = 0.00964044
Iteration 470, loss = 0.00961991
Iteration 471, loss = 0.00959702
Iteration 472, loss = 0.00954844
Iteration 473, loss = 0.00951716
Iteration 474, loss = 0.00949959
Iteration 475, loss = 0.00947953
Iteration 476, loss = 0.00953865
Iteration 477, loss = 0.00942304
Iteration 478, loss = 0.00945887
Iteration 479, loss = 0.00941067
Iteration 480, loss = 0.00936569
Iteration 481, loss = 0.00930348
Iteration 482, loss = 0.00922593
Iteration 483, loss = 0.00916004
Iteration 484, loss = 0.00917916
Iteration 485, loss = 0.00917013
Iteration 486, loss = 0.00913525
Iteration 487, loss = 0.00906126
Iteration 488, loss = 0.00905329
Iteration 489, loss = 0.00903577
Iteration 490, loss = 0.00902867
Iteration 491, loss = 0.00897629
Iteration 492, loss = 0.00891014
Iteration 493, loss = 0.00888000
Iteration 494, loss = 0.00885998
Iteration 495, loss = 0.00889540
Iteration 496, loss = 0.00883522
Iteration 497, loss = 0.00875826
Iteration 498, loss = 0.00873055
Iteration 499, loss = 0.00874514
Iteration 500, loss = 0.00872207
Iteration 501, loss = 0.00871610
Iteration 502, loss = 0.00870725
Iteration 503, loss = 0.00868340
Iteration 504, loss = 0.00868420
Iteration 505, loss = 0.00862214
Iteration 506, loss = 0.00856703
Iteration 507, loss = 0.00854765
Iteration 508, loss = 0.00848226
Iteration 509, loss = 0.00845243
Iteration 510, loss = 0.00857025
Iteration 511, loss = 0.00850987
Iteration 512, loss = 0.00845118
Iteration 513, loss = 0.00837039
Iteration 514, loss = 0.00837887
Iteration 515, loss = 0.00841150
Iteration 516, loss = 0.00838479
Iteration 517, loss = 0.00836523
Iteration 518, loss = 0.00834974
Iteration 519, loss = 0.00831526
Iteration 520, loss = 0.00829274
Iteration 521, loss = 0.00826891
Iteration 522, loss = 0.00824211
Iteration 523, loss = 0.00821854
Iteration 524, loss = 0.00821921
Iteration 525, loss = 0.00823230
Iteration 526, loss = 0.00818304
Iteration 527, loss = 0.00812628
Iteration 528, loss = 0.00805074
Iteration 529, loss = 0.00801568
Iteration 530, loss = 0.00796845
Iteration 531, loss = 0.00796200
Iteration 532, loss = 0.00795416
Iteration 533, loss = 0.00792141
Iteration 534, loss = 0.00787021
Iteration 535, loss = 0.00788259
Iteration 536, loss = 0.00783549
Iteration 537, loss = 0.00780854
Iteration 538, loss = 0.00779682
Iteration 539, loss = 0.00774120
Iteration 540, loss = 0.00772402
Iteration 541, loss = 0.00774213
Iteration 542, loss = 0.00776531
Iteration 543, loss = 0.00779299
Iteration 544, loss = 0.00777000
Iteration 545, loss = 0.00772202
Iteration 546, loss = 0.00764445
Iteration 547, loss = 0.00758284
Iteration 548, loss = 0.00753628
Iteration 549, loss = 0.00750473
Iteration 550, loss = 0.00747150
Iteration 551, loss = 0.00746280
Iteration 552, loss = 0.00747660
Iteration 553, loss = 0.00749180
Iteration 554, loss = 0.00745960
Iteration 555, loss = 0.00740905
Iteration 556, loss = 0.00736379
Iteration 557, loss = 0.00737022
Iteration 558, loss = 0.00730810
Iteration 559, loss = 0.00732007
Iteration 560, loss = 0.00732197
Iteration 561, loss = 0.00731456
Iteration 562, loss = 0.00732252
Iteration 563, loss = 0.00723347
Iteration 564, loss = 0.00724363
Iteration 565, loss = 0.00716730
Iteration 566, loss = 0.00714970
Iteration 567, loss = 0.00714997
Iteration 568, loss = 0.00717292
Iteration 569, loss = 0.00713003
Iteration 570, loss = 0.00708228
Iteration 571, loss = 0.00705622
Iteration 572, loss = 0.00707887
Iteration 573, loss = 0.00706646
Iteration 574, loss = 0.00702419
Iteration 575, loss = 0.00701851
Iteration 576, loss = 0.00697473
Iteration 577, loss = 0.00697404
Iteration 578, loss = 0.00696391
Iteration 579, loss = 0.00695216
Iteration 580, loss = 0.00692358
Iteration 581, loss = 0.00689012
Iteration 582, loss = 0.00687112
Iteration 583, loss = 0.00684831
Iteration 584, loss = 0.00683752
Iteration 585, loss = 0.00680911
Iteration 586, loss = 0.00678514
Iteration 587, loss = 0.00678669
Iteration 588, loss = 0.00674104
Iteration 589, loss = 0.00670675
Iteration 590, loss = 0.00670001
Iteration 591, loss = 0.00671327
Iteration 592, loss = 0.00672971
Iteration 593, loss = 0.00666677
Iteration 594, loss = 0.00662340
Iteration 595, loss = 0.00663164
Iteration 596, loss = 0.00666662
Iteration 597, loss = 0.00667238
Iteration 598, loss = 0.00664545
Iteration 599, loss = 0.00659391
Iteration 600, loss = 0.00653421
Iteration 601, loss = 0.00650921
Iteration 602, loss = 0.00650608
Iteration 603, loss = 0.00653290
Iteration 604, loss = 0.00655206
Iteration 605, loss = 0.00657402
Iteration 606, loss = 0.00661065
Iteration 607, loss = 0.00648304
Iteration 608, loss = 0.00642567
Iteration 609, loss = 0.00637486
Iteration 610, loss = 0.00639241
Iteration 611, loss = 0.00637265
Iteration 612, loss = 0.00635130
Iteration 613, loss = 0.00631989
Iteration 614, loss = 0.00630102
Iteration 615, loss = 0.00630506
Iteration 616, loss = 0.00630401
Iteration 617, loss = 0.00631596
Iteration 618, loss = 0.00627971
Iteration 619, loss = 0.00624768
Iteration 620, loss = 0.00623555
Iteration 621, loss = 0.00622157
Iteration 622, loss = 0.00621457
Iteration 623, loss = 0.00617190
Iteration 624, loss = 0.00618571
Iteration 625, loss = 0.00619564
Iteration 626, loss = 0.00618902
Iteration 627, loss = 0.00617324
Iteration 628, loss = 0.00614083
Iteration 629, loss = 0.00610631
Iteration 630, loss = 0.00609373
Iteration 631, loss = 0.00607020
Iteration 632, loss = 0.00607070
Iteration 633, loss = 0.00608985
Iteration 634, loss = 0.00605961
Iteration 635, loss = 0.00604519
Iteration 636, loss = 0.00604488
Iteration 637, loss = 0.00603770
Iteration 638, loss = 0.00602017
Iteration 639, loss = 0.00600706
Iteration 640, loss = 0.00600351
Iteration 641, loss = 0.00598339
Iteration 642, loss = 0.00596066
Iteration 643, loss = 0.00592532
Iteration 644, loss = 0.00591436
Iteration 645, loss = 0.00590554
Iteration 646, loss = 0.00590707
Iteration 647, loss = 0.00587753
Iteration 648, loss = 0.00585111
Iteration 649, loss = 0.00592459
Iteration 650, loss = 0.00582903
Iteration 651, loss = 0.00579948
Iteration 652, loss = 0.00578936
Iteration 653, loss = 0.00575794
Iteration 654, loss = 0.00575259
Iteration 655, loss = 0.00574289
Iteration 656, loss = 0.00575475
Iteration 657, loss = 0.00573429
Iteration 658, loss = 0.00572056
Iteration 659, loss = 0.00571904
Iteration 660, loss = 0.00570092
Iteration 661, loss = 0.00567762
Iteration 662, loss = 0.00568196
Iteration 663, loss = 0.00565845
Iteration 664, loss = 0.00563918
Iteration 665, loss = 0.00562353
Iteration 666, loss = 0.00560847
Iteration 667, loss = 0.00559962
Iteration 668, loss = 0.00557977
Iteration 669, loss = 0.00556955
Iteration 670, loss = 0.00555421
Iteration 671, loss = 0.00554179
Iteration 672, loss = 0.00553426
Iteration 673, loss = 0.00553015
Iteration 674, loss = 0.00552561
Iteration 675, loss = 0.00554229
Iteration 676, loss = 0.00557220
Iteration 677, loss = 0.00561108
Iteration 678, loss = 0.00561865
Iteration 679, loss = 0.00557429
Iteration 680, loss = 0.00547620
Iteration 681, loss = 0.00540033
Iteration 682, loss = 0.00542457
Iteration 683, loss = 0.00550022
Iteration 684, loss = 0.00558013
Iteration 685, loss = 0.00558244
Iteration 686, loss = 0.00552524
Iteration 687, loss = 0.00543179
Iteration 688, loss = 0.00535256
Iteration 689, loss = 0.00535758
Iteration 690, loss = 0.00531989
Iteration 691, loss = 0.00533227
Iteration 692, loss = 0.00533678
Iteration 693, loss = 0.00534630
Iteration 694, loss = 0.00538141
Iteration 695, loss = 0.00533241
Iteration 696, loss = 0.00528887
Iteration 697, loss = 0.00523530
Iteration 698, loss = 0.00523024
Iteration 699, loss = 0.00525466
Iteration 700, loss = 0.00524847
PARAMETROS DE INICIALIZACAO DA REDE
NUMERO DE NEURONIOS 
Camada de Entrada: 34
Camada Escondida: 3
Camada de Saida: 1

--- PARAMETROS DE CONFIGURACAO DA REDE ---
Numero de Epocas: 700
Taxa de Aprendizado: adaptive
Taxa de Aprendizado Inicial: 0.3
METRICAS

RESULTADOS:

[0 0 1 0 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 1 1
 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 1 1 1
 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 1 0 1 1 0 0 1 1 0 1 1 1 0 1 1 0]
ACURACIA: 0.9245283018867925

