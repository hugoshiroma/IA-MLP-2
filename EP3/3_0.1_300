Pesos Camada de Entrada: 
[[ 0.10416033 -0.09693455 -0.13808274]
 [ 0.1975364   0.12365709  0.16794801]
 [ 0.13023452  0.05474045  0.02052831]
 [-0.13552823 -0.10551651 -0.06973223]
 [-0.0350053  -0.02148928  0.05936226]
 [-0.03791324  0.02159356  0.11138419]
 [ 0.16112476  0.22734793 -0.00290029]
 [ 0.12369891 -0.00822805 -0.18911299]
 [ 0.21496158 -0.12094019  0.14858549]
 [-0.09150401 -0.18844599  0.01688379]
 [ 0.18216357 -0.16153115  0.03063265]
 [-0.01783569  0.02052865  0.08771583]
 [-0.18435973  0.02563792 -0.10981019]
 [ 0.20358226 -0.07016273 -0.16190386]
 [-0.10895155 -0.15506382 -0.19227553]
 [ 0.09970054  0.01113064 -0.19203371]
 [ 0.11067244  0.2291835   0.02006764]
 [ 0.16208073  0.18328876  0.12201483]
 [ 0.01041267  0.20474778 -0.01877941]
 [-0.022446    0.01763321  0.15538898]
 [ 0.20938121  0.05242353 -0.08617288]
 [-0.10176784 -0.12831412 -0.20031076]
 [ 0.13091156  0.01771757 -0.14678829]
 [ 0.1131211  -0.05502595 -0.00144914]
 [ 0.02174549 -0.11445606  0.19580734]
 [-0.01911539 -0.126619    0.11271687]
 [ 0.00133564  0.10777979 -0.06799044]
 [-0.12560835 -0.21439033 -0.05748764]
 [ 0.03654426  0.14343084 -0.13626353]
 [ 0.12056544 -0.00109514  0.04223982]
 [ 0.12300099  0.17430345 -0.03856153]
 [ 0.20680567 -0.06399503 -0.09584329]
 [-0.15331502  0.08691898 -0.1534543 ]
 [ 0.06849253 -0.16497201 -0.18756838]]
Bias Camada de Entrada: 
[-0.22009333  0.23175104 -0.00759404]
Pesos Camada Escondida: 
[[-0.32783158]
 [ 0.45770092]
 [ 0.00536236]]
Bias Camada Escondida: 
[0.12971394]
Iteration 1, loss = 0.66955584
Iteration 2, loss = 0.66043065
Iteration 3, loss = 0.65190700
Iteration 4, loss = 0.64777058
Iteration 5, loss = 0.64606186
Iteration 6, loss = 0.64411451
Iteration 7, loss = 0.64247401
Iteration 8, loss = 0.64040214
Iteration 9, loss = 0.63797073
Iteration 10, loss = 0.63426416
Iteration 11, loss = 0.63048868
Iteration 12, loss = 0.62723852
Iteration 13, loss = 0.62378123
Iteration 14, loss = 0.61996571
Iteration 15, loss = 0.61586523
Iteration 16, loss = 0.61176510
Iteration 17, loss = 0.60679133
Iteration 18, loss = 0.60177091
Iteration 19, loss = 0.59633951
Iteration 20, loss = 0.59060173
Iteration 21, loss = 0.58453052
Iteration 22, loss = 0.57782251
Iteration 23, loss = 0.57060514
Iteration 24, loss = 0.56303582
Iteration 25, loss = 0.55486044
Iteration 26, loss = 0.54593963
Iteration 27, loss = 0.53696468
Iteration 28, loss = 0.52779313
Iteration 29, loss = 0.51844119
Iteration 30, loss = 0.50809040
Iteration 31, loss = 0.49848747
Iteration 32, loss = 0.48914226
Iteration 33, loss = 0.47915888
Iteration 34, loss = 0.46983329
Iteration 35, loss = 0.46092024
Iteration 36, loss = 0.45179852
Iteration 37, loss = 0.44323302
Iteration 38, loss = 0.43461684
Iteration 39, loss = 0.42676520
Iteration 40, loss = 0.41870054
Iteration 41, loss = 0.41170641
Iteration 42, loss = 0.40451254
Iteration 43, loss = 0.39759574
Iteration 44, loss = 0.39111702
Iteration 45, loss = 0.38483952
Iteration 46, loss = 0.37913109
Iteration 47, loss = 0.37350029
Iteration 48, loss = 0.36840718
Iteration 49, loss = 0.36413140
Iteration 50, loss = 0.36045501
Iteration 51, loss = 0.35590829
Iteration 52, loss = 0.35130603
Iteration 53, loss = 0.34638512
Iteration 54, loss = 0.34285585
Iteration 55, loss = 0.33914733
Iteration 56, loss = 0.33568440
Iteration 57, loss = 0.33237502
Iteration 58, loss = 0.32956293
Iteration 59, loss = 0.32643265
Iteration 60, loss = 0.32327786
Iteration 61, loss = 0.32007883
Iteration 62, loss = 0.31751237
Iteration 63, loss = 0.31457570
Iteration 64, loss = 0.31208133
Iteration 65, loss = 0.30964081
Iteration 66, loss = 0.30710732
Iteration 67, loss = 0.30478117
Iteration 68, loss = 0.30270879
Iteration 69, loss = 0.30086290
Iteration 70, loss = 0.29881490
Iteration 71, loss = 0.29690292
Iteration 72, loss = 0.29455657
Iteration 73, loss = 0.29260540
Iteration 74, loss = 0.29092123
Iteration 75, loss = 0.28970760
Iteration 76, loss = 0.28761146
Iteration 77, loss = 0.28540524
Iteration 78, loss = 0.28378598
Iteration 79, loss = 0.28190493
Iteration 80, loss = 0.28086420
Iteration 81, loss = 0.27847665
Iteration 82, loss = 0.27639637
Iteration 83, loss = 0.27423042
Iteration 84, loss = 0.27243781
Iteration 85, loss = 0.27051089
Iteration 86, loss = 0.26909269
Iteration 87, loss = 0.26689665
Iteration 88, loss = 0.26543673
Iteration 89, loss = 0.26422418
Iteration 90, loss = 0.26163579
Iteration 91, loss = 0.26040812
Iteration 92, loss = 0.25849210
Iteration 93, loss = 0.25717219
Iteration 94, loss = 0.25550484
Iteration 95, loss = 0.25399898
Iteration 96, loss = 0.25238228
Iteration 97, loss = 0.25117044
Iteration 98, loss = 0.24939691
Iteration 99, loss = 0.24749388
Iteration 100, loss = 0.24598439
Iteration 101, loss = 0.24426753
Iteration 102, loss = 0.24275063
Iteration 103, loss = 0.24113283
Iteration 104, loss = 0.24014078
Iteration 105, loss = 0.23843050
Iteration 106, loss = 0.23684268
Iteration 107, loss = 0.23540646
Iteration 108, loss = 0.23411893
Iteration 109, loss = 0.23296962
Iteration 110, loss = 0.23140138
Iteration 111, loss = 0.23005386
Iteration 112, loss = 0.22893386
Iteration 113, loss = 0.22770473
Iteration 114, loss = 0.22621935
Iteration 115, loss = 0.22477079
Iteration 116, loss = 0.22331254
Iteration 117, loss = 0.22224297
Iteration 118, loss = 0.22107878
Iteration 119, loss = 0.21980675
Iteration 120, loss = 0.21853182
Iteration 121, loss = 0.21663996
Iteration 122, loss = 0.21493365
Iteration 123, loss = 0.21409044
Iteration 124, loss = 0.21437070
Iteration 125, loss = 0.21368413
Iteration 126, loss = 0.21185022
Iteration 127, loss = 0.20964488
Iteration 128, loss = 0.20734762
Iteration 129, loss = 0.20547320
Iteration 130, loss = 0.20404143
Iteration 131, loss = 0.20268117
Iteration 132, loss = 0.20171044
Iteration 133, loss = 0.20087505
Iteration 134, loss = 0.19912592
Iteration 135, loss = 0.19781340
Iteration 136, loss = 0.19653731
Iteration 137, loss = 0.19496252
Iteration 138, loss = 0.19347598
Iteration 139, loss = 0.19285431
Iteration 140, loss = 0.19268510
Iteration 141, loss = 0.19292773
Iteration 142, loss = 0.19347714
Iteration 143, loss = 0.19221738
Iteration 144, loss = 0.18782147
Iteration 145, loss = 0.18531832
Iteration 146, loss = 0.18404055
Iteration 147, loss = 0.18349755
Iteration 148, loss = 0.18316520
Iteration 149, loss = 0.18212439
Iteration 150, loss = 0.18029129
Iteration 151, loss = 0.17860181
Iteration 152, loss = 0.17727622
Iteration 153, loss = 0.17600408
Iteration 154, loss = 0.17523575
Iteration 155, loss = 0.17353126
Iteration 156, loss = 0.17218778
Iteration 157, loss = 0.17140949
Iteration 158, loss = 0.17011886
Iteration 159, loss = 0.16899428
Iteration 160, loss = 0.16803790
Iteration 161, loss = 0.16724970
Iteration 162, loss = 0.16640880
Iteration 163, loss = 0.16561107
Iteration 164, loss = 0.16530193
Iteration 165, loss = 0.16310020
Iteration 166, loss = 0.16212857
Iteration 167, loss = 0.16208015
Iteration 168, loss = 0.16206589
Iteration 169, loss = 0.16064211
Iteration 170, loss = 0.15891606
Iteration 171, loss = 0.15729109
Iteration 172, loss = 0.15606382
Iteration 173, loss = 0.15502353
Iteration 174, loss = 0.15390350
Iteration 175, loss = 0.15289196
Iteration 176, loss = 0.15194997
Iteration 177, loss = 0.15097289
Iteration 178, loss = 0.15011922
Iteration 179, loss = 0.14920708
Iteration 180, loss = 0.14820693
Iteration 181, loss = 0.14779310
Iteration 182, loss = 0.14705875
Iteration 183, loss = 0.14642078
Iteration 184, loss = 0.14581062
Iteration 185, loss = 0.14486799
Iteration 186, loss = 0.14397776
Iteration 187, loss = 0.14300856
Iteration 188, loss = 0.14191893
Iteration 189, loss = 0.14090069
Iteration 190, loss = 0.14039969
Iteration 191, loss = 0.14027370
Iteration 192, loss = 0.14059952
Iteration 193, loss = 0.14043398
Iteration 194, loss = 0.13885393
Iteration 195, loss = 0.13757900
Iteration 196, loss = 0.13668650
Iteration 197, loss = 0.13591641
Iteration 198, loss = 0.13559535
Iteration 199, loss = 0.13579816
Iteration 200, loss = 0.13512677
Iteration 201, loss = 0.13402827
Iteration 202, loss = 0.13299048
Iteration 203, loss = 0.13211285
Iteration 204, loss = 0.13158175
Iteration 205, loss = 0.13131902
Iteration 206, loss = 0.13104275
Iteration 207, loss = 0.13073054
Iteration 208, loss = 0.13006513
Iteration 209, loss = 0.12918581
Iteration 210, loss = 0.12825092
Iteration 211, loss = 0.12816690
Iteration 212, loss = 0.12831067
Iteration 213, loss = 0.12825007
Iteration 214, loss = 0.12740930
Iteration 215, loss = 0.12614123
Iteration 216, loss = 0.12506425
Iteration 217, loss = 0.12432620
Iteration 218, loss = 0.12397220
Iteration 219, loss = 0.12363826
Iteration 220, loss = 0.12298903
Iteration 221, loss = 0.12265768
Iteration 222, loss = 0.12213519
Iteration 223, loss = 0.12143253
Iteration 224, loss = 0.12079139
Iteration 225, loss = 0.12040634
Iteration 226, loss = 0.11992846
Iteration 227, loss = 0.11950679
Iteration 228, loss = 0.11897813
Iteration 229, loss = 0.11860932
Iteration 230, loss = 0.11856712
Iteration 231, loss = 0.11848588
Iteration 232, loss = 0.11818726
Iteration 233, loss = 0.11782124
Iteration 234, loss = 0.11728995
Iteration 235, loss = 0.11666733
Iteration 236, loss = 0.11588916
Iteration 237, loss = 0.11524261
Iteration 238, loss = 0.11462850
Iteration 239, loss = 0.11437751
Iteration 240, loss = 0.11393497
Iteration 241, loss = 0.11332643
Iteration 242, loss = 0.11296201
Iteration 243, loss = 0.11238000
Iteration 244, loss = 0.11192936
Iteration 245, loss = 0.11153590
Iteration 246, loss = 0.11151926
Iteration 247, loss = 0.11093366
Iteration 248, loss = 0.11055302
Iteration 249, loss = 0.11002977
Iteration 250, loss = 0.10965715
Iteration 251, loss = 0.10918884
Iteration 252, loss = 0.10874418
Iteration 253, loss = 0.10835468
Iteration 254, loss = 0.10795594
Iteration 255, loss = 0.10788037
Iteration 256, loss = 0.10734708
Iteration 257, loss = 0.10668975
Iteration 258, loss = 0.10628911
Iteration 259, loss = 0.10590500
Iteration 260, loss = 0.10542841
Iteration 261, loss = 0.10500531
Iteration 262, loss = 0.10464467
Iteration 263, loss = 0.10424236
Iteration 264, loss = 0.10387223
Iteration 265, loss = 0.10350769
Iteration 266, loss = 0.10327444
Iteration 267, loss = 0.10277510
Iteration 268, loss = 0.10242774
Iteration 269, loss = 0.10202885
Iteration 270, loss = 0.10174474
Iteration 271, loss = 0.10163474
Iteration 272, loss = 0.10102329
Iteration 273, loss = 0.10046701
Iteration 274, loss = 0.10016480
Iteration 275, loss = 0.09968913
Iteration 276, loss = 0.09928026
Iteration 277, loss = 0.09892796
Iteration 278, loss = 0.09849770
Iteration 279, loss = 0.09815682
Iteration 280, loss = 0.09772307
Iteration 281, loss = 0.09734062
Iteration 282, loss = 0.09702505
Iteration 283, loss = 0.09668292
Iteration 284, loss = 0.09623175
Iteration 285, loss = 0.09622299
Iteration 286, loss = 0.09570042
Iteration 287, loss = 0.09523320
Iteration 288, loss = 0.09484178
Iteration 289, loss = 0.09442156
Iteration 290, loss = 0.09421957
Iteration 291, loss = 0.09439789
Iteration 292, loss = 0.09362796
Iteration 293, loss = 0.09300147
Iteration 294, loss = 0.09249162
Iteration 295, loss = 0.09196350
Iteration 296, loss = 0.09159497
Iteration 297, loss = 0.09130157
Iteration 298, loss = 0.09113767
Iteration 299, loss = 0.09118899
Iteration 300, loss = 0.09133012
PARAMETROS DE INICIALIZACAO DA REDE
NUMERO DE NEURONIOS 
Camada de Entrada: 34
Camada Escondida: 3
Camada de Saida: 1

--- PARAMETROS DE CONFIGURACAO DA REDE ---
Numero de Epocas: 300
Taxa de Aprendizado: adaptive
Taxa de Aprendizado Inicial: 0.1
METRICAS

RESULTADOS:

[0 0 1 0 1 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 1 1
 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 1 1 1
 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 1 1]
ACURACIA: 0.9245283018867925

