Pesos Camada de Entrada: 
[[-0.1306656  -0.18407407 -0.15585089  0.19175867  0.00073697 -0.04206581
   0.13632299 -0.15913192 -0.05064959  0.11355728 -0.09121227 -0.15689441
  -0.1509803   0.04622292 -0.01385822]
 [-0.08175852 -0.18666971  0.12921621 -0.13216949 -0.04171735  0.17736561
  -0.19237157 -0.14794368  0.14600436 -0.2007526  -0.12071256 -0.06937799
   0.01027984 -0.14983596 -0.09094867]
 [-0.0787413   0.04823332  0.04923538  0.06746422 -0.08362164 -0.10614906
  -0.10501297 -0.00891197 -0.09219851 -0.18079452  0.01676475  0.14486645
  -0.11076463  0.07186476  0.1376376 ]
 [-0.0808872   0.08278105 -0.09712218  0.09406185  0.05445319 -0.13398246
   0.05586257 -0.11145647 -0.04834166 -0.00505416 -0.13999462  0.16837083
   0.1212117  -0.04118536  0.03926893]
 [-0.15398059 -0.19634254  0.15472304 -0.04772238 -0.045087   -0.01699881
   0.05153512  0.04285408 -0.08666762  0.1780607  -0.03962009 -0.01586348
  -0.13623751  0.19798368 -0.17180297]
 [ 0.15620893 -0.02435712 -0.19498258 -0.17875977  0.00376966  0.07018326
  -0.00082653 -0.15677112  0.18373197  0.07455237  0.05374639  0.08056556
  -0.0051923  -0.0705638   0.014376  ]
 [-0.20017792 -0.09758229 -0.19651638  0.11948135  0.14644372  0.17269562
  -0.19788849 -0.01088038 -0.03115769  0.02953599 -0.11106551 -0.07232066
  -0.15310182 -0.1146369   0.0951832 ]
 [-0.11284162 -0.10718098  0.19368629 -0.01548113 -0.00199557  0.09118159
   0.03861729 -0.02534766 -0.04696527 -0.10721803  0.05127725  0.03372412
   0.17556054 -0.12982356 -0.0078688 ]
 [-0.05751307 -0.0375815   0.01056637  0.18351746 -0.11390431 -0.12550591
   0.10142403 -0.03330725  0.18211942  0.00078536  0.14248559 -0.17614099
   0.0485142   0.09874375  0.10085472]
 [-0.10411667 -0.05986245 -0.09228747 -0.18739516  0.04015344 -0.07701917
   0.17402186  0.18484795 -0.06032251  0.08580865  0.19701934 -0.12515059
  -0.02208054  0.0489553  -0.07413828]
 [-0.09732514 -0.14016945  0.19459519 -0.11759005 -0.08410203  0.09934874
  -0.03584402  0.047604   -0.03706392 -0.03474288 -0.13762657  0.15200854
  -0.1845917  -0.0351219  -0.15222421]
 [ 0.1713852  -0.10164518 -0.1318255  -0.05119137  0.05367909  0.13336592
  -0.03216994  0.01664116 -0.14300915  0.13119377 -0.01324084  0.03172243
  -0.06801403 -0.20118102  0.04173158]
 [ 0.0320077   0.07670624 -0.00719264  0.02187777 -0.13855857  0.14926556
  -0.14388085  0.01127882 -0.11980681 -0.08618725 -0.04127527  0.13658015
  -0.03570789  0.10717187 -0.07303025]
 [ 0.10881245 -0.02452118 -0.13798367  0.0769773   0.05035086 -0.1193821
   0.1715264  -0.12968327 -0.17566401  0.06052387  0.03088518 -0.11319445
   0.17043046  0.18584307 -0.0972042 ]
 [-0.19093115 -0.05262009 -0.06790053  0.15203519  0.09685838 -0.1389172
  -0.04060754  0.01102757  0.05732882 -0.14205252  0.1198042  -0.02871087
   0.04310112  0.06017996  0.04751851]
 [ 0.19131003 -0.17946315 -0.08796828  0.02815358 -0.17847508 -0.13805786
   0.01306996 -0.16188497 -0.06986236 -0.11319413 -0.00244583  0.06415026
  -0.14402129  0.05136608  0.00149555]
 [ 0.07337252  0.00877422  0.11441885 -0.02826255 -0.19347876  0.18143744
  -0.20043115 -0.16343138 -0.03940202 -0.06392902 -0.15967862 -0.10968941
  -0.00566616 -0.07949042  0.15425   ]
 [-0.00578608  0.19528595  0.08309921  0.18573789  0.11447657 -0.09201862
  -0.05942676 -0.08213808  0.07990341  0.13735044 -0.07541739  0.1428431
   0.1878465  -0.00317463  0.02311498]
 [-0.00787244  0.02161646  0.07080811  0.00468389 -0.04279733 -0.14904614
  -0.03953943 -0.01363221 -0.12133377 -0.18919967  0.1636472   0.19812011
  -0.20131849  0.11820002  0.01859429]
 [-0.03452735 -0.17889152  0.16524496  0.11539489  0.09711691 -0.0362725
  -0.0568543  -0.04209807 -0.04584313  0.0543727  -0.10500676 -0.18889272
  -0.08793083 -0.12755243  0.05292048]
 [ 0.11736329 -0.18890868  0.18282205  0.16673112 -0.14980718  0.10909811
  -0.06794762  0.1871814  -0.06996747 -0.0867665   0.03408139  0.15449811
  -0.10255804  0.0287204  -0.06652741]
 [ 0.14546669 -0.03860833  0.1165023  -0.04598709 -0.12402251  0.0580767
   0.15090723  0.12325126  0.03667495 -0.17465077 -0.09311319  0.18114406
  -0.19473574  0.1160933  -0.0419104 ]
 [ 0.10240531 -0.16969892  0.02932969 -0.18588154 -0.02920584  0.16449704
  -0.10496703 -0.06957847 -0.10764813  0.01770536 -0.10726422  0.1558164
  -0.11053138 -0.16742723 -0.08325951]
 [ 0.12228662 -0.10113017 -0.11088235 -0.02273008  0.045944    0.16838683
  -0.05924955  0.03015378 -0.02845317 -0.00840918  0.1114738   0.11584881
  -0.06747411 -0.01089541  0.09901299]
 [ 0.10616978 -0.04918999 -0.20135272 -0.17980479 -0.0629727   0.05596095
   0.19298626 -0.19697814 -0.12070697 -0.06498952 -0.17667471 -0.1304842
  -0.04221673  0.02465863 -0.0446321 ]
 [-0.19567049 -0.00412503 -0.12743787  0.08695684 -0.0195773  -0.0557995
   0.09808126  0.02576018 -0.16594723  0.0229791   0.05408817 -0.02897398
   0.18549374 -0.07108655  0.09575128]
 [-0.00465921 -0.06585093 -0.19862225  0.13491283 -0.10645117 -0.16725057
   0.03391234 -0.08704489  0.19487333 -0.04237282  0.12710687 -0.02575923
   0.02118181  0.05507428 -0.11823875]
 [ 0.05494187  0.14810162 -0.14678285 -0.19204054  0.01662044  0.13667806
   0.00351961 -0.10485534  0.02497872  0.1368676  -0.10877146 -0.1988002
   0.08331115  0.20114992  0.09221563]
 [-0.04642524  0.14152469  0.12155809 -0.0353647  -0.05461389 -0.13359303
  -0.02240779 -0.09232268  0.05772537 -0.18853849  0.04460409  0.17344802
   0.04590344 -0.13008807 -0.03722341]
 [ 0.06115351  0.05975444 -0.00416431  0.16801567 -0.13288711 -0.09238088
  -0.01328462  0.13300623 -0.18809195 -0.08045625 -0.09285911 -0.16348216
  -0.10177567  0.04214115  0.07415323]
 [ 0.01887584  0.04544055 -0.06644728 -0.00594557 -0.06605866  0.02232214
  -0.07665332 -0.17952918  0.16211469 -0.17152496 -0.11077649 -0.18199607
   0.14813869 -0.05283232 -0.08978888]
 [-0.13430845 -0.15700654  0.19207957 -0.14666954  0.10382191 -0.03821131
   0.04671143  0.15768999 -0.07783038 -0.05314137 -0.19706258  0.19268615
  -0.04478814 -0.12170198  0.04613801]
 [-0.15646726  0.16523921  0.11979218 -0.06794239 -0.1069118   0.02585867
  -0.00198537  0.13792439  0.18527744  0.04768079  0.15352385  0.11547485
  -0.14175726  0.13749185  0.06042375]
 [ 0.19240613  0.14828304  0.05310555  0.09750048  0.19049541  0.05014811
  -0.02498476 -0.11472338  0.00828241  0.15724562 -0.17910622 -0.03523066
  -0.08543208 -0.07749939  0.06344636]]
Bias Camada de Entrada: 
[-0.01492409 -0.10707612  0.00746874 -0.08664194 -0.13256734  0.09715563
  0.11152348 -0.19902175  0.13439067  0.19350807 -0.0524331   0.13464895
  0.0526816  -0.18148799  0.03584934]
Pesos Camada Escondida: 
[[-0.02583669]
 [ 0.09429505]
 [-0.34731589]
 [ 0.09335636]
 [-0.22487946]
 [ 0.29041503]
 [ 0.12608161]
 [-0.00727004]
 [ 0.15056841]
 [-0.28058801]
 [-0.3432666 ]
 [ 0.082002  ]
 [-0.02537435]
 [ 0.12759346]
 [-0.09790698]]
Bias Camada Escondida: 
[-0.23008382]
Iteration 1, loss = 0.72820614
Iteration 2, loss = 0.68320860
Iteration 3, loss = 0.62579751
Iteration 4, loss = 0.60595072
Iteration 5, loss = 0.56161667
Iteration 6, loss = 0.53064517
Iteration 7, loss = 0.49594451
Iteration 8, loss = 0.43175282
Iteration 9, loss = 0.39199583
Iteration 10, loss = 0.36344750
Iteration 11, loss = 0.33769186
Iteration 12, loss = 0.31876165
Iteration 13, loss = 0.30420434
Iteration 14, loss = 0.29148767
Iteration 15, loss = 0.28206604
Iteration 16, loss = 0.27422426
Iteration 17, loss = 0.26856033
Iteration 18, loss = 0.25685997
Iteration 19, loss = 0.26487783
Iteration 20, loss = 0.23696609
Iteration 21, loss = 0.23434173
Iteration 22, loss = 0.22188668
Iteration 23, loss = 0.21286545
Iteration 24, loss = 0.20270185
Iteration 25, loss = 0.19469213
Iteration 26, loss = 0.18856804
Iteration 27, loss = 0.18284828
Iteration 28, loss = 0.18198272
Iteration 29, loss = 0.17135793
Iteration 30, loss = 0.16996838
Iteration 31, loss = 0.16173640
Iteration 32, loss = 0.15220289
Iteration 33, loss = 0.14775483
Iteration 34, loss = 0.14695351
Iteration 35, loss = 0.14048560
Iteration 36, loss = 0.14205727
Iteration 37, loss = 0.13153388
Iteration 38, loss = 0.12688438
Iteration 39, loss = 0.12416116
Iteration 40, loss = 0.12136207
Iteration 41, loss = 0.12308366
Iteration 42, loss = 0.12405765
Iteration 43, loss = 0.11342712
Iteration 44, loss = 0.10936181
Iteration 45, loss = 0.10958796
Iteration 46, loss = 0.10436807
Iteration 47, loss = 0.12230184
Iteration 48, loss = 0.09778702
Iteration 49, loss = 0.09773640
Iteration 50, loss = 0.09278010
Iteration 51, loss = 0.10079325
Iteration 52, loss = 0.09170324
Iteration 53, loss = 0.08549046
Iteration 54, loss = 0.08666646
Iteration 55, loss = 0.08283632
Iteration 56, loss = 0.08276882
Iteration 57, loss = 0.08296236
Iteration 58, loss = 0.07720102
Iteration 59, loss = 0.07528503
Iteration 60, loss = 0.07346969
Iteration 61, loss = 0.07050303
Iteration 62, loss = 0.06888095
Iteration 63, loss = 0.06775637
Iteration 64, loss = 0.07277691
Iteration 65, loss = 0.06551969
Iteration 66, loss = 0.06197618
Iteration 67, loss = 0.05856467
Iteration 68, loss = 0.05782635
Iteration 69, loss = 0.05654905
Iteration 70, loss = 0.05538641
Iteration 71, loss = 0.05771398
Iteration 72, loss = 0.05514919
Iteration 73, loss = 0.05041486
Iteration 74, loss = 0.04998124
Iteration 75, loss = 0.05120675
Iteration 76, loss = 0.04896998
Iteration 77, loss = 0.04564932
Iteration 78, loss = 0.04610445
Iteration 79, loss = 0.04534976
Iteration 80, loss = 0.04285209
Iteration 81, loss = 0.04113774
Iteration 82, loss = 0.04026538
Iteration 83, loss = 0.04004651
Iteration 84, loss = 0.03869609
Iteration 85, loss = 0.03757135
Iteration 86, loss = 0.03674818
Iteration 87, loss = 0.03652821
Iteration 88, loss = 0.03524423
Iteration 89, loss = 0.03487769
Iteration 90, loss = 0.03386840
Iteration 91, loss = 0.03313673
Iteration 92, loss = 0.03261947
Iteration 93, loss = 0.03213502
Iteration 94, loss = 0.03181922
Iteration 95, loss = 0.03178494
Iteration 96, loss = 0.03074339
Iteration 97, loss = 0.03052045
Iteration 98, loss = 0.02844515
Iteration 99, loss = 0.03116388
Iteration 100, loss = 0.03030696
Iteration 101, loss = 0.02713449
Iteration 102, loss = 0.02915370
Iteration 103, loss = 0.02612996
Iteration 104, loss = 0.02701985
Iteration 105, loss = 0.02795136
Iteration 106, loss = 0.02470417
Iteration 107, loss = 0.02475986
Iteration 108, loss = 0.02479959
Iteration 109, loss = 0.02369755
Iteration 110, loss = 0.02320654
Iteration 111, loss = 0.02310449
Iteration 112, loss = 0.02263073
Iteration 113, loss = 0.02207118
Iteration 114, loss = 0.02149674
Iteration 115, loss = 0.02131179
Iteration 116, loss = 0.02195966
Iteration 117, loss = 0.02220670
Iteration 118, loss = 0.02128966
Iteration 119, loss = 0.02024617
Iteration 120, loss = 0.01953264
Iteration 121, loss = 0.02023711
Iteration 122, loss = 0.01981333
Iteration 123, loss = 0.01934346
Iteration 124, loss = 0.01907997
Iteration 125, loss = 0.01887813
Iteration 126, loss = 0.01858631
Iteration 127, loss = 0.01781654
Iteration 128, loss = 0.01779411
Iteration 129, loss = 0.01728945
Iteration 130, loss = 0.01773220
Iteration 131, loss = 0.01790218
Iteration 132, loss = 0.01643604
Iteration 133, loss = 0.01634535
Iteration 134, loss = 0.01704130
Iteration 135, loss = 0.01620114
Iteration 136, loss = 0.01558216
Iteration 137, loss = 0.01578362
Iteration 138, loss = 0.01563590
Iteration 139, loss = 0.01549330
Iteration 140, loss = 0.01527034
Iteration 141, loss = 0.01513880
Iteration 142, loss = 0.01510410
Iteration 143, loss = 0.01493512
Iteration 144, loss = 0.01454279
Iteration 145, loss = 0.01413562
Iteration 146, loss = 0.01387298
Iteration 147, loss = 0.01382293
Iteration 148, loss = 0.01389269
Iteration 149, loss = 0.01403755
Iteration 150, loss = 0.01346755
Iteration 151, loss = 0.01338384
Iteration 152, loss = 0.01299387
Iteration 153, loss = 0.01276838
Iteration 154, loss = 0.01283000
Iteration 155, loss = 0.01277588
Iteration 156, loss = 0.01243389
Iteration 157, loss = 0.01256888
Iteration 158, loss = 0.01255576
Iteration 159, loss = 0.01227069
Iteration 160, loss = 0.01186578
Iteration 161, loss = 0.01202033
Iteration 162, loss = 0.01230755
Iteration 163, loss = 0.01265595
Iteration 164, loss = 0.01245120
Iteration 165, loss = 0.01153658
Iteration 166, loss = 0.01139914
Iteration 167, loss = 0.01135364
Iteration 168, loss = 0.01122282
Iteration 169, loss = 0.01101142
Iteration 170, loss = 0.01096987
Iteration 171, loss = 0.01098602
Iteration 172, loss = 0.01081802
Iteration 173, loss = 0.01070388
Iteration 174, loss = 0.01055080
Iteration 175, loss = 0.01037252
Iteration 176, loss = 0.01023711
Iteration 177, loss = 0.01019797
Iteration 178, loss = 0.01015849
Iteration 179, loss = 0.01012720
Iteration 180, loss = 0.01004558
Iteration 181, loss = 0.00987870
Iteration 182, loss = 0.00969507
Iteration 183, loss = 0.00958443
Iteration 184, loss = 0.00957770
Iteration 185, loss = 0.00967936
Iteration 186, loss = 0.00940605
Iteration 187, loss = 0.00922074
Iteration 188, loss = 0.00915481
Iteration 189, loss = 0.00914230
Iteration 190, loss = 0.00914529
Iteration 191, loss = 0.00922256
Iteration 192, loss = 0.00880710
Iteration 193, loss = 0.00897341
Iteration 194, loss = 0.00877542
Iteration 195, loss = 0.00862645
Iteration 196, loss = 0.00853561
Iteration 197, loss = 0.00848640
Iteration 198, loss = 0.00848354
Iteration 199, loss = 0.00840365
Iteration 200, loss = 0.00834972
Iteration 201, loss = 0.00829101
Iteration 202, loss = 0.00821218
Iteration 203, loss = 0.00815951
Iteration 204, loss = 0.00815331
Iteration 205, loss = 0.00806012
Iteration 206, loss = 0.00790171
Iteration 207, loss = 0.00785249
Iteration 208, loss = 0.00786474
Iteration 209, loss = 0.00796143
Iteration 210, loss = 0.00797132
Iteration 211, loss = 0.00769908
Iteration 212, loss = 0.00759379
Iteration 213, loss = 0.00764226
Iteration 214, loss = 0.00782444
Iteration 215, loss = 0.00773105
Iteration 216, loss = 0.00749718
Iteration 217, loss = 0.00738436
Iteration 218, loss = 0.00730944
Iteration 219, loss = 0.00729963
Iteration 220, loss = 0.00718679
Iteration 221, loss = 0.00712914
Iteration 222, loss = 0.00713115
Iteration 223, loss = 0.00699438
Iteration 224, loss = 0.00697358
Iteration 225, loss = 0.00693370
Iteration 226, loss = 0.00687730
Iteration 227, loss = 0.00684182
Iteration 228, loss = 0.00679617
Iteration 229, loss = 0.00678263
Iteration 230, loss = 0.00666144
Iteration 231, loss = 0.00668706
Iteration 232, loss = 0.00670136
Iteration 233, loss = 0.00667441
Iteration 234, loss = 0.00664761
Iteration 235, loss = 0.00661319
Iteration 236, loss = 0.00642158
Iteration 237, loss = 0.00634856
Iteration 238, loss = 0.00636942
Iteration 239, loss = 0.00631948
Iteration 240, loss = 0.00627344
Iteration 241, loss = 0.00624324
Iteration 242, loss = 0.00621703
Iteration 243, loss = 0.00609978
Iteration 244, loss = 0.00607487
Iteration 245, loss = 0.00603235
Iteration 246, loss = 0.00599875
Iteration 247, loss = 0.00596537
Iteration 248, loss = 0.00592476
Iteration 249, loss = 0.00587943
Iteration 250, loss = 0.00585644
Iteration 251, loss = 0.00587778
Iteration 252, loss = 0.00585052
Iteration 253, loss = 0.00579525
Iteration 254, loss = 0.00572161
Iteration 255, loss = 0.00575447
Iteration 256, loss = 0.00572351
Iteration 257, loss = 0.00562833
Iteration 258, loss = 0.00561936
Iteration 259, loss = 0.00568632
Iteration 260, loss = 0.00564672
Iteration 261, loss = 0.00552353
Iteration 262, loss = 0.00547210
Iteration 263, loss = 0.00541906
Iteration 264, loss = 0.00539118
Iteration 265, loss = 0.00536567
Iteration 266, loss = 0.00534661
Iteration 267, loss = 0.00530762
Iteration 268, loss = 0.00532551
Iteration 269, loss = 0.00525429
Iteration 270, loss = 0.00525337
Iteration 271, loss = 0.00520130
Iteration 272, loss = 0.00520834
Iteration 273, loss = 0.00518581
Iteration 274, loss = 0.00516544
Iteration 275, loss = 0.00510995
Iteration 276, loss = 0.00508035
Iteration 277, loss = 0.00510550
Iteration 278, loss = 0.00514839
Iteration 279, loss = 0.00518595
Iteration 280, loss = 0.00520957
Iteration 281, loss = 0.00515740
Iteration 282, loss = 0.00505499
Iteration 283, loss = 0.00492495
Iteration 284, loss = 0.00487570
Iteration 285, loss = 0.00482983
Iteration 286, loss = 0.00487178
Iteration 287, loss = 0.00479209
Iteration 288, loss = 0.00476548
Iteration 289, loss = 0.00470236
Iteration 290, loss = 0.00465719
Iteration 291, loss = 0.00466676
Iteration 292, loss = 0.00466436
Iteration 293, loss = 0.00463261
Iteration 294, loss = 0.00455093
Iteration 295, loss = 0.00452167
Iteration 296, loss = 0.00454768
Iteration 297, loss = 0.00452458
Iteration 298, loss = 0.00447310
Iteration 299, loss = 0.00443442
Iteration 300, loss = 0.00443852
Iteration 301, loss = 0.00444037
Iteration 302, loss = 0.00442969
Iteration 303, loss = 0.00443032
Iteration 304, loss = 0.00434707
Iteration 305, loss = 0.00433873
Iteration 306, loss = 0.00432995
Iteration 307, loss = 0.00428881
Iteration 308, loss = 0.00423924
Iteration 309, loss = 0.00422042
Iteration 310, loss = 0.00422531
Iteration 311, loss = 0.00418485
Iteration 312, loss = 0.00418683
Iteration 313, loss = 0.00420043
Iteration 314, loss = 0.00415134
Iteration 315, loss = 0.00409971
Iteration 316, loss = 0.00406635
Iteration 317, loss = 0.00409260
Iteration 318, loss = 0.00404416
Iteration 319, loss = 0.00402876
Iteration 320, loss = 0.00400423
Iteration 321, loss = 0.00397771
Iteration 322, loss = 0.00395823
Iteration 323, loss = 0.00394324
Iteration 324, loss = 0.00392814
Iteration 325, loss = 0.00392776
Iteration 326, loss = 0.00395214
Iteration 327, loss = 0.00392982
Iteration 328, loss = 0.00388487
Iteration 329, loss = 0.00383719
Iteration 330, loss = 0.00382087
Iteration 331, loss = 0.00379908
Iteration 332, loss = 0.00380149
Iteration 333, loss = 0.00378670
Iteration 334, loss = 0.00376570
Iteration 335, loss = 0.00374395
Iteration 336, loss = 0.00373904
Iteration 337, loss = 0.00373686
Iteration 338, loss = 0.00369600
Iteration 339, loss = 0.00368125
Iteration 340, loss = 0.00365679
Iteration 341, loss = 0.00363728
Iteration 342, loss = 0.00361956
Iteration 343, loss = 0.00360697
Iteration 344, loss = 0.00359432
Iteration 345, loss = 0.00358234
Iteration 346, loss = 0.00357017
Iteration 347, loss = 0.00355512
Iteration 348, loss = 0.00354017
Iteration 349, loss = 0.00352656
Iteration 350, loss = 0.00351277
Iteration 351, loss = 0.00350148
Iteration 352, loss = 0.00348440
Iteration 353, loss = 0.00348417
Iteration 354, loss = 0.00345583
Iteration 355, loss = 0.00344254
Iteration 356, loss = 0.00344144
Iteration 357, loss = 0.00342228
Iteration 358, loss = 0.00341153
Iteration 359, loss = 0.00339653
Iteration 360, loss = 0.00340195
Iteration 361, loss = 0.00336493
Iteration 362, loss = 0.00336452
Iteration 363, loss = 0.00332869
Iteration 364, loss = 0.00331177
Iteration 365, loss = 0.00329599
Iteration 366, loss = 0.00330695
Iteration 367, loss = 0.00327533
Iteration 368, loss = 0.00326258
Iteration 369, loss = 0.00325280
Iteration 370, loss = 0.00324768
Iteration 371, loss = 0.00322180
Iteration 372, loss = 0.00320328
Iteration 373, loss = 0.00318571
Iteration 374, loss = 0.00317705
Iteration 375, loss = 0.00316863
Iteration 376, loss = 0.00314933
Iteration 377, loss = 0.00313628
Iteration 378, loss = 0.00312929
Iteration 379, loss = 0.00312161
Iteration 380, loss = 0.00310551
Iteration 381, loss = 0.00309440
Iteration 382, loss = 0.00311552
Iteration 383, loss = 0.00305750
Iteration 384, loss = 0.00308099
Iteration 385, loss = 0.00308451
Iteration 386, loss = 0.00307353
Iteration 387, loss = 0.00305170
Iteration 388, loss = 0.00301889
Iteration 389, loss = 0.00300716
Iteration 390, loss = 0.00298442
Iteration 391, loss = 0.00298789
Iteration 392, loss = 0.00298926
Iteration 393, loss = 0.00296199
Iteration 394, loss = 0.00294246
Iteration 395, loss = 0.00293871
Iteration 396, loss = 0.00293223
Iteration 397, loss = 0.00292444
Iteration 398, loss = 0.00288172
Iteration 399, loss = 0.00288658
Iteration 400, loss = 0.00287682
Iteration 401, loss = 0.00285410
Iteration 402, loss = 0.00286810
Iteration 403, loss = 0.00283508
Iteration 404, loss = 0.00282191
Iteration 405, loss = 0.00282376
Iteration 406, loss = 0.00282826
Iteration 407, loss = 0.00282155
Iteration 408, loss = 0.00280411
Iteration 409, loss = 0.00278514
Iteration 410, loss = 0.00277497
Iteration 411, loss = 0.00275884
Iteration 412, loss = 0.00274737
Iteration 413, loss = 0.00274823
Iteration 414, loss = 0.00278422
Iteration 415, loss = 0.00275742
Iteration 416, loss = 0.00274147
Iteration 417, loss = 0.00272115
Iteration 418, loss = 0.00270791
Iteration 419, loss = 0.00269314
Iteration 420, loss = 0.00268338
Iteration 421, loss = 0.00267700
Iteration 422, loss = 0.00268105
Iteration 423, loss = 0.00265928
Iteration 424, loss = 0.00265774
Iteration 425, loss = 0.00264123
Iteration 426, loss = 0.00262565
Iteration 427, loss = 0.00260582
Iteration 428, loss = 0.00260113
Iteration 429, loss = 0.00261010
Iteration 430, loss = 0.00261559
Iteration 431, loss = 0.00261875
Iteration 432, loss = 0.00262787
Iteration 433, loss = 0.00263179
Iteration 434, loss = 0.00260864
Iteration 435, loss = 0.00257956
Iteration 436, loss = 0.00255327
Iteration 437, loss = 0.00253460
Iteration 438, loss = 0.00252274
Iteration 439, loss = 0.00251168
Iteration 440, loss = 0.00250247
Iteration 441, loss = 0.00249627
Iteration 442, loss = 0.00248897
Iteration 443, loss = 0.00248086
Iteration 444, loss = 0.00247372
Iteration 445, loss = 0.00246801
Iteration 446, loss = 0.00245888
Iteration 447, loss = 0.00245381
Iteration 448, loss = 0.00245915
Iteration 449, loss = 0.00245513
Iteration 450, loss = 0.00246507
Iteration 451, loss = 0.00247795
Iteration 452, loss = 0.00247530
Iteration 453, loss = 0.00245624
Iteration 454, loss = 0.00242536
Iteration 455, loss = 0.00240017
Iteration 456, loss = 0.00240038
Iteration 457, loss = 0.00238252
Iteration 458, loss = 0.00238512
Iteration 459, loss = 0.00237759
Iteration 460, loss = 0.00238816
Iteration 461, loss = 0.00239493
Iteration 462, loss = 0.00238574
Iteration 463, loss = 0.00237705
Iteration 464, loss = 0.00235438
Iteration 465, loss = 0.00233023
Iteration 466, loss = 0.00231914
Iteration 467, loss = 0.00230762
Iteration 468, loss = 0.00230647
Iteration 469, loss = 0.00229906
Iteration 470, loss = 0.00229059
Iteration 471, loss = 0.00230340
Iteration 472, loss = 0.00227616
Iteration 473, loss = 0.00228128
Iteration 474, loss = 0.00228442
Iteration 475, loss = 0.00228620
Iteration 476, loss = 0.00228754
Iteration 477, loss = 0.00228902
Iteration 478, loss = 0.00228670
Iteration 479, loss = 0.00228148
Iteration 480, loss = 0.00227429
Iteration 481, loss = 0.00225684
Iteration 482, loss = 0.00223218
Iteration 483, loss = 0.00221544
Iteration 484, loss = 0.00220354
Iteration 485, loss = 0.00220454
Iteration 486, loss = 0.00220854
Iteration 487, loss = 0.00220635
Iteration 488, loss = 0.00220697
Iteration 489, loss = 0.00218349
Iteration 490, loss = 0.00218369
Iteration 491, loss = 0.00216705
Iteration 492, loss = 0.00216211
Iteration 493, loss = 0.00215782
Iteration 494, loss = 0.00215302
Iteration 495, loss = 0.00214930
Iteration 496, loss = 0.00214521
Iteration 497, loss = 0.00213797
Iteration 498, loss = 0.00213001
Iteration 499, loss = 0.00212164
Iteration 500, loss = 0.00212316
Iteration 501, loss = 0.00211073
Iteration 502, loss = 0.00210371
Iteration 503, loss = 0.00209686
Iteration 504, loss = 0.00209162
Iteration 505, loss = 0.00208684
Iteration 506, loss = 0.00208296
Iteration 507, loss = 0.00207819
Iteration 508, loss = 0.00207386
Iteration 509, loss = 0.00206785
Iteration 510, loss = 0.00206060
Iteration 511, loss = 0.00205727
Iteration 512, loss = 0.00205216
Iteration 513, loss = 0.00204883
Iteration 514, loss = 0.00205282
Iteration 515, loss = 0.00205046
Iteration 516, loss = 0.00204683
Iteration 517, loss = 0.00204217
Iteration 518, loss = 0.00203476
Iteration 519, loss = 0.00202492
Iteration 520, loss = 0.00201032
Iteration 521, loss = 0.00201293
Iteration 522, loss = 0.00200180
Iteration 523, loss = 0.00199065
Iteration 524, loss = 0.00198717
Iteration 525, loss = 0.00198050
Iteration 526, loss = 0.00197524
Iteration 527, loss = 0.00197205
Iteration 528, loss = 0.00196609
Iteration 529, loss = 0.00196333
Iteration 530, loss = 0.00195919
Iteration 531, loss = 0.00195485
Iteration 532, loss = 0.00195038
Iteration 533, loss = 0.00194826
Iteration 534, loss = 0.00194201
Iteration 535, loss = 0.00193340
Iteration 536, loss = 0.00193006
Iteration 537, loss = 0.00192848
Iteration 538, loss = 0.00192505
Iteration 539, loss = 0.00192322
Iteration 540, loss = 0.00192221
Iteration 541, loss = 0.00191799
Iteration 542, loss = 0.00191602
Iteration 543, loss = 0.00189951
Iteration 544, loss = 0.00189303
Iteration 545, loss = 0.00189010
Iteration 546, loss = 0.00188774
Iteration 547, loss = 0.00189205
Iteration 548, loss = 0.00188433
Iteration 549, loss = 0.00187955
Iteration 550, loss = 0.00186642
Iteration 551, loss = 0.00186254
Iteration 552, loss = 0.00185984
Iteration 553, loss = 0.00185440
Iteration 554, loss = 0.00184951
Iteration 555, loss = 0.00184018
Iteration 556, loss = 0.00183927
Iteration 557, loss = 0.00183346
Iteration 558, loss = 0.00182904
Iteration 559, loss = 0.00183309
Iteration 560, loss = 0.00182661
Iteration 561, loss = 0.00182357
Iteration 562, loss = 0.00181877
Iteration 563, loss = 0.00181739
Iteration 564, loss = 0.00180137
Iteration 565, loss = 0.00179768
Iteration 566, loss = 0.00179046
Iteration 567, loss = 0.00178958
Iteration 568, loss = 0.00177744
Iteration 569, loss = 0.00177156
Iteration 570, loss = 0.00176624
Iteration 571, loss = 0.00176381
Iteration 572, loss = 0.00175825
Iteration 573, loss = 0.00177235
Iteration 574, loss = 0.00176509
Iteration 575, loss = 0.00176172
Iteration 576, loss = 0.00176443
Iteration 577, loss = 0.00175093
Iteration 578, loss = 0.00173878
Iteration 579, loss = 0.00173106
Iteration 580, loss = 0.00172801
Iteration 581, loss = 0.00173569
Iteration 582, loss = 0.00172902
Iteration 583, loss = 0.00172015
Iteration 584, loss = 0.00171842
Iteration 585, loss = 0.00171446
Iteration 586, loss = 0.00170554
Iteration 587, loss = 0.00170211
Iteration 588, loss = 0.00169783
Iteration 589, loss = 0.00169152
Iteration 590, loss = 0.00168749
Iteration 591, loss = 0.00169176
Iteration 592, loss = 0.00168013
Iteration 593, loss = 0.00168499
Iteration 594, loss = 0.00167921
Iteration 595, loss = 0.00167297
Iteration 596, loss = 0.00166855
Iteration 597, loss = 0.00166285
Iteration 598, loss = 0.00166366
Iteration 599, loss = 0.00166084
Iteration 600, loss = 0.00165798
Iteration 601, loss = 0.00165640
Iteration 602, loss = 0.00164532
Iteration 603, loss = 0.00163431
Iteration 604, loss = 0.00162686
Iteration 605, loss = 0.00162933
Iteration 606, loss = 0.00162317
Iteration 607, loss = 0.00162407
Iteration 608, loss = 0.00162334
Iteration 609, loss = 0.00162346
Iteration 610, loss = 0.00160733
Iteration 611, loss = 0.00160205
Iteration 612, loss = 0.00159963
Iteration 613, loss = 0.00161428
Iteration 614, loss = 0.00159564
Iteration 615, loss = 0.00158971
Iteration 616, loss = 0.00158619
Iteration 617, loss = 0.00158341
Iteration 618, loss = 0.00158032
Iteration 619, loss = 0.00157898
Iteration 620, loss = 0.00157603
Iteration 621, loss = 0.00157370
Iteration 622, loss = 0.00157534
Iteration 623, loss = 0.00157322
Iteration 624, loss = 0.00157108
Iteration 625, loss = 0.00157020
Iteration 626, loss = 0.00156671
Iteration 627, loss = 0.00156239
Iteration 628, loss = 0.00155593
Iteration 629, loss = 0.00155677
Iteration 630, loss = 0.00154618
Iteration 631, loss = 0.00154106
Iteration 632, loss = 0.00154003
Iteration 633, loss = 0.00153811
Iteration 634, loss = 0.00153596
Iteration 635, loss = 0.00153353
Iteration 636, loss = 0.00153844
Iteration 637, loss = 0.00152886
Iteration 638, loss = 0.00152331
Iteration 639, loss = 0.00151458
Iteration 640, loss = 0.00150682
Iteration 641, loss = 0.00151198
Iteration 642, loss = 0.00150128
Iteration 643, loss = 0.00149884
Iteration 644, loss = 0.00149756
Iteration 645, loss = 0.00149380
Iteration 646, loss = 0.00148886
Iteration 647, loss = 0.00148831
Iteration 648, loss = 0.00149015
Iteration 649, loss = 0.00149656
Iteration 650, loss = 0.00150375
Iteration 651, loss = 0.00149666
Iteration 652, loss = 0.00149512
Iteration 653, loss = 0.00148684
Iteration 654, loss = 0.00148131
Iteration 655, loss = 0.00147250
Iteration 656, loss = 0.00146585
Iteration 657, loss = 0.00145740
Iteration 658, loss = 0.00145178
Iteration 659, loss = 0.00144983
Iteration 660, loss = 0.00144956
Iteration 661, loss = 0.00145461
Iteration 662, loss = 0.00145741
Iteration 663, loss = 0.00146075
Iteration 664, loss = 0.00146205
Iteration 665, loss = 0.00145835
Iteration 666, loss = 0.00145272
Iteration 667, loss = 0.00144611
Iteration 668, loss = 0.00144001
Iteration 669, loss = 0.00143405
Iteration 670, loss = 0.00143113
Iteration 671, loss = 0.00142836
Iteration 672, loss = 0.00142807
Iteration 673, loss = 0.00142058
Iteration 674, loss = 0.00142191
Iteration 675, loss = 0.00141392
Iteration 676, loss = 0.00141271
Iteration 677, loss = 0.00140959
Iteration 678, loss = 0.00140700
Iteration 679, loss = 0.00141112
Iteration 680, loss = 0.00140135
Iteration 681, loss = 0.00139505
Iteration 682, loss = 0.00139062
Iteration 683, loss = 0.00138673
Iteration 684, loss = 0.00138594
Iteration 685, loss = 0.00138981
Iteration 686, loss = 0.00139264
Iteration 687, loss = 0.00137948
Iteration 688, loss = 0.00137336
Iteration 689, loss = 0.00136894
Iteration 690, loss = 0.00137266
Iteration 691, loss = 0.00137445
Iteration 692, loss = 0.00138039
Iteration 693, loss = 0.00138664
Iteration 694, loss = 0.00138992
Iteration 695, loss = 0.00139137
Iteration 696, loss = 0.00138987
Iteration 697, loss = 0.00138399
Iteration 698, loss = 0.00137359
Iteration 699, loss = 0.00135922
Iteration 700, loss = 0.00134773
Iteration 701, loss = 0.00134225
Iteration 702, loss = 0.00134317
Iteration 703, loss = 0.00134171
Iteration 704, loss = 0.00134806
Iteration 705, loss = 0.00134100
Iteration 706, loss = 0.00133795
Iteration 707, loss = 0.00132881
Iteration 708, loss = 0.00132358
Iteration 709, loss = 0.00132346
Iteration 710, loss = 0.00133301
Iteration 711, loss = 0.00132680
Iteration 712, loss = 0.00132245
Iteration 713, loss = 0.00132265
Iteration 714, loss = 0.00132118
Iteration 715, loss = 0.00131910
Iteration 716, loss = 0.00131926
Iteration 717, loss = 0.00131088
Iteration 718, loss = 0.00129990
Iteration 719, loss = 0.00129116
Iteration 720, loss = 0.00129396
Iteration 721, loss = 0.00130356
Iteration 722, loss = 0.00130789
Iteration 723, loss = 0.00132095
Iteration 724, loss = 0.00131312
Iteration 725, loss = 0.00130689
Iteration 726, loss = 0.00130072
Iteration 727, loss = 0.00129475
Iteration 728, loss = 0.00128868
Iteration 729, loss = 0.00128078
Iteration 730, loss = 0.00127341
Iteration 731, loss = 0.00126811
Iteration 732, loss = 0.00126428
Iteration 733, loss = 0.00126202
Iteration 734, loss = 0.00126528
Iteration 735, loss = 0.00125920
Iteration 736, loss = 0.00125669
Iteration 737, loss = 0.00125420
Iteration 738, loss = 0.00125310
Iteration 739, loss = 0.00125516
Iteration 740, loss = 0.00124750
Iteration 741, loss = 0.00124543
Iteration 742, loss = 0.00124130
Iteration 743, loss = 0.00124127
Iteration 744, loss = 0.00123707
Iteration 745, loss = 0.00123529
Iteration 746, loss = 0.00123857
Iteration 747, loss = 0.00123500
Iteration 748, loss = 0.00123320
Iteration 749, loss = 0.00123146
Iteration 750, loss = 0.00123319
Iteration 751, loss = 0.00122682
Iteration 752, loss = 0.00122488
Iteration 753, loss = 0.00122490
Iteration 754, loss = 0.00122177
Iteration 755, loss = 0.00122161
Iteration 756, loss = 0.00121550
Iteration 757, loss = 0.00121452
Iteration 758, loss = 0.00121151
Iteration 759, loss = 0.00120784
Iteration 760, loss = 0.00120369
Iteration 761, loss = 0.00119966
Iteration 762, loss = 0.00119718
Iteration 763, loss = 0.00119620
Iteration 764, loss = 0.00119242
Iteration 765, loss = 0.00119145
Iteration 766, loss = 0.00119232
Iteration 767, loss = 0.00119357
Iteration 768, loss = 0.00118929
Iteration 769, loss = 0.00118950
Iteration 770, loss = 0.00118650
Iteration 771, loss = 0.00118309
Iteration 772, loss = 0.00118197
Iteration 773, loss = 0.00117712
Iteration 774, loss = 0.00117694
Iteration 775, loss = 0.00117371
Iteration 776, loss = 0.00117407
Iteration 777, loss = 0.00116850
Iteration 778, loss = 0.00116507
Iteration 779, loss = 0.00116217
Iteration 780, loss = 0.00116144
Iteration 781, loss = 0.00115850
Iteration 782, loss = 0.00115579
Iteration 783, loss = 0.00115932
Iteration 784, loss = 0.00115659
Iteration 785, loss = 0.00115899
Iteration 786, loss = 0.00115392
Iteration 787, loss = 0.00114763
Iteration 788, loss = 0.00114388
Iteration 789, loss = 0.00114270
Iteration 790, loss = 0.00114627
Iteration 791, loss = 0.00114605
Iteration 792, loss = 0.00114785
Iteration 793, loss = 0.00114335
Iteration 794, loss = 0.00114016
Iteration 795, loss = 0.00113714
Iteration 796, loss = 0.00113451
Iteration 797, loss = 0.00113275
Iteration 798, loss = 0.00113000
Iteration 799, loss = 0.00112611
Iteration 800, loss = 0.00112539
Iteration 801, loss = 0.00112248
Iteration 802, loss = 0.00112014
Iteration 803, loss = 0.00111976
Iteration 804, loss = 0.00111673
Iteration 805, loss = 0.00111796
Iteration 806, loss = 0.00111681
Iteration 807, loss = 0.00111595
Iteration 808, loss = 0.00111553
Iteration 809, loss = 0.00111328
Iteration 810, loss = 0.00111633
Iteration 811, loss = 0.00110594
Iteration 812, loss = 0.00110109
Iteration 813, loss = 0.00110246
Iteration 814, loss = 0.00109931
Iteration 815, loss = 0.00109806
Iteration 816, loss = 0.00109971
Iteration 817, loss = 0.00109744
Iteration 818, loss = 0.00109680
Iteration 819, loss = 0.00109469
Iteration 820, loss = 0.00109143
Iteration 821, loss = 0.00108858
Iteration 822, loss = 0.00108574
Iteration 823, loss = 0.00108669
Iteration 824, loss = 0.00108312
Iteration 825, loss = 0.00108203
Iteration 826, loss = 0.00108177
Iteration 827, loss = 0.00108125
Iteration 828, loss = 0.00108062
Iteration 829, loss = 0.00107997
Iteration 830, loss = 0.00107867
Iteration 831, loss = 0.00107870
Iteration 832, loss = 0.00107403
Iteration 833, loss = 0.00107038
Iteration 834, loss = 0.00106764
Iteration 835, loss = 0.00106500
Iteration 836, loss = 0.00106348
Iteration 837, loss = 0.00106080
Iteration 838, loss = 0.00105927
Iteration 839, loss = 0.00105763
Iteration 840, loss = 0.00105698
Iteration 841, loss = 0.00105510
Iteration 842, loss = 0.00105657
Iteration 843, loss = 0.00105741
Iteration 844, loss = 0.00105603
Iteration 845, loss = 0.00105369
Iteration 846, loss = 0.00105264
Iteration 847, loss = 0.00105020
Iteration 848, loss = 0.00104883
Iteration 849, loss = 0.00104663
Iteration 850, loss = 0.00104153
Iteration 851, loss = 0.00103937
Iteration 852, loss = 0.00103707
Iteration 853, loss = 0.00103686
Iteration 854, loss = 0.00103574
Iteration 855, loss = 0.00103458
Iteration 856, loss = 0.00103393
Iteration 857, loss = 0.00103364
Iteration 858, loss = 0.00103266
Iteration 859, loss = 0.00103167
Iteration 860, loss = 0.00103000
Iteration 861, loss = 0.00102874
Iteration 862, loss = 0.00102769
Iteration 863, loss = 0.00102424
Iteration 864, loss = 0.00102105
Iteration 865, loss = 0.00101915
Iteration 866, loss = 0.00101798
Iteration 867, loss = 0.00101833
Iteration 868, loss = 0.00101526
Iteration 869, loss = 0.00101460
Iteration 870, loss = 0.00101237
Iteration 871, loss = 0.00101466
Iteration 872, loss = 0.00101064
Iteration 873, loss = 0.00100929
Iteration 874, loss = 0.00100785
Iteration 875, loss = 0.00100910
Iteration 876, loss = 0.00100637
Iteration 877, loss = 0.00100282
Iteration 878, loss = 0.00100177
Iteration 879, loss = 0.00099980
Iteration 880, loss = 0.00099888
Iteration 881, loss = 0.00099641
Iteration 882, loss = 0.00099418
Iteration 883, loss = 0.00099263
Iteration 884, loss = 0.00099118
Iteration 885, loss = 0.00098979
Iteration 886, loss = 0.00098865
Iteration 887, loss = 0.00098845
Iteration 888, loss = 0.00098675
Iteration 889, loss = 0.00098797
Iteration 890, loss = 0.00099199
Iteration 891, loss = 0.00099463
Iteration 892, loss = 0.00099802
Iteration 893, loss = 0.00099739
Iteration 894, loss = 0.00099628
Iteration 895, loss = 0.00099548
Iteration 896, loss = 0.00099178
Iteration 897, loss = 0.00098616
Iteration 898, loss = 0.00097936
Iteration 899, loss = 0.00097296
Iteration 900, loss = 0.00097115
Iteration 901, loss = 0.00096599
Iteration 902, loss = 0.00097022
Iteration 903, loss = 0.00096531
Iteration 904, loss = 0.00096291
Iteration 905, loss = 0.00096150
Iteration 906, loss = 0.00096027
Iteration 907, loss = 0.00095782
Iteration 908, loss = 0.00095832
Iteration 909, loss = 0.00095568
Iteration 910, loss = 0.00095448
Iteration 911, loss = 0.00095384
Iteration 912, loss = 0.00095435
Iteration 913, loss = 0.00095021
Iteration 914, loss = 0.00095136
Iteration 915, loss = 0.00094775
Iteration 916, loss = 0.00094655
Iteration 917, loss = 0.00094522
Iteration 918, loss = 0.00094479
Iteration 919, loss = 0.00094478
Iteration 920, loss = 0.00094524
Iteration 921, loss = 0.00094816
Iteration 922, loss = 0.00095006
Iteration 923, loss = 0.00094884
Iteration 924, loss = 0.00094724
Iteration 925, loss = 0.00094529
Iteration 926, loss = 0.00094318
Iteration 927, loss = 0.00093984
Iteration 928, loss = 0.00093818
Iteration 929, loss = 0.00093467
Iteration 930, loss = 0.00093382
Iteration 931, loss = 0.00093161
Iteration 932, loss = 0.00092955
Iteration 933, loss = 0.00092794
Iteration 934, loss = 0.00092638
Iteration 935, loss = 0.00092375
Iteration 936, loss = 0.00092375
Iteration 937, loss = 0.00092017
Iteration 938, loss = 0.00091826
Iteration 939, loss = 0.00091676
Iteration 940, loss = 0.00091571
Iteration 941, loss = 0.00091849
Iteration 942, loss = 0.00091898
Iteration 943, loss = 0.00091425
Iteration 944, loss = 0.00091176
Iteration 945, loss = 0.00091038
Iteration 946, loss = 0.00090885
Iteration 947, loss = 0.00090773
Iteration 948, loss = 0.00090649
Iteration 949, loss = 0.00090555
Iteration 950, loss = 0.00090438
Iteration 951, loss = 0.00090667
Iteration 952, loss = 0.00090485
Iteration 953, loss = 0.00090405
Iteration 954, loss = 0.00090264
Iteration 955, loss = 0.00090140
Iteration 956, loss = 0.00090014
Iteration 957, loss = 0.00089961
Iteration 958, loss = 0.00089689
Iteration 959, loss = 0.00089478
Iteration 960, loss = 0.00089374
Iteration 961, loss = 0.00089284
Iteration 962, loss = 0.00089207
Iteration 963, loss = 0.00089111
Iteration 964, loss = 0.00089078
Iteration 965, loss = 0.00089006
Iteration 966, loss = 0.00088931
Iteration 967, loss = 0.00088886
Iteration 968, loss = 0.00088713
Iteration 969, loss = 0.00088616
Iteration 970, loss = 0.00088379
Iteration 971, loss = 0.00088193
Iteration 972, loss = 0.00088086
Iteration 973, loss = 0.00088045
Iteration 974, loss = 0.00087970
Iteration 975, loss = 0.00087788
Iteration 976, loss = 0.00087679
Iteration 977, loss = 0.00087573
Iteration 978, loss = 0.00087586
Iteration 979, loss = 0.00087557
Iteration 980, loss = 0.00087219
Iteration 981, loss = 0.00087129
Iteration 982, loss = 0.00087184
Iteration 983, loss = 0.00086792
Iteration 984, loss = 0.00086703
Iteration 985, loss = 0.00086791
Iteration 986, loss = 0.00086428
Iteration 987, loss = 0.00086284
Iteration 988, loss = 0.00086380
Iteration 989, loss = 0.00086278
Iteration 990, loss = 0.00086216
Iteration 991, loss = 0.00086331
Iteration 992, loss = 0.00086046
Iteration 993, loss = 0.00085768
Iteration 994, loss = 0.00085562
Iteration 995, loss = 0.00085424
Iteration 996, loss = 0.00085267
Iteration 997, loss = 0.00085497
Iteration 998, loss = 0.00084976
Iteration 999, loss = 0.00085076
Iteration 1000, loss = 0.00084835
PARAMETROS DE INICIALIZACAO DA REDE
NUMERO DE NEURONIOS 
Camada de Entrada: 34
Camada Escondida: 15
Camada de Saida: 1

--- PARAMETROS DE CONFIGURACAO DA REDE ---
Numero de Epocas: 1000
Taxa de Aprendizado: adaptive
Taxa de Aprendizado Inicial: 0.6
METRICAS

RESULTADOS:

[0 0 1 0 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 1 1 1 1 1
 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 1 1 1
 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 1 0]
ACURACIA: 0.9245283018867925

