Pesos Camada de Entrada: 
[[-6.26772353e-02 -6.59618378e-02  5.99899282e-02  1.02502059e-01
  -7.98988963e-02  7.29474625e-02  1.94638100e-01 -1.02205091e-01
  -1.91092016e-01  1.36771907e-01 -8.67624445e-02 -4.97013493e-02
   4.25515769e-03  5.42412802e-02 -3.78679077e-02]
 [-9.34297585e-03 -1.77532320e-01  1.73085847e-01  1.45190957e-01
   1.46790151e-01  1.72555447e-01  8.01391434e-02 -5.82561123e-02
  -5.14866883e-02  1.88815807e-01 -7.33968725e-02  1.73100775e-01
   4.45080158e-02  3.68159663e-02  7.29838160e-02]
 [-8.77255704e-02 -1.58886108e-01  1.33003255e-01 -4.38151213e-03
   1.86990148e-01  1.05452147e-02  5.97729579e-02 -1.75880326e-01
   1.45843463e-01  2.95173856e-03 -1.71376733e-01 -1.08598239e-01
   6.42643242e-02 -1.72860459e-01 -3.62970159e-02]
 [-8.00915422e-02 -1.59307217e-01  5.32273361e-02  4.72852927e-02
  -6.96580325e-02 -6.99953519e-02  1.49402389e-02  1.63045828e-01
  -1.57420105e-01 -1.69466053e-01 -8.22052892e-02 -1.21343777e-01
   1.42965854e-01  2.83358416e-02 -1.21220733e-01]
 [-1.45135175e-01 -1.69393886e-01  1.31266742e-01 -5.80133144e-02
  -1.48160207e-01  1.06902734e-02 -7.38692036e-02 -1.65573153e-01
  -1.25190150e-01  7.92956937e-02  5.24475976e-02 -1.79247973e-01
  -6.26241814e-02  1.43009047e-01  7.13729630e-02]
 [-7.47784663e-02  1.90490145e-01 -1.35920442e-01 -4.19377092e-02
   1.48146476e-01  6.93053636e-02  2.09864031e-02 -1.38539471e-01
   6.40341129e-02  8.83017281e-02  1.86473379e-01 -1.64052517e-02
  -9.05675484e-02  7.61006505e-02  1.52353653e-01]
 [-1.48334236e-01  1.48649388e-01  5.47964292e-02  8.51525622e-02
   1.69329919e-01 -2.31932260e-02 -9.87112211e-02 -5.09535065e-03
   6.95221511e-02 -7.84838545e-02 -1.54241660e-01 -9.13688163e-02
   8.92400112e-02 -1.73833761e-01 -3.09446287e-02]
 [ 2.01777761e-01  5.92070989e-02  1.41552963e-01  1.19775703e-01
  -3.43116574e-03  1.86919225e-01 -5.90095338e-02  1.62216463e-01
  -1.31370575e-01 -1.38794125e-01  8.43826160e-02 -4.85822999e-02
   1.59932724e-01  1.30085957e-01  7.90569498e-02]
 [-3.88918699e-02  6.96994199e-03 -6.62989863e-02 -1.80220761e-01
  -1.06025901e-01  1.71970111e-01 -3.79883969e-02  1.28408963e-01
   1.64990615e-01  3.06338326e-02 -2.46856414e-02 -1.20249481e-01
   9.17847689e-02  1.97769908e-01 -8.78183699e-02]
 [ 3.93519195e-02  1.72752051e-01 -9.05731696e-02 -2.49043385e-02
   4.98392673e-02 -1.57762485e-01  1.34299283e-01 -2.00007165e-01
  -1.53336019e-01 -1.69893963e-02 -7.28488511e-02  1.82530243e-01
  -1.41689850e-01  7.02896371e-02  1.36846611e-01]
 [ 1.69234317e-01  1.11594109e-01 -1.34849878e-01 -9.00449964e-02
   1.50841762e-01  6.80007329e-02  1.56202511e-01 -6.22344895e-02
  -1.18964769e-01  2.40000383e-02 -8.46703059e-02  1.89872587e-01
   5.97745369e-02  6.26868533e-03  9.31756983e-02]
 [ 3.70987473e-02 -5.17776233e-02 -1.13096309e-01  7.18173448e-02
  -8.92393630e-02 -1.42716533e-01 -3.42919068e-02 -4.15684506e-02
   1.94927725e-01 -5.26247693e-02 -1.36617504e-01 -5.81937227e-02
   1.12547684e-02  8.88724462e-02 -1.33889617e-01]
 [ 1.98508333e-01 -1.78371248e-01  2.51654563e-03  1.59908420e-01
  -2.20697896e-02  1.84426085e-01 -2.67548126e-03 -1.65698277e-01
  -1.65032825e-01  2.13725885e-02 -1.94902857e-01 -3.92281136e-02
   1.68355186e-01  7.08713065e-02 -8.23056111e-02]
 [ 1.68149011e-01  4.85022532e-02 -2.66136223e-02 -8.95447294e-03
   1.29203637e-01 -1.04259926e-01 -1.32759669e-01  1.24161597e-01
   1.79797640e-01 -3.66854810e-02  6.02695586e-02  3.84337366e-02
   1.38237929e-01  9.12132680e-02 -7.07446314e-02]
 [ 1.68001549e-01 -4.08352155e-02 -2.69521243e-02 -1.28997494e-01
   6.37523268e-02  5.55847881e-02  1.18887654e-01  1.23190263e-01
  -1.30648773e-02 -1.65377987e-01 -2.44885547e-02 -2.17523959e-02
  -1.29843645e-01  2.58745894e-02  1.95122252e-01]
 [ 1.93309211e-02 -9.18920288e-02 -1.66406569e-01 -9.94952647e-02
  -1.41623279e-01  1.68284250e-02  4.86329954e-02 -1.09715199e-01
  -6.17658287e-02  1.42205728e-01  1.62862010e-01  4.17389121e-02
   7.90019970e-02  1.08043414e-01  3.10635388e-02]
 [ 8.43855656e-02 -1.32036944e-01  1.38559696e-01 -1.28656691e-01
  -3.87586086e-02 -1.32544514e-02 -1.48138841e-01 -5.16974280e-02
  -9.92552419e-02 -4.79600883e-02  4.79377127e-02 -1.38990269e-01
  -1.87186683e-01 -4.61997128e-02 -4.18718136e-02]
 [ 5.71941641e-02 -8.15315987e-05  1.10737635e-01  2.36642380e-02
   3.07405201e-03 -8.64746139e-02  1.93556008e-01 -2.47721455e-02
  -1.18871097e-02  1.62676946e-01  7.43473966e-02  1.15214439e-01
   1.90604117e-01  1.65186753e-01 -2.01506742e-01]
 [-1.61980999e-01  1.39391188e-01 -1.64938990e-01  2.37414316e-02
  -1.22030966e-01 -1.43855095e-01 -1.78395974e-04  4.93860795e-02
  -8.69391023e-02 -1.51551495e-01  1.20751333e-01  8.86936912e-02
   1.99889978e-01 -8.34128659e-02 -1.16457033e-01]
 [-1.17438082e-01  9.98792309e-02 -7.55867453e-03 -9.29524205e-02
  -4.95960089e-02 -9.05565033e-02  9.24041931e-02  7.44069946e-02
   3.79221589e-04  1.84078717e-01 -8.56994556e-02  7.58635094e-02
  -1.31887675e-01 -9.61325170e-02 -1.61601607e-01]
 [-6.19359933e-02  1.26572108e-01 -1.39325046e-01 -9.02548468e-02
  -6.21738483e-02  4.57115957e-02  1.12261785e-01 -2.20638960e-04
   1.36787519e-01  1.34667630e-01  1.50864290e-01  1.43222570e-02
  -1.94569025e-01  1.73664127e-01  7.44229256e-02]
 [-1.48260768e-01 -5.04721446e-03  5.41376000e-02  5.23631588e-02
  -1.83213506e-01 -1.98082388e-02 -4.29637421e-02  1.25250912e-01
  -4.59623401e-02 -6.49594365e-02 -2.19213372e-02  1.28404896e-01
  -1.69845996e-01 -1.84286930e-01  1.03122698e-01]
 [-1.87011686e-01  1.53706802e-01  5.17060829e-03 -1.80325619e-02
   1.24425271e-01  1.64435929e-01 -3.00583017e-02 -1.92793686e-01
   1.98525749e-01  4.36049849e-02 -1.65525931e-01  1.09917712e-01
  -6.87436230e-02  1.70970352e-01 -1.17351412e-01]
 [ 1.99383267e-01 -4.81842030e-02  9.00535294e-02  1.86403895e-01
  -1.79391641e-01  2.01829674e-01  2.43226667e-02  4.35632454e-02
  -1.35651178e-01  1.58028432e-01  1.25591491e-01  7.72410442e-02
   9.56306294e-02  3.91996754e-02 -8.65451853e-02]
 [-1.83958076e-01  1.64033493e-01 -6.88992427e-02  1.06927704e-01
  -1.11284633e-01 -1.44756742e-01  1.30203547e-02  1.26667586e-01
  -1.69091550e-01 -1.45549500e-01 -2.97996790e-02 -4.65344984e-02
  -7.87841137e-02 -9.73670723e-03  1.09079579e-01]
 [ 1.67608748e-01 -1.13720129e-02 -1.20478423e-01  1.48949271e-01
   1.78346489e-01  1.48587923e-01  4.91823451e-02 -2.01751109e-01
  -1.33578448e-01  1.46056172e-02 -2.75084188e-02  3.58473708e-02
   7.35359795e-03  6.25082168e-02 -8.07445159e-02]
 [-1.11200499e-01 -1.07296217e-01 -1.85025081e-01  5.68588715e-03
   6.19581704e-02 -8.93103116e-02  1.93865230e-01  1.86570901e-01
   4.42213783e-02  1.36043667e-01  1.03737828e-01  7.01572380e-02
   6.74875308e-02 -5.52828346e-02 -1.07308343e-01]
 [-1.10363723e-01  9.20399160e-02 -8.15600310e-02 -7.64497604e-02
   1.13267981e-01 -1.56291919e-01  7.70059037e-02 -3.64836089e-02
   1.55481051e-01  6.10288118e-02 -1.85938557e-01 -4.10770910e-02
   5.61415633e-02 -2.29384530e-02  8.96934985e-02]
 [-1.85932410e-02  1.61095022e-01  1.88193944e-01  1.35927401e-01
   1.53091283e-01 -9.48873233e-02 -4.45067546e-02  1.14701509e-03
   1.13187412e-01  1.40618316e-01 -1.30971191e-02 -4.88903717e-04
  -1.32429332e-01 -1.32132469e-02 -1.03158677e-01]
 [ 1.66907170e-01 -4.51694666e-02  4.62632331e-02 -1.46745654e-01
  -8.36628220e-02 -8.00346589e-02 -1.67841988e-01 -1.12612879e-01
   1.87195762e-01  1.95515754e-01  1.14601571e-01 -8.62217869e-02
  -6.30509819e-03  1.72713131e-01 -1.25843574e-01]
 [-1.61369468e-01 -9.65842117e-02  6.24810179e-02 -8.66670912e-02
  -1.98302932e-01 -1.75178001e-01 -8.73408626e-02 -1.89379942e-01
   1.49133737e-01  5.17180706e-02  6.27882511e-02 -7.43012007e-02
  -1.60344967e-03  1.38304995e-01  1.23107602e-01]
 [ 3.17985467e-02  1.09369449e-01 -6.75668675e-02 -7.10998011e-02
   6.40996723e-03 -4.62615447e-02  1.73969696e-01 -1.00328447e-01
   9.40061354e-02  1.30684508e-01 -1.91049981e-01  7.89180242e-02
  -9.07592680e-02 -2.96917570e-02  1.20786417e-01]
 [ 1.48611487e-01  1.61360396e-02  1.68335817e-01 -4.43428464e-02
   6.82018353e-02 -5.63443733e-02 -1.04461591e-01 -1.75030130e-01
   7.71930936e-02  1.77876912e-01  1.82562098e-01 -7.94241280e-02
  -1.55403301e-01 -1.65165026e-01 -7.46292217e-02]
 [-4.64465002e-02  1.80378458e-01  6.04474935e-02 -9.04930529e-04
  -1.01977928e-01 -1.31029877e-02  7.10108955e-02 -1.12278411e-01
  -1.30280362e-01 -3.14867010e-02 -1.31939881e-01  1.94025393e-01
   3.25872986e-02  1.26143300e-01  2.66240864e-03]]
Bias Camada de Entrada: 
[ 0.04495734 -0.0391448  -0.19331788  0.01343573 -0.0967509   0.0951995
 -0.17900351 -0.09088055 -0.00824683  0.19440884  0.16050382  0.04772361
  0.01150928  0.02457447  0.17292761]
Pesos Camada Escondida: 
[[ 0.1201665 ]
 [ 0.29282137]
 [-0.23532434]
 [ 0.27674081]
 [-0.18821439]
 [-0.09402225]
 [-0.25212638]
 [-0.02722208]
 [ 0.10996737]
 [ 0.28012616]
 [ 0.16239523]
 [ 0.25883598]
 [-0.05362997]
 [-0.15166592]
 [ 0.29300765]]
Bias Camada Escondida: 
[0.14999826]
Iteration 1, loss = 0.66321321
Iteration 2, loss = 0.65741319
Iteration 3, loss = 0.64803018
Iteration 4, loss = 0.63496626
Iteration 5, loss = 0.62567254
Iteration 6, loss = 0.62000460
Iteration 7, loss = 0.59578454
Iteration 8, loss = 0.56176222
Iteration 9, loss = 0.53772532
Iteration 10, loss = 0.50817222
Iteration 11, loss = 0.48026866
Iteration 12, loss = 0.45340406
Iteration 13, loss = 0.42910596
Iteration 14, loss = 0.40548493
Iteration 15, loss = 0.38591308
Iteration 16, loss = 0.37083582
Iteration 17, loss = 0.37525928
Iteration 18, loss = 0.35368957
Iteration 19, loss = 0.32985727
Iteration 20, loss = 0.32899964
Iteration 21, loss = 0.32030024
Iteration 22, loss = 0.30560392
Iteration 23, loss = 0.29520827
Iteration 24, loss = 0.28929002
Iteration 25, loss = 0.28056020
Iteration 26, loss = 0.27126302
Iteration 27, loss = 0.26352995
Iteration 28, loss = 0.25685943
Iteration 29, loss = 0.25075258
Iteration 30, loss = 0.24722827
Iteration 31, loss = 0.24524131
Iteration 32, loss = 0.23278981
Iteration 33, loss = 0.22705357
Iteration 34, loss = 0.22203099
Iteration 35, loss = 0.21692128
Iteration 36, loss = 0.21154203
Iteration 37, loss = 0.20617603
Iteration 38, loss = 0.20177592
Iteration 39, loss = 0.19632425
Iteration 40, loss = 0.19258828
Iteration 41, loss = 0.18857585
Iteration 42, loss = 0.18487433
Iteration 43, loss = 0.18037116
Iteration 44, loss = 0.17691523
Iteration 45, loss = 0.17549020
Iteration 46, loss = 0.17475960
Iteration 47, loss = 0.16628930
Iteration 48, loss = 0.16242285
Iteration 49, loss = 0.16116531
Iteration 50, loss = 0.15847608
Iteration 51, loss = 0.15493743
Iteration 52, loss = 0.15126725
Iteration 53, loss = 0.14838898
Iteration 54, loss = 0.14756544
Iteration 55, loss = 0.14364849
Iteration 56, loss = 0.14130664
Iteration 57, loss = 0.14063706
Iteration 58, loss = 0.13713328
Iteration 59, loss = 0.13495421
Iteration 60, loss = 0.13215961
Iteration 61, loss = 0.13051012
Iteration 62, loss = 0.12835720
Iteration 63, loss = 0.12698588
Iteration 64, loss = 0.12481120
Iteration 65, loss = 0.12346853
Iteration 66, loss = 0.12282058
Iteration 67, loss = 0.12190321
Iteration 68, loss = 0.11909600
Iteration 69, loss = 0.11700661
Iteration 70, loss = 0.11525943
Iteration 71, loss = 0.11264957
Iteration 72, loss = 0.11083853
Iteration 73, loss = 0.10992145
Iteration 74, loss = 0.10935687
Iteration 75, loss = 0.11150810
Iteration 76, loss = 0.10881676
Iteration 77, loss = 0.10449100
Iteration 78, loss = 0.10344435
Iteration 79, loss = 0.10252631
Iteration 80, loss = 0.10072267
Iteration 81, loss = 0.10074195
Iteration 82, loss = 0.09828191
Iteration 83, loss = 0.09700161
Iteration 84, loss = 0.09609981
Iteration 85, loss = 0.09461478
Iteration 86, loss = 0.09373520
Iteration 87, loss = 0.09272769
Iteration 88, loss = 0.09207494
Iteration 89, loss = 0.09106618
Iteration 90, loss = 0.08953424
Iteration 91, loss = 0.08880468
Iteration 92, loss = 0.08966531
Iteration 93, loss = 0.08856510
Iteration 94, loss = 0.08627806
Iteration 95, loss = 0.08798233
Iteration 96, loss = 0.08534684
Iteration 97, loss = 0.08423843
Iteration 98, loss = 0.08259178
Iteration 99, loss = 0.08146441
Iteration 100, loss = 0.08112093
Iteration 101, loss = 0.07958569
Iteration 102, loss = 0.07884680
Iteration 103, loss = 0.07776048
Iteration 104, loss = 0.07801886
Iteration 105, loss = 0.07696490
Iteration 106, loss = 0.07770399
Iteration 107, loss = 0.07574813
Iteration 108, loss = 0.07400786
Iteration 109, loss = 0.07443204
Iteration 110, loss = 0.07340179
Iteration 111, loss = 0.07244450
Iteration 112, loss = 0.07103306
Iteration 113, loss = 0.07068633
Iteration 114, loss = 0.07104483
Iteration 115, loss = 0.07004925
Iteration 116, loss = 0.06983309
Iteration 117, loss = 0.06986293
Iteration 118, loss = 0.06914853
Iteration 119, loss = 0.06878150
Iteration 120, loss = 0.06789723
Iteration 121, loss = 0.06622818
Iteration 122, loss = 0.06495744
Iteration 123, loss = 0.06373511
Iteration 124, loss = 0.06408120
Iteration 125, loss = 0.06318118
Iteration 126, loss = 0.06224396
Iteration 127, loss = 0.06153181
Iteration 128, loss = 0.06104790
Iteration 129, loss = 0.06072860
Iteration 130, loss = 0.05982092
Iteration 131, loss = 0.05939915
Iteration 132, loss = 0.05897307
Iteration 133, loss = 0.05776187
Iteration 134, loss = 0.05711454
Iteration 135, loss = 0.05692319
Iteration 136, loss = 0.05627873
Iteration 137, loss = 0.05577449
Iteration 138, loss = 0.05543343
Iteration 139, loss = 0.05457751
Iteration 140, loss = 0.05410998
Iteration 141, loss = 0.05364048
Iteration 142, loss = 0.05282727
Iteration 143, loss = 0.05226490
Iteration 144, loss = 0.05329604
Iteration 145, loss = 0.05126096
Iteration 146, loss = 0.05116895
Iteration 147, loss = 0.05043590
Iteration 148, loss = 0.05107006
Iteration 149, loss = 0.05060696
Iteration 150, loss = 0.04996231
Iteration 151, loss = 0.04906209
Iteration 152, loss = 0.04872855
Iteration 153, loss = 0.04893133
Iteration 154, loss = 0.04921662
Iteration 155, loss = 0.04684261
Iteration 156, loss = 0.04744048
Iteration 157, loss = 0.04776745
Iteration 158, loss = 0.04768164
Iteration 159, loss = 0.04733815
Iteration 160, loss = 0.04690878
Iteration 161, loss = 0.04605178
Iteration 162, loss = 0.04585728
Iteration 163, loss = 0.04449914
Iteration 164, loss = 0.04690897
Iteration 165, loss = 0.04837571
Iteration 166, loss = 0.04647675
Iteration 167, loss = 0.04335996
Iteration 168, loss = 0.04163953
Iteration 169, loss = 0.04155094
Iteration 170, loss = 0.04253947
Iteration 171, loss = 0.04142784
Iteration 172, loss = 0.03956568
Iteration 173, loss = 0.04053717
Iteration 174, loss = 0.04252557
Iteration 175, loss = 0.04323396
Iteration 176, loss = 0.04200636
Iteration 177, loss = 0.03965582
Iteration 178, loss = 0.03831370
Iteration 179, loss = 0.03775937
Iteration 180, loss = 0.03790974
Iteration 181, loss = 0.03714346
Iteration 182, loss = 0.03699468
Iteration 183, loss = 0.03667802
Iteration 184, loss = 0.03639027
Iteration 185, loss = 0.03624887
Iteration 186, loss = 0.03580614
Iteration 187, loss = 0.03541438
Iteration 188, loss = 0.03523780
Iteration 189, loss = 0.03534918
Iteration 190, loss = 0.03495730
Iteration 191, loss = 0.03445587
Iteration 192, loss = 0.03422431
Iteration 193, loss = 0.03449899
Iteration 194, loss = 0.03450522
Iteration 195, loss = 0.03376471
Iteration 196, loss = 0.03304224
Iteration 197, loss = 0.03283290
Iteration 198, loss = 0.03287914
Iteration 199, loss = 0.03278003
Iteration 200, loss = 0.03283377
Iteration 201, loss = 0.03213415
Iteration 202, loss = 0.03237701
Iteration 203, loss = 0.03265641
Iteration 204, loss = 0.03227758
Iteration 205, loss = 0.03102872
Iteration 206, loss = 0.03026962
Iteration 207, loss = 0.03062123
Iteration 208, loss = 0.03034647
Iteration 209, loss = 0.02990650
Iteration 210, loss = 0.02951470
Iteration 211, loss = 0.02925439
Iteration 212, loss = 0.02901912
Iteration 213, loss = 0.02879665
Iteration 214, loss = 0.02854532
Iteration 215, loss = 0.02839356
Iteration 216, loss = 0.02825461
Iteration 217, loss = 0.02824976
Iteration 218, loss = 0.02805938
Iteration 219, loss = 0.02731945
Iteration 220, loss = 0.02834604
Iteration 221, loss = 0.02932080
Iteration 222, loss = 0.02686862
Iteration 223, loss = 0.02784753
Iteration 224, loss = 0.02908940
Iteration 225, loss = 0.02801050
Iteration 226, loss = 0.02662506
Iteration 227, loss = 0.02617060
Iteration 228, loss = 0.02571575
Iteration 229, loss = 0.02558172
Iteration 230, loss = 0.02524321
Iteration 231, loss = 0.02522505
Iteration 232, loss = 0.02548748
Iteration 233, loss = 0.02505248
Iteration 234, loss = 0.02448891
Iteration 235, loss = 0.02419903
Iteration 236, loss = 0.02415420
Iteration 237, loss = 0.02398160
Iteration 238, loss = 0.02404091
Iteration 239, loss = 0.02405887
Iteration 240, loss = 0.02411234
Iteration 241, loss = 0.02331270
Iteration 242, loss = 0.02279401
Iteration 243, loss = 0.02273694
Iteration 244, loss = 0.02280380
Iteration 245, loss = 0.02307492
Iteration 246, loss = 0.02248044
Iteration 247, loss = 0.02208052
Iteration 248, loss = 0.02204070
Iteration 249, loss = 0.02221667
Iteration 250, loss = 0.02161294
Iteration 251, loss = 0.02171790
Iteration 252, loss = 0.02158545
Iteration 253, loss = 0.02111205
Iteration 254, loss = 0.02074692
Iteration 255, loss = 0.02088220
Iteration 256, loss = 0.02073731
Iteration 257, loss = 0.02060986
Iteration 258, loss = 0.02033726
Iteration 259, loss = 0.02033359
Iteration 260, loss = 0.02061709
Iteration 261, loss = 0.02030155
Iteration 262, loss = 0.01975220
Iteration 263, loss = 0.01973038
Iteration 264, loss = 0.01965063
Iteration 265, loss = 0.01926524
Iteration 266, loss = 0.01969739
Iteration 267, loss = 0.01945068
Iteration 268, loss = 0.01898396
Iteration 269, loss = 0.01876054
Iteration 270, loss = 0.01855262
Iteration 271, loss = 0.01842982
Iteration 272, loss = 0.01839827
Iteration 273, loss = 0.01816739
Iteration 274, loss = 0.01792662
Iteration 275, loss = 0.01780408
Iteration 276, loss = 0.01778254
Iteration 277, loss = 0.01761534
Iteration 278, loss = 0.01745167
Iteration 279, loss = 0.01735228
Iteration 280, loss = 0.01722387
Iteration 281, loss = 0.01710756
Iteration 282, loss = 0.01703689
Iteration 283, loss = 0.01702036
Iteration 284, loss = 0.01687944
Iteration 285, loss = 0.01686597
Iteration 286, loss = 0.01696891
Iteration 287, loss = 0.01682768
Iteration 288, loss = 0.01665569
Iteration 289, loss = 0.01649858
Iteration 290, loss = 0.01628565
Iteration 291, loss = 0.01615720
Iteration 292, loss = 0.01627610
Iteration 293, loss = 0.01619275
Iteration 294, loss = 0.01620883
Iteration 295, loss = 0.01586044
Iteration 296, loss = 0.01555291
Iteration 297, loss = 0.01552349
Iteration 298, loss = 0.01551758
Iteration 299, loss = 0.01562217
Iteration 300, loss = 0.01514113
Iteration 301, loss = 0.01529568
Iteration 302, loss = 0.01555267
Iteration 303, loss = 0.01543969
Iteration 304, loss = 0.01509784
Iteration 305, loss = 0.01481634
Iteration 306, loss = 0.01457703
Iteration 307, loss = 0.01452167
Iteration 308, loss = 0.01449333
Iteration 309, loss = 0.01450009
Iteration 310, loss = 0.01450483
Iteration 311, loss = 0.01446094
Iteration 312, loss = 0.01432781
Iteration 313, loss = 0.01422654
Iteration 314, loss = 0.01405358
Iteration 315, loss = 0.01396820
Iteration 316, loss = 0.01388081
Iteration 317, loss = 0.01382485
Iteration 318, loss = 0.01371815
Iteration 319, loss = 0.01359412
Iteration 320, loss = 0.01376220
Iteration 321, loss = 0.01351034
Iteration 322, loss = 0.01334852
Iteration 323, loss = 0.01331120
Iteration 324, loss = 0.01351703
Iteration 325, loss = 0.01319722
Iteration 326, loss = 0.01314072
Iteration 327, loss = 0.01313073
Iteration 328, loss = 0.01316664
Iteration 329, loss = 0.01305336
Iteration 330, loss = 0.01285079
Iteration 331, loss = 0.01271975
Iteration 332, loss = 0.01273283
Iteration 333, loss = 0.01266927
Iteration 334, loss = 0.01258511
Iteration 335, loss = 0.01248822
Iteration 336, loss = 0.01244098
Iteration 337, loss = 0.01237302
Iteration 338, loss = 0.01240496
Iteration 339, loss = 0.01231453
Iteration 340, loss = 0.01222178
Iteration 341, loss = 0.01213689
Iteration 342, loss = 0.01205144
Iteration 343, loss = 0.01193841
Iteration 344, loss = 0.01185562
Iteration 345, loss = 0.01177639
Iteration 346, loss = 0.01182004
Iteration 347, loss = 0.01171778
Iteration 348, loss = 0.01165394
Iteration 349, loss = 0.01159319
Iteration 350, loss = 0.01152678
Iteration 351, loss = 0.01147399
Iteration 352, loss = 0.01141604
Iteration 353, loss = 0.01138871
Iteration 354, loss = 0.01133898
Iteration 355, loss = 0.01130773
Iteration 356, loss = 0.01119357
Iteration 357, loss = 0.01111150
Iteration 358, loss = 0.01109178
Iteration 359, loss = 0.01101044
Iteration 360, loss = 0.01097328
Iteration 361, loss = 0.01108327
Iteration 362, loss = 0.01121182
Iteration 363, loss = 0.01083315
Iteration 364, loss = 0.01070785
Iteration 365, loss = 0.01081493
Iteration 366, loss = 0.01077878
Iteration 367, loss = 0.01072367
Iteration 368, loss = 0.01060501
Iteration 369, loss = 0.01047701
Iteration 370, loss = 0.01038609
Iteration 371, loss = 0.01042899
Iteration 372, loss = 0.01044867
Iteration 373, loss = 0.01024376
Iteration 374, loss = 0.01020307
Iteration 375, loss = 0.01036391
Iteration 376, loss = 0.01089226
Iteration 377, loss = 0.01078847
Iteration 378, loss = 0.01044542
Iteration 379, loss = 0.01012210
Iteration 380, loss = 0.01005256
Iteration 381, loss = 0.00987876
Iteration 382, loss = 0.00980569
Iteration 383, loss = 0.00982594
Iteration 384, loss = 0.00975284
Iteration 385, loss = 0.00971498
Iteration 386, loss = 0.00971055
Iteration 387, loss = 0.00964142
Iteration 388, loss = 0.00958819
Iteration 389, loss = 0.00955424
Iteration 390, loss = 0.00946644
Iteration 391, loss = 0.00943307
Iteration 392, loss = 0.00948064
Iteration 393, loss = 0.00950965
Iteration 394, loss = 0.00943924
Iteration 395, loss = 0.00935160
Iteration 396, loss = 0.00926523
Iteration 397, loss = 0.00920641
Iteration 398, loss = 0.00922435
Iteration 399, loss = 0.00929692
Iteration 400, loss = 0.00930808
Iteration 401, loss = 0.00920172
Iteration 402, loss = 0.00904865
Iteration 403, loss = 0.00891838
Iteration 404, loss = 0.00889916
Iteration 405, loss = 0.00889450
Iteration 406, loss = 0.00889731
Iteration 407, loss = 0.00888406
Iteration 408, loss = 0.00880900
Iteration 409, loss = 0.00877291
Iteration 410, loss = 0.00873374
Iteration 411, loss = 0.00865872
Iteration 412, loss = 0.00860762
Iteration 413, loss = 0.00854501
Iteration 414, loss = 0.00850752
Iteration 415, loss = 0.00846625
Iteration 416, loss = 0.00843877
Iteration 417, loss = 0.00839461
Iteration 418, loss = 0.00837875
Iteration 419, loss = 0.00835312
Iteration 420, loss = 0.00833851
Iteration 421, loss = 0.00835005
Iteration 422, loss = 0.00829140
Iteration 423, loss = 0.00821103
Iteration 424, loss = 0.00818063
Iteration 425, loss = 0.00813416
Iteration 426, loss = 0.00811434
Iteration 427, loss = 0.00812636
Iteration 428, loss = 0.00813122
Iteration 429, loss = 0.00811459
Iteration 430, loss = 0.00807386
Iteration 431, loss = 0.00803579
Iteration 432, loss = 0.00799518
Iteration 433, loss = 0.00795268
Iteration 434, loss = 0.00795919
Iteration 435, loss = 0.00792427
Iteration 436, loss = 0.00792079
Iteration 437, loss = 0.00785100
Iteration 438, loss = 0.00780745
Iteration 439, loss = 0.00778177
Iteration 440, loss = 0.00778312
Iteration 441, loss = 0.00783102
Iteration 442, loss = 0.00795539
Iteration 443, loss = 0.00796892
Iteration 444, loss = 0.00789927
Iteration 445, loss = 0.00780493
Iteration 446, loss = 0.00761688
Iteration 447, loss = 0.00750843
Iteration 448, loss = 0.00748005
Iteration 449, loss = 0.00750744
Iteration 450, loss = 0.00751346
Iteration 451, loss = 0.00749428
Iteration 452, loss = 0.00745272
Iteration 453, loss = 0.00742614
Iteration 454, loss = 0.00737874
Iteration 455, loss = 0.00731637
Iteration 456, loss = 0.00729252
Iteration 457, loss = 0.00723247
Iteration 458, loss = 0.00718962
Iteration 459, loss = 0.00716089
Iteration 460, loss = 0.00716151
Iteration 461, loss = 0.00713301
Iteration 462, loss = 0.00711321
Iteration 463, loss = 0.00708542
Iteration 464, loss = 0.00703823
Iteration 465, loss = 0.00698351
Iteration 466, loss = 0.00698968
Iteration 467, loss = 0.00696103
Iteration 468, loss = 0.00695748
Iteration 469, loss = 0.00695307
Iteration 470, loss = 0.00694943
Iteration 471, loss = 0.00695025
Iteration 472, loss = 0.00690851
Iteration 473, loss = 0.00682774
Iteration 474, loss = 0.00677935
Iteration 475, loss = 0.00673956
Iteration 476, loss = 0.00673232
Iteration 477, loss = 0.00677588
Iteration 478, loss = 0.00679295
Iteration 479, loss = 0.00674815
Iteration 480, loss = 0.00668600
Iteration 481, loss = 0.00664154
Iteration 482, loss = 0.00663435
Iteration 483, loss = 0.00666000
Iteration 484, loss = 0.00664587
Iteration 485, loss = 0.00667140
Iteration 486, loss = 0.00661272
Iteration 487, loss = 0.00652703
Iteration 488, loss = 0.00643747
Iteration 489, loss = 0.00637075
Iteration 490, loss = 0.00642550
Iteration 491, loss = 0.00639701
Iteration 492, loss = 0.00635934
Iteration 493, loss = 0.00629897
Iteration 494, loss = 0.00628574
Iteration 495, loss = 0.00624555
Iteration 496, loss = 0.00622030
Iteration 497, loss = 0.00619706
Iteration 498, loss = 0.00617395
Iteration 499, loss = 0.00615943
Iteration 500, loss = 0.00614350
Iteration 501, loss = 0.00614201
Iteration 502, loss = 0.00615457
Iteration 503, loss = 0.00617684
Iteration 504, loss = 0.00617491
Iteration 505, loss = 0.00613891
Iteration 506, loss = 0.00614539
Iteration 507, loss = 0.00607112
Iteration 508, loss = 0.00606535
Iteration 509, loss = 0.00606471
Iteration 510, loss = 0.00605984
Iteration 511, loss = 0.00604847
Iteration 512, loss = 0.00603728
Iteration 513, loss = 0.00597623
Iteration 514, loss = 0.00591029
Iteration 515, loss = 0.00591036
Iteration 516, loss = 0.00588496
Iteration 517, loss = 0.00585315
Iteration 518, loss = 0.00581807
Iteration 519, loss = 0.00580745
Iteration 520, loss = 0.00576093
Iteration 521, loss = 0.00575110
Iteration 522, loss = 0.00574699
Iteration 523, loss = 0.00577385
Iteration 524, loss = 0.00578324
Iteration 525, loss = 0.00571870
Iteration 526, loss = 0.00566344
Iteration 527, loss = 0.00563148
Iteration 528, loss = 0.00560899
Iteration 529, loss = 0.00559954
Iteration 530, loss = 0.00559911
Iteration 531, loss = 0.00560294
Iteration 532, loss = 0.00555610
Iteration 533, loss = 0.00553028
Iteration 534, loss = 0.00550964
Iteration 535, loss = 0.00549725
Iteration 536, loss = 0.00547382
Iteration 537, loss = 0.00546704
Iteration 538, loss = 0.00544617
Iteration 539, loss = 0.00545596
Iteration 540, loss = 0.00545810
Iteration 541, loss = 0.00542158
Iteration 542, loss = 0.00539608
Iteration 543, loss = 0.00537642
Iteration 544, loss = 0.00535199
Iteration 545, loss = 0.00533889
Iteration 546, loss = 0.00531855
Iteration 547, loss = 0.00531902
Iteration 548, loss = 0.00529678
Iteration 549, loss = 0.00528782
Iteration 550, loss = 0.00526843
Iteration 551, loss = 0.00528394
Iteration 552, loss = 0.00527902
Iteration 553, loss = 0.00524786
Iteration 554, loss = 0.00523978
Iteration 555, loss = 0.00522381
Iteration 556, loss = 0.00520657
Iteration 557, loss = 0.00520270
Iteration 558, loss = 0.00517318
Iteration 559, loss = 0.00513469
Iteration 560, loss = 0.00511292
Iteration 561, loss = 0.00509869
Iteration 562, loss = 0.00508612
Iteration 563, loss = 0.00507400
Iteration 564, loss = 0.00507087
Iteration 565, loss = 0.00507528
Iteration 566, loss = 0.00502524
Iteration 567, loss = 0.00500255
Iteration 568, loss = 0.00499185
Iteration 569, loss = 0.00498725
Iteration 570, loss = 0.00496191
Iteration 571, loss = 0.00493459
Iteration 572, loss = 0.00492414
Iteration 573, loss = 0.00493844
Iteration 574, loss = 0.00493097
Iteration 575, loss = 0.00491739
Iteration 576, loss = 0.00490221
Iteration 577, loss = 0.00488155
Iteration 578, loss = 0.00486038
Iteration 579, loss = 0.00482772
Iteration 580, loss = 0.00480921
Iteration 581, loss = 0.00483645
Iteration 582, loss = 0.00478613
Iteration 583, loss = 0.00477089
Iteration 584, loss = 0.00479160
Iteration 585, loss = 0.00475294
Iteration 586, loss = 0.00473626
Iteration 587, loss = 0.00472870
Iteration 588, loss = 0.00471850
Iteration 589, loss = 0.00471444
Iteration 590, loss = 0.00469084
Iteration 591, loss = 0.00467702
Iteration 592, loss = 0.00466764
Iteration 593, loss = 0.00466334
Iteration 594, loss = 0.00463536
Iteration 595, loss = 0.00461973
Iteration 596, loss = 0.00462414
Iteration 597, loss = 0.00460868
Iteration 598, loss = 0.00459871
Iteration 599, loss = 0.00458560
Iteration 600, loss = 0.00457860
Iteration 601, loss = 0.00456878
Iteration 602, loss = 0.00455557
Iteration 603, loss = 0.00454584
Iteration 604, loss = 0.00453697
Iteration 605, loss = 0.00452493
Iteration 606, loss = 0.00450284
Iteration 607, loss = 0.00449267
Iteration 608, loss = 0.00448330
Iteration 609, loss = 0.00445544
Iteration 610, loss = 0.00444954
Iteration 611, loss = 0.00445948
Iteration 612, loss = 0.00445264
Iteration 613, loss = 0.00441196
Iteration 614, loss = 0.00439420
Iteration 615, loss = 0.00440961
Iteration 616, loss = 0.00437203
Iteration 617, loss = 0.00435612
Iteration 618, loss = 0.00434227
Iteration 619, loss = 0.00433938
Iteration 620, loss = 0.00435436
Iteration 621, loss = 0.00435386
Iteration 622, loss = 0.00432651
Iteration 623, loss = 0.00430237
Iteration 624, loss = 0.00427741
Iteration 625, loss = 0.00425179
Iteration 626, loss = 0.00424486
Iteration 627, loss = 0.00423428
Iteration 628, loss = 0.00423188
Iteration 629, loss = 0.00422684
Iteration 630, loss = 0.00423087
Iteration 631, loss = 0.00419853
Iteration 632, loss = 0.00418210
Iteration 633, loss = 0.00417302
Iteration 634, loss = 0.00416246
Iteration 635, loss = 0.00416196
Iteration 636, loss = 0.00416294
Iteration 637, loss = 0.00414910
Iteration 638, loss = 0.00413732
Iteration 639, loss = 0.00412567
Iteration 640, loss = 0.00414601
Iteration 641, loss = 0.00410063
Iteration 642, loss = 0.00408561
Iteration 643, loss = 0.00406938
Iteration 644, loss = 0.00408710
Iteration 645, loss = 0.00412333
Iteration 646, loss = 0.00417104
Iteration 647, loss = 0.00419299
Iteration 648, loss = 0.00416952
Iteration 649, loss = 0.00411025
Iteration 650, loss = 0.00404428
Iteration 651, loss = 0.00400851
Iteration 652, loss = 0.00397808
Iteration 653, loss = 0.00396925
Iteration 654, loss = 0.00399319
Iteration 655, loss = 0.00399561
Iteration 656, loss = 0.00400617
Iteration 657, loss = 0.00402985
Iteration 658, loss = 0.00401397
Iteration 659, loss = 0.00397908
Iteration 660, loss = 0.00394258
Iteration 661, loss = 0.00390896
Iteration 662, loss = 0.00389668
Iteration 663, loss = 0.00388579
Iteration 664, loss = 0.00388246
Iteration 665, loss = 0.00387218
Iteration 666, loss = 0.00385884
Iteration 667, loss = 0.00384395
Iteration 668, loss = 0.00383310
Iteration 669, loss = 0.00382067
Iteration 670, loss = 0.00381045
Iteration 671, loss = 0.00380553
Iteration 672, loss = 0.00379683
Iteration 673, loss = 0.00378691
Iteration 674, loss = 0.00378371
Iteration 675, loss = 0.00377073
Iteration 676, loss = 0.00376345
Iteration 677, loss = 0.00375867
Iteration 678, loss = 0.00374786
Iteration 679, loss = 0.00373444
Iteration 680, loss = 0.00372346
Iteration 681, loss = 0.00373016
Iteration 682, loss = 0.00371460
Iteration 683, loss = 0.00370726
Iteration 684, loss = 0.00369737
Iteration 685, loss = 0.00368085
Iteration 686, loss = 0.00367557
Iteration 687, loss = 0.00367353
Iteration 688, loss = 0.00367806
Iteration 689, loss = 0.00368650
Iteration 690, loss = 0.00370113
Iteration 691, loss = 0.00372729
Iteration 692, loss = 0.00374291
Iteration 693, loss = 0.00370285
Iteration 694, loss = 0.00363335
Iteration 695, loss = 0.00359149
Iteration 696, loss = 0.00360556
Iteration 697, loss = 0.00363510
Iteration 698, loss = 0.00369362
Iteration 699, loss = 0.00368496
Iteration 700, loss = 0.00365964
PARAMETROS DE INICIALIZACAO DA REDE
NUMERO DE NEURONIOS 
Camada de Entrada: 34
Camada Escondida: 15
Camada de Saida: 1

--- PARAMETROS DE CONFIGURACAO DA REDE ---
Numero de Epocas: 700
Taxa de Aprendizado: adaptive
Taxa de Aprendizado Inicial: 0.3
METRICAS

RESULTADOS:

[0 0 1 0 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 1 1 1 1 1
 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 1 1 1
 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 1 0]
ACURACIA: 0.9339622641509434

