Pesos Camada de Entrada: 
[[-0.16531256 -0.08216068  0.02363383 -0.06130201 -0.09239363  0.10708764
  -0.09923348 -0.02157697 -0.02628432 -0.04119209  0.15490075  0.12997431
  -0.08852353 -0.16422225 -0.157507  ]
 [-0.10330143 -0.17994609  0.00264759  0.11530183  0.19250702  0.06854794
  -0.18253926  0.03603795 -0.14864748 -0.12703303  0.14394548  0.08686945
  -0.00362568 -0.16889373  0.11194173]
 [ 0.07225723  0.19985341 -0.20026047 -0.00042195  0.02591958 -0.13241082
  -0.01099756 -0.03037361  0.19810092 -0.16121759  0.15305968 -0.10482676
  -0.1327806   0.14066876 -0.05115716]
 [-0.02308608 -0.09235116  0.12778272  0.16882469 -0.02813548  0.01716917
  -0.12186283 -0.03496335  0.05510321  0.04153456 -0.09212316 -0.04531629
  -0.18369085 -0.06961084 -0.06012805]
 [ 0.13961482 -0.17959143  0.20106059 -0.00616503 -0.02300119 -0.01570569
   0.09035504  0.15222533 -0.18302669 -0.0339615  -0.11267378 -0.18523044
  -0.12335961 -0.01173062 -0.16739541]
 [ 0.0579817  -0.11960014 -0.11542641 -0.00697818  0.17575773  0.03666872
   0.02749976 -0.19274711 -0.06506059  0.1069968   0.003247    0.02838997
  -0.00106858 -0.09242239 -0.19584743]
 [ 0.10695418  0.16657366 -0.07915499 -0.01318303  0.08320989  0.09882013
   0.15124742  0.09511558 -0.15396391  0.12533273  0.04664797 -0.06700128
   0.07549356 -0.14090389 -0.14603071]
 [ 0.02070629  0.16257018  0.03029939  0.12777989 -0.04391692 -0.17632473
   0.14997359  0.13293559  0.20091542  0.04613035 -0.17533632  0.19292008
   0.11504396  0.02206728  0.06664442]
 [ 0.17196196  0.0859014   0.18834114  0.1105066   0.06732305  0.00129523
   0.15909579 -0.09483288  0.02714492 -0.09396969 -0.09906873 -0.11424666
  -0.1205857  -0.14505456  0.19305544]
 [-0.09111256 -0.0017193   0.15893442  0.04379186 -0.18909769  0.09873668
   0.1528258   0.191606    0.07530103  0.07327982  0.08870537  0.15280775
  -0.13786252 -0.11569974 -0.14099525]
 [ 0.11570393 -0.14961952  0.00376901 -0.17188153  0.09653516 -0.05702114
  -0.15423389 -0.06558825  0.04119123 -0.13427284 -0.10668175  0.14748173
  -0.15223451 -0.17675254 -0.18195784]
 [-0.06602973 -0.00222351 -0.12677052  0.13186045  0.04204687  0.10734878
   0.17475681 -0.09975905 -0.03383728  0.14544809  0.06288074 -0.00611995
   0.02142778  0.13290458  0.07695219]
 [ 0.1162506   0.08382049  0.1106318   0.1569712  -0.14927046  0.20078922
  -0.06748103  0.03418024 -0.01912254  0.04665702  0.18465433  0.05802835
   0.15366099 -0.04182952 -0.02998679]
 [-0.12470974  0.03226687 -0.02217493  0.03271194  0.09397035 -0.16629756
  -0.15582605 -0.15066906 -0.06624854 -0.15410198  0.17890964  0.1295976
   0.07533567 -0.01238305 -0.16109013]
 [ 0.01349371  0.1311994  -0.12217288  0.05521614 -0.10281461 -0.12534115
   0.19068456 -0.12617169  0.05046237 -0.0264375  -0.15700644 -0.06344703
   0.08357587 -0.05779331 -0.06924596]
 [ 0.00838185 -0.07946165  0.18852378 -0.05007327  0.1471536   0.1678816
   0.09210546  0.1708639   0.10227667 -0.12306278 -0.0466516   0.09917768
  -0.07857997 -0.05045498 -0.152334  ]
 [-0.05635377 -0.15754447  0.00611914 -0.05627263  0.07206101 -0.19938574
  -0.16246     0.01945494  0.04894795  0.16721608 -0.0892691  -0.128868
   0.10859269 -0.18691391 -0.14701777]
 [ 0.05487266  0.10174093 -0.18992468 -0.0864938   0.08111847 -0.06250166
  -0.0039198  -0.15417305  0.12494085  0.07919873 -0.11393426 -0.12358696
   0.14642594  0.18155811 -0.16336752]
 [ 0.03068395 -0.12425429 -0.10862254  0.10027208  0.01040032 -0.11966993
  -0.0066547   0.07911052 -0.19844976 -0.09671331 -0.04712319  0.06795323
   0.19747981  0.02687867  0.01480639]
 [-0.04604161 -0.17465097  0.04208222  0.09108119  0.00867505  0.16018967
   0.18534512  0.20047101 -0.11905279  0.0582067  -0.06572249 -0.14894263
  -0.12817185 -0.12686878  0.00251527]
 [-0.13702084  0.05131526 -0.05116464  0.17506218 -0.1507847   0.14486458
   0.0821332   0.14484288  0.04546454 -0.01373025 -0.02249627  0.0602899
  -0.13250168 -0.11720037  0.08643607]
 [-0.02367818 -0.19671054 -0.135781   -0.15729067 -0.16158101 -0.04348484
   0.14578452  0.08350021 -0.04406935 -0.02650373  0.10946711 -0.08333065
   0.13816659  0.03070698 -0.15353756]
 [ 0.11544194  0.06244268  0.03095348  0.01112083 -0.12038434 -0.02347044
  -0.12412542 -0.17800233  0.19930817 -0.01193366  0.14062261  0.03808732
  -0.08226498 -0.06168417  0.03756475]
 [-0.01150206 -0.05140376 -0.00788634 -0.01950474  0.00140974  0.08800908
   0.03063816 -0.16719536  0.09852444 -0.103488    0.13902432 -0.16453921
   0.14434909  0.14290741  0.04819739]
 [ 0.10976987  0.17824301 -0.05903315  0.05511242  0.11415843  0.19625464
   0.16301635 -0.05108646  0.15710895  0.12805104 -0.06445276 -0.19717076
   0.11990946  0.10170153 -0.14211126]
 [-0.02723619  0.00322244 -0.06636192  0.08730573  0.04486083 -0.16217309
  -0.018216   -0.07201856  0.20114636  0.16374925 -0.07036451 -0.03944519
  -0.0979131   0.01131369  0.01840579]
 [ 0.08885739 -0.04334908  0.0670989   0.11008803 -0.01132182 -0.10804828
  -0.02715088  0.11326478  0.08245229 -0.0216524   0.09298199 -0.10152807
  -0.10654657  0.12934815 -0.16480342]
 [-0.18031687  0.1821073  -0.09334725 -0.14030264  0.03858566  0.17116554
   0.06137776 -0.04909939  0.13871135 -0.1880336   0.04328618 -0.07298216
   0.10751376 -0.04603235  0.11046496]
 [ 0.0717454  -0.14699862 -0.12713059  0.04475646  0.00409703  0.00534995
  -0.12133992 -0.0146241   0.15672948  0.04815766 -0.19700689 -0.17403994
  -0.1046646   0.07703629  0.17951391]
 [ 0.01320326 -0.15569402  0.02032768  0.17292384 -0.15156686 -0.03629359
  -0.00346818 -0.06459358  0.13186227 -0.20130217 -0.02016325  0.17879154
   0.04491646  0.17853297 -0.1999511 ]
 [-0.11998272 -0.12367572 -0.05642041 -0.09081288 -0.04336814  0.01985109
   0.05971084  0.05676001 -0.02655636 -0.12207549 -0.07385865  0.07715918
  -0.11507394  0.11492076  0.04103964]
 [ 0.19533825  0.07076329 -0.08101704 -0.17634267 -0.07589129 -0.17987995
  -0.16252701 -0.02788641 -0.07977927  0.19856799  0.20144101  0.16379178
   0.08612513  0.09522699  0.16510007]
 [ 0.14589688 -0.17330993 -0.11149506  0.02308164 -0.10409758  0.19682328
   0.04631976 -0.12497926 -0.15740264 -0.12814064  0.1244625   0.09862261
  -0.1554326  -0.06434581 -0.15791881]
 [-0.19096715  0.13830554 -0.07733579 -0.05376586 -0.05780827 -0.00488929
  -0.07759455  0.13284152 -0.17501162 -0.17349048  0.19278845  0.17401427
  -0.01007445  0.07012778  0.15546492]]
Bias Camada de Entrada: 
[ 0.09508654 -0.19002809  0.02103306 -0.1454325  -0.03985506 -0.13095422
  0.11867433 -0.19470446  0.18274864 -0.03673925  0.17186895 -0.01144738
 -0.07431038  0.09105918 -0.09843412]
Pesos Camada Escondida: 
[[ 0.27829879]
 [ 0.34604511]
 [-0.14784603]
 [-0.18867446]
 [ 0.1673898 ]
 [-0.10238123]
 [ 0.08730527]
 [-0.24284627]
 [-0.07046539]
 [-0.09779673]
 [-0.01897345]
 [ 0.19371604]
 [ 0.13682668]
 [ 0.29080602]
 [-0.09895745]]
Bias Camada Escondida: 
[-0.18603405]
Iteration 1, loss = 0.68207460
Iteration 2, loss = 0.65603684
Iteration 3, loss = 0.64859256
Iteration 4, loss = 0.64697836
Iteration 5, loss = 0.64016002
Iteration 6, loss = 0.63375742
Iteration 7, loss = 0.62663544
Iteration 8, loss = 0.61974868
Iteration 9, loss = 0.61307091
Iteration 10, loss = 0.60471519
Iteration 11, loss = 0.59753024
Iteration 12, loss = 0.58767940
Iteration 13, loss = 0.57890650
Iteration 14, loss = 0.56884549
Iteration 15, loss = 0.55861928
Iteration 16, loss = 0.54790864
Iteration 17, loss = 0.53698438
Iteration 18, loss = 0.52592444
Iteration 19, loss = 0.51397698
Iteration 20, loss = 0.50434321
Iteration 21, loss = 0.49305357
Iteration 22, loss = 0.48052260
Iteration 23, loss = 0.46741141
Iteration 24, loss = 0.45633115
Iteration 25, loss = 0.44591541
Iteration 26, loss = 0.43380533
Iteration 27, loss = 0.42346021
Iteration 28, loss = 0.41339360
Iteration 29, loss = 0.40394100
Iteration 30, loss = 0.39569099
Iteration 31, loss = 0.38720901
Iteration 32, loss = 0.37976157
Iteration 33, loss = 0.37302933
Iteration 34, loss = 0.36689431
Iteration 35, loss = 0.36203535
Iteration 36, loss = 0.35668045
Iteration 37, loss = 0.35150570
Iteration 38, loss = 0.34697591
Iteration 39, loss = 0.34260182
Iteration 40, loss = 0.33822042
Iteration 41, loss = 0.33403126
Iteration 42, loss = 0.33076425
Iteration 43, loss = 0.32725524
Iteration 44, loss = 0.32393687
Iteration 45, loss = 0.32194444
Iteration 46, loss = 0.31741278
Iteration 47, loss = 0.31402448
Iteration 48, loss = 0.31190949
Iteration 49, loss = 0.30989104
Iteration 50, loss = 0.30607667
Iteration 51, loss = 0.30251344
Iteration 52, loss = 0.30026471
Iteration 53, loss = 0.29688574
Iteration 54, loss = 0.29461556
Iteration 55, loss = 0.29155965
Iteration 56, loss = 0.28922414
Iteration 57, loss = 0.28639596
Iteration 58, loss = 0.28361059
Iteration 59, loss = 0.28114835
Iteration 60, loss = 0.27895612
Iteration 61, loss = 0.27677398
Iteration 62, loss = 0.27593489
Iteration 63, loss = 0.27381007
Iteration 64, loss = 0.26988981
Iteration 65, loss = 0.26831767
Iteration 66, loss = 0.26553762
Iteration 67, loss = 0.26352668
Iteration 68, loss = 0.26158534
Iteration 69, loss = 0.25863909
Iteration 70, loss = 0.25642926
Iteration 71, loss = 0.25435481
Iteration 72, loss = 0.25271475
Iteration 73, loss = 0.25077020
Iteration 74, loss = 0.24901285
Iteration 75, loss = 0.24720289
Iteration 76, loss = 0.24517991
Iteration 77, loss = 0.24279112
Iteration 78, loss = 0.24003606
Iteration 79, loss = 0.23972074
Iteration 80, loss = 0.23648195
Iteration 81, loss = 0.23439141
Iteration 82, loss = 0.23303823
Iteration 83, loss = 0.23169610
Iteration 84, loss = 0.23156383
Iteration 85, loss = 0.23062931
Iteration 86, loss = 0.22875259
Iteration 87, loss = 0.22583851
Iteration 88, loss = 0.22335357
Iteration 89, loss = 0.22161264
Iteration 90, loss = 0.22031269
Iteration 91, loss = 0.21877845
Iteration 92, loss = 0.21697476
Iteration 93, loss = 0.21518247
Iteration 94, loss = 0.21368058
Iteration 95, loss = 0.21248018
Iteration 96, loss = 0.21206761
Iteration 97, loss = 0.21224114
Iteration 98, loss = 0.21064881
Iteration 99, loss = 0.20843631
Iteration 100, loss = 0.20610085
Iteration 101, loss = 0.20402824
Iteration 102, loss = 0.20230462
Iteration 103, loss = 0.20134285
Iteration 104, loss = 0.20042071
Iteration 105, loss = 0.19828733
Iteration 106, loss = 0.19693069
Iteration 107, loss = 0.19556173
Iteration 108, loss = 0.19404407
Iteration 109, loss = 0.19307206
Iteration 110, loss = 0.19176847
Iteration 111, loss = 0.19058818
Iteration 112, loss = 0.18839590
Iteration 113, loss = 0.18862467
Iteration 114, loss = 0.18673177
Iteration 115, loss = 0.18529178
Iteration 116, loss = 0.18382744
Iteration 117, loss = 0.18181556
Iteration 118, loss = 0.18147062
Iteration 119, loss = 0.18219746
Iteration 120, loss = 0.17823957
Iteration 121, loss = 0.17716088
Iteration 122, loss = 0.17704895
Iteration 123, loss = 0.17634165
Iteration 124, loss = 0.17449120
Iteration 125, loss = 0.17239533
Iteration 126, loss = 0.17091263
Iteration 127, loss = 0.17054808
Iteration 128, loss = 0.17040989
Iteration 129, loss = 0.16732957
Iteration 130, loss = 0.16810386
Iteration 131, loss = 0.16690519
Iteration 132, loss = 0.16579842
Iteration 133, loss = 0.16443894
Iteration 134, loss = 0.16299663
Iteration 135, loss = 0.16152861
Iteration 136, loss = 0.16068702
Iteration 137, loss = 0.16174164
Iteration 138, loss = 0.15964988
Iteration 139, loss = 0.15817953
Iteration 140, loss = 0.15786365
Iteration 141, loss = 0.15653005
Iteration 142, loss = 0.15547039
Iteration 143, loss = 0.15432127
Iteration 144, loss = 0.15357835
Iteration 145, loss = 0.15270061
Iteration 146, loss = 0.15282827
Iteration 147, loss = 0.15349588
Iteration 148, loss = 0.15200247
Iteration 149, loss = 0.14983820
Iteration 150, loss = 0.14796175
Iteration 151, loss = 0.14679355
Iteration 152, loss = 0.14693638
Iteration 153, loss = 0.14734887
Iteration 154, loss = 0.14696287
Iteration 155, loss = 0.14520411
Iteration 156, loss = 0.14371987
Iteration 157, loss = 0.14231104
Iteration 158, loss = 0.14152700
Iteration 159, loss = 0.14170900
Iteration 160, loss = 0.14232875
Iteration 161, loss = 0.13966320
Iteration 162, loss = 0.13818724
Iteration 163, loss = 0.13777951
Iteration 164, loss = 0.13703243
Iteration 165, loss = 0.13661324
Iteration 166, loss = 0.13627243
Iteration 167, loss = 0.13578673
Iteration 168, loss = 0.13571798
Iteration 169, loss = 0.13478888
Iteration 170, loss = 0.13508543
Iteration 171, loss = 0.13356443
Iteration 172, loss = 0.13249125
Iteration 173, loss = 0.13219870
Iteration 174, loss = 0.13147219
Iteration 175, loss = 0.13066020
Iteration 176, loss = 0.12975844
Iteration 177, loss = 0.12886946
Iteration 178, loss = 0.12825177
Iteration 179, loss = 0.12737262
Iteration 180, loss = 0.12676847
Iteration 181, loss = 0.12635930
Iteration 182, loss = 0.12552252
Iteration 183, loss = 0.12525201
Iteration 184, loss = 0.12506156
Iteration 185, loss = 0.12486446
Iteration 186, loss = 0.12416792
Iteration 187, loss = 0.12321823
Iteration 188, loss = 0.12274370
Iteration 189, loss = 0.12242925
Iteration 190, loss = 0.12192192
Iteration 191, loss = 0.12128200
Iteration 192, loss = 0.12063802
Iteration 193, loss = 0.12016165
Iteration 194, loss = 0.11949930
Iteration 195, loss = 0.11901869
Iteration 196, loss = 0.11835004
Iteration 197, loss = 0.11807042
Iteration 198, loss = 0.11769093
Iteration 199, loss = 0.11752240
Iteration 200, loss = 0.11649889
Iteration 201, loss = 0.11606220
Iteration 202, loss = 0.11555693
Iteration 203, loss = 0.11500374
Iteration 204, loss = 0.11491809
Iteration 205, loss = 0.11506455
Iteration 206, loss = 0.11438377
Iteration 207, loss = 0.11345917
Iteration 208, loss = 0.11291074
Iteration 209, loss = 0.11227831
Iteration 210, loss = 0.11174700
Iteration 211, loss = 0.11132894
Iteration 212, loss = 0.11092762
Iteration 213, loss = 0.11097609
Iteration 214, loss = 0.11074129
Iteration 215, loss = 0.11008191
Iteration 216, loss = 0.10924439
Iteration 217, loss = 0.10912650
Iteration 218, loss = 0.10904265
Iteration 219, loss = 0.10869065
Iteration 220, loss = 0.10814566
Iteration 221, loss = 0.10760381
Iteration 222, loss = 0.10698145
Iteration 223, loss = 0.10694084
Iteration 224, loss = 0.10757124
Iteration 225, loss = 0.10642641
Iteration 226, loss = 0.10504829
Iteration 227, loss = 0.10564190
Iteration 228, loss = 0.10601831
Iteration 229, loss = 0.10539245
Iteration 230, loss = 0.10427001
Iteration 231, loss = 0.10337269
Iteration 232, loss = 0.10329928
Iteration 233, loss = 0.10333607
Iteration 234, loss = 0.10279262
Iteration 235, loss = 0.10185039
Iteration 236, loss = 0.10137911
Iteration 237, loss = 0.10159054
Iteration 238, loss = 0.10254657
Iteration 239, loss = 0.10209720
Iteration 240, loss = 0.10126812
Iteration 241, loss = 0.10049454
Iteration 242, loss = 0.09983274
Iteration 243, loss = 0.09955614
Iteration 244, loss = 0.09908642
Iteration 245, loss = 0.09897276
Iteration 246, loss = 0.09875220
Iteration 247, loss = 0.09847073
Iteration 248, loss = 0.09819872
Iteration 249, loss = 0.09802965
Iteration 250, loss = 0.09801166
Iteration 251, loss = 0.09760956
Iteration 252, loss = 0.09671889
Iteration 253, loss = 0.09612510
Iteration 254, loss = 0.09593208
Iteration 255, loss = 0.09600189
Iteration 256, loss = 0.09552059
Iteration 257, loss = 0.09501614
Iteration 258, loss = 0.09449452
Iteration 259, loss = 0.09425252
Iteration 260, loss = 0.09391852
Iteration 261, loss = 0.09401008
Iteration 262, loss = 0.09324655
Iteration 263, loss = 0.09300400
Iteration 264, loss = 0.09284731
Iteration 265, loss = 0.09213286
Iteration 266, loss = 0.09180274
Iteration 267, loss = 0.09151817
Iteration 268, loss = 0.09131018
Iteration 269, loss = 0.09105304
Iteration 270, loss = 0.09081043
Iteration 271, loss = 0.09074900
Iteration 272, loss = 0.09044158
Iteration 273, loss = 0.08965906
Iteration 274, loss = 0.08920166
Iteration 275, loss = 0.08942158
Iteration 276, loss = 0.08916332
Iteration 277, loss = 0.08894367
Iteration 278, loss = 0.08866865
Iteration 279, loss = 0.08830232
Iteration 280, loss = 0.08793659
Iteration 281, loss = 0.08737026
Iteration 282, loss = 0.08709056
Iteration 283, loss = 0.08646031
Iteration 284, loss = 0.08615386
Iteration 285, loss = 0.08584328
Iteration 286, loss = 0.08552009
Iteration 287, loss = 0.08534161
Iteration 288, loss = 0.08496853
Iteration 289, loss = 0.08468739
Iteration 290, loss = 0.08429378
Iteration 291, loss = 0.08437913
Iteration 292, loss = 0.08479149
Iteration 293, loss = 0.08426406
Iteration 294, loss = 0.08307747
Iteration 295, loss = 0.08306297
Iteration 296, loss = 0.08398269
Iteration 297, loss = 0.08312803
Iteration 298, loss = 0.08240872
Iteration 299, loss = 0.08199738
Iteration 300, loss = 0.08156319
PARAMETROS DE INICIALIZACAO DA REDE
NUMERO DE NEURONIOS 
Camada de Entrada: 34
Camada Escondida: 15
Camada de Saida: 1

--- PARAMETROS DE CONFIGURACAO DA REDE ---
Numero de Epocas: 300
Taxa de Aprendizado: adaptive
Taxa de Aprendizado Inicial: 0.1
METRICAS

RESULTADOS:

[0 0 1 0 1 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1 1 1 1 1 1 1
 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 1 1 1
 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 1 0]
ACURACIA: 0.9339622641509434

