Pesos Camada de Entrada: 
[[-0.12803151 -0.12549058  0.0323963   0.20810282  0.10062523 -0.02415469
   0.05627698  0.11043767  0.13711404  0.20066816]
 [-0.09339455 -0.01853252  0.06602694  0.04930793 -0.19966841  0.05874677
  -0.02189406  0.0304532   0.04745288  0.14728923]
 [ 0.14411286  0.09000962  0.09742693 -0.07567315 -0.16584783  0.06360926
   0.04457567  0.16445741 -0.00568977  0.12508433]
 [-0.10444058  0.16554768 -0.13675257  0.03415489 -0.03088447 -0.19861406
  -0.10129803 -0.18158612  0.04430611  0.16972751]
 [-0.11074884  0.06460444 -0.14411278 -0.19726351  0.0688792   0.09928245
  -0.0622254  -0.05997432  0.14586792  0.0618383 ]
 [ 0.03609005 -0.01035862 -0.17716952  0.10420466 -0.06101811 -0.14884183
  -0.15019642  0.00237664 -0.21198182  0.03356633]
 [ 0.10075753 -0.11841242  0.19509754 -0.07280901  0.1140568   0.20332189
  -0.16732389 -0.11679655 -0.14259446  0.07148816]
 [-0.04974019 -0.20531454  0.06053398 -0.19720557  0.11501475 -0.06486231
   0.05757567  0.17892327 -0.05662681  0.04950532]
 [ 0.20443267 -0.03587659  0.09929526  0.10083694  0.00120816  0.1284535
   0.15858606  0.10398901 -0.11954016 -0.02080805]
 [ 0.09198795 -0.1577165  -0.16495618  0.00573758  0.10007464 -0.05858257
   0.14141531  0.09500638  0.20490712  0.19541623]
 [-0.08344303  0.09100323 -0.18320778 -0.06928468  0.01359572 -0.18709633
  -0.09417806 -0.11782681 -0.0080352   0.13090772]
 [-0.02858431 -0.03360829 -0.18530017 -0.05683943 -0.16439863 -0.11593101
  -0.14384118  0.14256156 -0.01196939  0.11063154]
 [-0.17199366 -0.02502403 -0.19051652 -0.10139861 -0.06426922 -0.1507354
   0.15970793  0.18723433 -0.02528127 -0.13156791]
 [-0.05267436  0.16738604 -0.13451303 -0.0252953   0.1565908   0.08296412
  -0.17333367 -0.10625557  0.16850967 -0.01409058]
 [-0.18175356  0.16569477 -0.10899751  0.16256343 -0.11338931  0.06483666
  -0.09524038 -0.08380664  0.09936876 -0.13961987]
 [ 0.21026845 -0.21132496  0.21038716  0.01864833 -0.00042807 -0.16451213
   0.07998277  0.12925307  0.18873069 -0.1007573 ]
 [-0.12624487 -0.04920155  0.1963362   0.1309122   0.06472654  0.19242502
  -0.13671461 -0.07931626 -0.14805132 -0.01695188]
 [-0.1630189   0.13104945  0.19635682  0.1754977  -0.15175497  0.01701143
  -0.08857572 -0.18242047  0.15011107 -0.05044025]
 [-0.13024733  0.06721418 -0.07368508  0.1188218  -0.02617941 -0.11262171
  -0.01240191 -0.01559819  0.18483431 -0.04505305]
 [ 0.06890512 -0.1454489  -0.09853864  0.07486835  0.11323782 -0.08544615
   0.15583436 -0.04280743 -0.01955892 -0.17856683]
 [-0.05272776 -0.0072362  -0.08657878  0.17257554 -0.00850298  0.00717012
   0.21200331 -0.01646054 -0.13622478  0.08959128]
 [-0.13890217 -0.19262119 -0.16771233 -0.19734728  0.15136765 -0.03553892
   0.1388192  -0.20095523  0.07524451  0.03419819]
 [-0.06305562 -0.07734556 -0.03216609 -0.01459999  0.06500436 -0.13763027
   0.11622845  0.0141304   0.16516876 -0.16578337]
 [ 0.00271269  0.1737105  -0.0048328  -0.1881584  -0.0574188  -0.10004348
  -0.10427509  0.02849946 -0.1674381  -0.15394585]
 [-0.16888229  0.07016437  0.0643668   0.04360591  0.12653402 -0.14406374
  -0.21317737 -0.10399817  0.03278499  0.08357248]
 [ 0.19451705  0.06995382  0.03495201 -0.11287697  0.12942643  0.06725993
   0.15192058  0.18385287  0.18415019  0.1793923 ]
 [-0.20051678 -0.0990575  -0.00645453 -0.1202104  -0.06664369  0.06700934
   0.05880126 -0.11879355 -0.05255108 -0.11655449]
 [-0.04114226 -0.17121412  0.07904093  0.0431899  -0.19028104 -0.09598473
  -0.05263831 -0.02618106  0.09497993  0.17487904]
 [ 0.21036465  0.1418001   0.11815599 -0.02998754 -0.13338717 -0.01852519
  -0.20813588  0.1362388   0.09573391  0.18068202]
 [-0.14390975  0.11783651  0.1739296  -0.10646424  0.10969613  0.09379462
   0.00470628 -0.16856927  0.01156843 -0.03102333]
 [ 0.18002354  0.0151413  -0.02949461  0.06382998  0.02209269  0.02708733
   0.20084375  0.06682795 -0.184186    0.11590693]
 [-0.20110553  0.08970422 -0.17627673 -0.14244141  0.15748884  0.07109193
  -0.02893794  0.20691542  0.20949493 -0.20679229]
 [ 0.00380684  0.11721999  0.10287014  0.09469844 -0.00889567  0.21078759
  -0.07786763 -0.01261932  0.171115    0.00462617]
 [ 0.05128039  0.05057886  0.01543371  0.13762605 -0.07521123  0.00392481
  -0.16063709  0.17566376  0.15109552 -0.00683123]]
Bias Camada de Entrada: 
[-0.1791606  -0.16708867  0.02220151 -0.083785    0.01878393 -0.20724071
 -0.03068484 -0.05540941 -0.02747808  0.02311523]
Pesos Camada Escondida: 
[[-0.21506482]
 [-0.21601714]
 [ 0.40666369]
 [-0.08110298]
 [ 0.22285948]
 [-0.19439174]
 [ 0.23686777]
 [ 0.1758416 ]
 [-0.18138902]
 [-0.27222714]]
Bias Camada Escondida: 
[-0.30766845]
Iteration 1, loss = 0.75898356
Iteration 2, loss = 0.68571499
Iteration 3, loss = 0.65264446
Iteration 4, loss = 0.64664342
Iteration 5, loss = 0.64655483
Iteration 6, loss = 0.64152738
Iteration 7, loss = 0.63121535
Iteration 8, loss = 0.62681120
Iteration 9, loss = 0.62426399
Iteration 10, loss = 0.61888488
Iteration 11, loss = 0.61193766
Iteration 12, loss = 0.60402094
Iteration 13, loss = 0.59814194
Iteration 14, loss = 0.59452917
Iteration 15, loss = 0.58862674
Iteration 16, loss = 0.58119666
Iteration 17, loss = 0.57219009
Iteration 18, loss = 0.56160187
Iteration 19, loss = 0.55039641
Iteration 20, loss = 0.54017025
Iteration 21, loss = 0.53140536
Iteration 22, loss = 0.52086259
Iteration 23, loss = 0.50928124
Iteration 24, loss = 0.50114227
Iteration 25, loss = 0.49101977
Iteration 26, loss = 0.47927075
Iteration 27, loss = 0.46661693
Iteration 28, loss = 0.45899876
Iteration 29, loss = 0.44984754
Iteration 30, loss = 0.44051438
Iteration 31, loss = 0.42698360
Iteration 32, loss = 0.41709564
Iteration 33, loss = 0.41154692
Iteration 34, loss = 0.40579083
Iteration 35, loss = 0.39734292
Iteration 36, loss = 0.38751004
Iteration 37, loss = 0.37852969
Iteration 38, loss = 0.37115996
Iteration 39, loss = 0.36665153
Iteration 40, loss = 0.36162392
Iteration 41, loss = 0.35575113
Iteration 42, loss = 0.34995560
Iteration 43, loss = 0.34471167
Iteration 44, loss = 0.34000250
Iteration 45, loss = 0.33606973
Iteration 46, loss = 0.33218331
Iteration 47, loss = 0.32783404
Iteration 48, loss = 0.32352056
Iteration 49, loss = 0.31942328
Iteration 50, loss = 0.31697118
Iteration 51, loss = 0.31688928
Iteration 52, loss = 0.31229412
Iteration 53, loss = 0.30713139
Iteration 54, loss = 0.30255704
Iteration 55, loss = 0.29966001
Iteration 56, loss = 0.29711141
Iteration 57, loss = 0.29398577
Iteration 58, loss = 0.29077477
Iteration 59, loss = 0.28824512
Iteration 60, loss = 0.28682469
Iteration 61, loss = 0.28282056
Iteration 62, loss = 0.27990536
Iteration 63, loss = 0.27731389
Iteration 64, loss = 0.27525912
Iteration 65, loss = 0.27251551
Iteration 66, loss = 0.27022752
Iteration 67, loss = 0.26843091
Iteration 68, loss = 0.26659771
Iteration 69, loss = 0.26413183
Iteration 70, loss = 0.26323595
Iteration 71, loss = 0.26118285
Iteration 72, loss = 0.25893148
Iteration 73, loss = 0.25627136
Iteration 74, loss = 0.25505807
Iteration 75, loss = 0.25337895
Iteration 76, loss = 0.25149999
Iteration 77, loss = 0.24932518
Iteration 78, loss = 0.24726927
Iteration 79, loss = 0.24564114
Iteration 80, loss = 0.24414860
Iteration 81, loss = 0.24207364
Iteration 82, loss = 0.24016335
Iteration 83, loss = 0.23884024
Iteration 84, loss = 0.23685282
Iteration 85, loss = 0.23524860
Iteration 86, loss = 0.23387992
Iteration 87, loss = 0.23223054
Iteration 88, loss = 0.23075682
Iteration 89, loss = 0.22918202
Iteration 90, loss = 0.22771045
Iteration 91, loss = 0.22621022
Iteration 92, loss = 0.22481343
Iteration 93, loss = 0.22307165
Iteration 94, loss = 0.22113105
Iteration 95, loss = 0.21933101
Iteration 96, loss = 0.21780998
Iteration 97, loss = 0.21626840
Iteration 98, loss = 0.21503533
Iteration 99, loss = 0.21466767
Iteration 100, loss = 0.21463083
Iteration 101, loss = 0.21360017
Iteration 102, loss = 0.21040657
Iteration 103, loss = 0.20709856
Iteration 104, loss = 0.20544429
Iteration 105, loss = 0.20401101
Iteration 106, loss = 0.20357469
Iteration 107, loss = 0.20195612
Iteration 108, loss = 0.20012979
Iteration 109, loss = 0.19784544
Iteration 110, loss = 0.19553580
Iteration 111, loss = 0.19480194
Iteration 112, loss = 0.19583718
Iteration 113, loss = 0.19451116
Iteration 114, loss = 0.19230986
Iteration 115, loss = 0.18916101
Iteration 116, loss = 0.18708480
Iteration 117, loss = 0.18708338
Iteration 118, loss = 0.18834987
Iteration 119, loss = 0.18827722
Iteration 120, loss = 0.18563627
Iteration 121, loss = 0.18183593
Iteration 122, loss = 0.17882121
Iteration 123, loss = 0.17743546
Iteration 124, loss = 0.17717874
Iteration 125, loss = 0.17652061
Iteration 126, loss = 0.17552756
Iteration 127, loss = 0.17363239
Iteration 128, loss = 0.17181270
Iteration 129, loss = 0.17085801
Iteration 130, loss = 0.16970371
Iteration 131, loss = 0.16876300
Iteration 132, loss = 0.16802133
Iteration 133, loss = 0.16669937
Iteration 134, loss = 0.16535351
Iteration 135, loss = 0.16429194
Iteration 136, loss = 0.16287147
Iteration 137, loss = 0.16193923
Iteration 138, loss = 0.16241094
Iteration 139, loss = 0.16110425
Iteration 140, loss = 0.15944120
Iteration 141, loss = 0.15732832
Iteration 142, loss = 0.15598083
Iteration 143, loss = 0.15525634
Iteration 144, loss = 0.15558879
Iteration 145, loss = 0.15450017
Iteration 146, loss = 0.15296993
Iteration 147, loss = 0.15163920
Iteration 148, loss = 0.15104611
Iteration 149, loss = 0.15093917
Iteration 150, loss = 0.15031880
Iteration 151, loss = 0.14856403
Iteration 152, loss = 0.14890053
Iteration 153, loss = 0.14729762
Iteration 154, loss = 0.14657019
Iteration 155, loss = 0.14532028
Iteration 156, loss = 0.14465473
Iteration 157, loss = 0.14400649
Iteration 158, loss = 0.14311784
Iteration 159, loss = 0.14280993
Iteration 160, loss = 0.14172611
Iteration 161, loss = 0.14085817
Iteration 162, loss = 0.14032374
Iteration 163, loss = 0.13978675
Iteration 164, loss = 0.14009130
Iteration 165, loss = 0.13963152
Iteration 166, loss = 0.13808274
Iteration 167, loss = 0.13727843
Iteration 168, loss = 0.13644221
Iteration 169, loss = 0.13592080
Iteration 170, loss = 0.13556890
Iteration 171, loss = 0.13515107
Iteration 172, loss = 0.13393471
Iteration 173, loss = 0.13326026
Iteration 174, loss = 0.13253904
Iteration 175, loss = 0.13208148
Iteration 176, loss = 0.13126775
Iteration 177, loss = 0.13064995
Iteration 178, loss = 0.13021848
Iteration 179, loss = 0.12989546
Iteration 180, loss = 0.12965077
Iteration 181, loss = 0.12879719
Iteration 182, loss = 0.12788407
Iteration 183, loss = 0.12749297
Iteration 184, loss = 0.12681798
Iteration 185, loss = 0.12630706
Iteration 186, loss = 0.12621799
Iteration 187, loss = 0.12538911
Iteration 188, loss = 0.12486709
Iteration 189, loss = 0.12470959
Iteration 190, loss = 0.12439069
Iteration 191, loss = 0.12383745
Iteration 192, loss = 0.12315150
Iteration 193, loss = 0.12231238
Iteration 194, loss = 0.12237945
Iteration 195, loss = 0.12200457
Iteration 196, loss = 0.12159809
Iteration 197, loss = 0.12104299
Iteration 198, loss = 0.12046103
Iteration 199, loss = 0.11985395
Iteration 200, loss = 0.11930699
Iteration 201, loss = 0.11882349
Iteration 202, loss = 0.11843239
Iteration 203, loss = 0.11796469
Iteration 204, loss = 0.11757412
Iteration 205, loss = 0.11685251
Iteration 206, loss = 0.11645968
Iteration 207, loss = 0.11627070
Iteration 208, loss = 0.11633883
Iteration 209, loss = 0.11677758
Iteration 210, loss = 0.11657888
Iteration 211, loss = 0.11555186
Iteration 212, loss = 0.11478097
Iteration 213, loss = 0.11398281
Iteration 214, loss = 0.11363039
Iteration 215, loss = 0.11308192
Iteration 216, loss = 0.11256250
Iteration 217, loss = 0.11204974
Iteration 218, loss = 0.11187764
Iteration 219, loss = 0.11146331
Iteration 220, loss = 0.11057268
Iteration 221, loss = 0.11014640
Iteration 222, loss = 0.10950114
Iteration 223, loss = 0.10895343
Iteration 224, loss = 0.10888190
Iteration 225, loss = 0.10852435
Iteration 226, loss = 0.10827968
Iteration 227, loss = 0.10793655
Iteration 228, loss = 0.10750460
Iteration 229, loss = 0.10702117
Iteration 230, loss = 0.10668710
Iteration 231, loss = 0.10602514
Iteration 232, loss = 0.10552427
Iteration 233, loss = 0.10490479
Iteration 234, loss = 0.10446840
Iteration 235, loss = 0.10413171
Iteration 236, loss = 0.10368520
Iteration 237, loss = 0.10362114
Iteration 238, loss = 0.10365333
Iteration 239, loss = 0.10290898
Iteration 240, loss = 0.10270272
Iteration 241, loss = 0.10198542
Iteration 242, loss = 0.10162216
Iteration 243, loss = 0.10129299
Iteration 244, loss = 0.10135959
Iteration 245, loss = 0.10063754
Iteration 246, loss = 0.10005286
Iteration 247, loss = 0.09959105
Iteration 248, loss = 0.09918049
Iteration 249, loss = 0.09901118
Iteration 250, loss = 0.09847989
Iteration 251, loss = 0.09810440
Iteration 252, loss = 0.09771670
Iteration 253, loss = 0.09783869
Iteration 254, loss = 0.09761479
Iteration 255, loss = 0.09764250
Iteration 256, loss = 0.09655828
Iteration 257, loss = 0.09627636
Iteration 258, loss = 0.09594248
Iteration 259, loss = 0.09573827
Iteration 260, loss = 0.09535442
Iteration 261, loss = 0.09500360
Iteration 262, loss = 0.09468745
Iteration 263, loss = 0.09426079
Iteration 264, loss = 0.09405830
Iteration 265, loss = 0.09358875
Iteration 266, loss = 0.09314871
Iteration 267, loss = 0.09265951
Iteration 268, loss = 0.09229500
Iteration 269, loss = 0.09198647
Iteration 270, loss = 0.09175004
Iteration 271, loss = 0.09161209
Iteration 272, loss = 0.09159983
Iteration 273, loss = 0.09105401
Iteration 274, loss = 0.09058548
Iteration 275, loss = 0.09023791
Iteration 276, loss = 0.08980764
Iteration 277, loss = 0.08965448
Iteration 278, loss = 0.08974928
Iteration 279, loss = 0.08939055
Iteration 280, loss = 0.08909046
Iteration 281, loss = 0.08882698
Iteration 282, loss = 0.08827695
Iteration 283, loss = 0.08755581
Iteration 284, loss = 0.08736347
Iteration 285, loss = 0.08740955
Iteration 286, loss = 0.08644185
Iteration 287, loss = 0.08650866
Iteration 288, loss = 0.08641265
Iteration 289, loss = 0.08602242
Iteration 290, loss = 0.08568270
Iteration 291, loss = 0.08544168
Iteration 292, loss = 0.08500530
Iteration 293, loss = 0.08470236
Iteration 294, loss = 0.08416862
Iteration 295, loss = 0.08394319
Iteration 296, loss = 0.08361479
Iteration 297, loss = 0.08338474
Iteration 298, loss = 0.08313901
Iteration 299, loss = 0.08293163
Iteration 300, loss = 0.08270166
Iteration 301, loss = 0.08254223
Iteration 302, loss = 0.08174523
Iteration 303, loss = 0.08140529
Iteration 304, loss = 0.08211397
Iteration 305, loss = 0.08136124
Iteration 306, loss = 0.08092557
Iteration 307, loss = 0.08065458
Iteration 308, loss = 0.08040171
Iteration 309, loss = 0.07989871
Iteration 310, loss = 0.07956345
Iteration 311, loss = 0.07940881
Iteration 312, loss = 0.07978374
Iteration 313, loss = 0.07901245
Iteration 314, loss = 0.07904874
Iteration 315, loss = 0.07835409
Iteration 316, loss = 0.07818525
Iteration 317, loss = 0.07784659
Iteration 318, loss = 0.07793079
Iteration 319, loss = 0.07799302
Iteration 320, loss = 0.07767301
Iteration 321, loss = 0.07697625
Iteration 322, loss = 0.07669477
Iteration 323, loss = 0.07683368
Iteration 324, loss = 0.07687043
Iteration 325, loss = 0.07665641
Iteration 326, loss = 0.07623614
Iteration 327, loss = 0.07577186
Iteration 328, loss = 0.07519101
Iteration 329, loss = 0.07487299
Iteration 330, loss = 0.07474532
Iteration 331, loss = 0.07444982
Iteration 332, loss = 0.07417600
Iteration 333, loss = 0.07397441
Iteration 334, loss = 0.07376328
Iteration 335, loss = 0.07364212
Iteration 336, loss = 0.07351028
Iteration 337, loss = 0.07313416
Iteration 338, loss = 0.07291141
Iteration 339, loss = 0.07276847
Iteration 340, loss = 0.07240426
Iteration 341, loss = 0.07214037
Iteration 342, loss = 0.07180637
Iteration 343, loss = 0.07156989
Iteration 344, loss = 0.07145032
Iteration 345, loss = 0.07137892
Iteration 346, loss = 0.07117492
Iteration 347, loss = 0.07093081
Iteration 348, loss = 0.07077285
Iteration 349, loss = 0.07064332
Iteration 350, loss = 0.07072502
Iteration 351, loss = 0.07012180
Iteration 352, loss = 0.06942072
Iteration 353, loss = 0.06954685
Iteration 354, loss = 0.07002693
Iteration 355, loss = 0.06991377
Iteration 356, loss = 0.06984386
Iteration 357, loss = 0.06894662
Iteration 358, loss = 0.06840417
Iteration 359, loss = 0.06808897
Iteration 360, loss = 0.06772881
Iteration 361, loss = 0.06752164
Iteration 362, loss = 0.06730889
Iteration 363, loss = 0.06715031
Iteration 364, loss = 0.06690414
Iteration 365, loss = 0.06674301
Iteration 366, loss = 0.06661078
Iteration 367, loss = 0.06634931
Iteration 368, loss = 0.06619365
Iteration 369, loss = 0.06600300
Iteration 370, loss = 0.06607940
Iteration 371, loss = 0.06562640
Iteration 372, loss = 0.06543978
Iteration 373, loss = 0.06521914
Iteration 374, loss = 0.06504824
Iteration 375, loss = 0.06482908
Iteration 376, loss = 0.06459615
Iteration 377, loss = 0.06461572
Iteration 378, loss = 0.06505303
Iteration 379, loss = 0.06463970
Iteration 380, loss = 0.06393691
Iteration 381, loss = 0.06329999
Iteration 382, loss = 0.06305882
Iteration 383, loss = 0.06306848
Iteration 384, loss = 0.06329947
Iteration 385, loss = 0.06277378
Iteration 386, loss = 0.06246412
Iteration 387, loss = 0.06235697
Iteration 388, loss = 0.06221956
Iteration 389, loss = 0.06234242
Iteration 390, loss = 0.06280276
Iteration 391, loss = 0.06303848
Iteration 392, loss = 0.06246227
Iteration 393, loss = 0.06188472
Iteration 394, loss = 0.06119070
Iteration 395, loss = 0.06106485
Iteration 396, loss = 0.06100948
Iteration 397, loss = 0.06070650
Iteration 398, loss = 0.06056344
Iteration 399, loss = 0.06049045
Iteration 400, loss = 0.06032840
Iteration 401, loss = 0.06029542
Iteration 402, loss = 0.05999304
Iteration 403, loss = 0.05973705
Iteration 404, loss = 0.05944915
Iteration 405, loss = 0.05926688
Iteration 406, loss = 0.05905371
Iteration 407, loss = 0.05891564
Iteration 408, loss = 0.05879204
Iteration 409, loss = 0.05851130
Iteration 410, loss = 0.05831313
Iteration 411, loss = 0.05800756
Iteration 412, loss = 0.05779469
Iteration 413, loss = 0.05763981
Iteration 414, loss = 0.05762901
Iteration 415, loss = 0.05772134
Iteration 416, loss = 0.05786086
Iteration 417, loss = 0.05792844
Iteration 418, loss = 0.05734785
Iteration 419, loss = 0.05680476
Iteration 420, loss = 0.05650761
Iteration 421, loss = 0.05643380
Iteration 422, loss = 0.05626763
Iteration 423, loss = 0.05628928
Iteration 424, loss = 0.05587879
Iteration 425, loss = 0.05564259
Iteration 426, loss = 0.05539720
Iteration 427, loss = 0.05526017
Iteration 428, loss = 0.05525329
Iteration 429, loss = 0.05531029
Iteration 430, loss = 0.05567940
Iteration 431, loss = 0.05504677
Iteration 432, loss = 0.05469813
Iteration 433, loss = 0.05415417
Iteration 434, loss = 0.05447208
Iteration 435, loss = 0.05497123
Iteration 436, loss = 0.05517471
Iteration 437, loss = 0.05501550
Iteration 438, loss = 0.05405567
Iteration 439, loss = 0.05347798
Iteration 440, loss = 0.05321896
Iteration 441, loss = 0.05372283
Iteration 442, loss = 0.05310241
Iteration 443, loss = 0.05277864
Iteration 444, loss = 0.05268958
Iteration 445, loss = 0.05252721
Iteration 446, loss = 0.05228441
Iteration 447, loss = 0.05203832
Iteration 448, loss = 0.05183373
Iteration 449, loss = 0.05178588
Iteration 450, loss = 0.05152632
Iteration 451, loss = 0.05135392
Iteration 452, loss = 0.05119475
Iteration 453, loss = 0.05107408
Iteration 454, loss = 0.05090870
Iteration 455, loss = 0.05079796
Iteration 456, loss = 0.05076571
Iteration 457, loss = 0.05081637
Iteration 458, loss = 0.05062098
Iteration 459, loss = 0.05053443
Iteration 460, loss = 0.05046943
Iteration 461, loss = 0.05024218
Iteration 462, loss = 0.05012508
Iteration 463, loss = 0.05004455
Iteration 464, loss = 0.04995168
Iteration 465, loss = 0.04998636
Iteration 466, loss = 0.05023964
Iteration 467, loss = 0.04998148
Iteration 468, loss = 0.04967304
Iteration 469, loss = 0.04936700
Iteration 470, loss = 0.04899146
Iteration 471, loss = 0.04879176
Iteration 472, loss = 0.04906152
Iteration 473, loss = 0.04970360
Iteration 474, loss = 0.04955557
Iteration 475, loss = 0.04866662
Iteration 476, loss = 0.04800778
Iteration 477, loss = 0.04775395
Iteration 478, loss = 0.04757641
Iteration 479, loss = 0.04774519
Iteration 480, loss = 0.04764410
Iteration 481, loss = 0.04750269
Iteration 482, loss = 0.04732986
Iteration 483, loss = 0.04708622
Iteration 484, loss = 0.04688066
Iteration 485, loss = 0.04671082
Iteration 486, loss = 0.04655654
Iteration 487, loss = 0.04638321
Iteration 488, loss = 0.04615035
Iteration 489, loss = 0.04600259
Iteration 490, loss = 0.04587460
Iteration 491, loss = 0.04587709
Iteration 492, loss = 0.04590739
Iteration 493, loss = 0.04586932
Iteration 494, loss = 0.04573451
Iteration 495, loss = 0.04529246
Iteration 496, loss = 0.04499613
Iteration 497, loss = 0.04485450
Iteration 498, loss = 0.04476119
Iteration 499, loss = 0.04492109
Iteration 500, loss = 0.04462479
Iteration 501, loss = 0.04438473
Iteration 502, loss = 0.04426584
Iteration 503, loss = 0.04430502
Iteration 504, loss = 0.04416871
Iteration 505, loss = 0.04423643
Iteration 506, loss = 0.04417682
Iteration 507, loss = 0.04434139
Iteration 508, loss = 0.04461839
Iteration 509, loss = 0.04448697
Iteration 510, loss = 0.04377262
Iteration 511, loss = 0.04326096
Iteration 512, loss = 0.04305486
Iteration 513, loss = 0.04312271
Iteration 514, loss = 0.04268973
Iteration 515, loss = 0.04297360
Iteration 516, loss = 0.04300151
Iteration 517, loss = 0.04277732
Iteration 518, loss = 0.04246114
Iteration 519, loss = 0.04217441
Iteration 520, loss = 0.04185998
Iteration 521, loss = 0.04225243
Iteration 522, loss = 0.04196032
Iteration 523, loss = 0.04192930
Iteration 524, loss = 0.04177109
Iteration 525, loss = 0.04152256
Iteration 526, loss = 0.04107834
Iteration 527, loss = 0.04130335
Iteration 528, loss = 0.04112522
Iteration 529, loss = 0.04094456
Iteration 530, loss = 0.04100451
Iteration 531, loss = 0.04059300
Iteration 532, loss = 0.04046302
Iteration 533, loss = 0.04027294
Iteration 534, loss = 0.04026627
Iteration 535, loss = 0.04006829
Iteration 536, loss = 0.03994444
Iteration 537, loss = 0.03988173
Iteration 538, loss = 0.03963135
Iteration 539, loss = 0.03970266
Iteration 540, loss = 0.03958332
Iteration 541, loss = 0.03947654
Iteration 542, loss = 0.03935707
Iteration 543, loss = 0.03925745
Iteration 544, loss = 0.03930105
Iteration 545, loss = 0.03925912
Iteration 546, loss = 0.03895171
Iteration 547, loss = 0.03879243
Iteration 548, loss = 0.03862820
Iteration 549, loss = 0.03852867
Iteration 550, loss = 0.03842682
Iteration 551, loss = 0.03833787
Iteration 552, loss = 0.03837681
Iteration 553, loss = 0.03806947
Iteration 554, loss = 0.03809496
Iteration 555, loss = 0.03809013
Iteration 556, loss = 0.03802522
Iteration 557, loss = 0.03788224
Iteration 558, loss = 0.03783010
Iteration 559, loss = 0.03768405
Iteration 560, loss = 0.03776218
Iteration 561, loss = 0.03767570
Iteration 562, loss = 0.03751829
Iteration 563, loss = 0.03725580
Iteration 564, loss = 0.03699924
Iteration 565, loss = 0.03672057
Iteration 566, loss = 0.03662054
Iteration 567, loss = 0.03722014
Iteration 568, loss = 0.03700908
Iteration 569, loss = 0.03691985
Iteration 570, loss = 0.03665171
Iteration 571, loss = 0.03642158
Iteration 572, loss = 0.03619409
Iteration 573, loss = 0.03601554
Iteration 574, loss = 0.03609069
Iteration 575, loss = 0.03595123
Iteration 576, loss = 0.03584809
Iteration 577, loss = 0.03575477
Iteration 578, loss = 0.03553853
Iteration 579, loss = 0.03540912
Iteration 580, loss = 0.03529624
Iteration 581, loss = 0.03515305
Iteration 582, loss = 0.03512547
Iteration 583, loss = 0.03503866
Iteration 584, loss = 0.03499828
Iteration 585, loss = 0.03500366
Iteration 586, loss = 0.03491556
Iteration 587, loss = 0.03479152
Iteration 588, loss = 0.03462498
Iteration 589, loss = 0.03455910
Iteration 590, loss = 0.03452692
Iteration 591, loss = 0.03444824
Iteration 592, loss = 0.03435231
Iteration 593, loss = 0.03449827
Iteration 594, loss = 0.03404607
Iteration 595, loss = 0.03432164
Iteration 596, loss = 0.03399895
Iteration 597, loss = 0.03379081
Iteration 598, loss = 0.03356606
Iteration 599, loss = 0.03357981
Iteration 600, loss = 0.03364072
Iteration 601, loss = 0.03378830
Iteration 602, loss = 0.03392230
Iteration 603, loss = 0.03389802
Iteration 604, loss = 0.03368993
Iteration 605, loss = 0.03348492
Iteration 606, loss = 0.03325941
Iteration 607, loss = 0.03306877
Iteration 608, loss = 0.03294648
Iteration 609, loss = 0.03278617
Iteration 610, loss = 0.03274711
Iteration 611, loss = 0.03270585
Iteration 612, loss = 0.03284385
Iteration 613, loss = 0.03286632
Iteration 614, loss = 0.03273933
Iteration 615, loss = 0.03253876
Iteration 616, loss = 0.03234827
Iteration 617, loss = 0.03224297
Iteration 618, loss = 0.03211227
Iteration 619, loss = 0.03208192
Iteration 620, loss = 0.03228536
Iteration 621, loss = 0.03220184
Iteration 622, loss = 0.03186769
Iteration 623, loss = 0.03178485
Iteration 624, loss = 0.03151581
Iteration 625, loss = 0.03144384
Iteration 626, loss = 0.03130961
Iteration 627, loss = 0.03126015
Iteration 628, loss = 0.03113946
Iteration 629, loss = 0.03109266
Iteration 630, loss = 0.03096441
Iteration 631, loss = 0.03105483
Iteration 632, loss = 0.03086485
Iteration 633, loss = 0.03084872
Iteration 634, loss = 0.03074205
Iteration 635, loss = 0.03051416
Iteration 636, loss = 0.03041257
Iteration 637, loss = 0.03029949
Iteration 638, loss = 0.03021238
Iteration 639, loss = 0.03008922
Iteration 640, loss = 0.03007566
Iteration 641, loss = 0.03033117
Iteration 642, loss = 0.03030021
Iteration 643, loss = 0.03028124
Iteration 644, loss = 0.03022334
Iteration 645, loss = 0.02995108
Iteration 646, loss = 0.02973603
Iteration 647, loss = 0.02959057
Iteration 648, loss = 0.02955217
Iteration 649, loss = 0.02949540
Iteration 650, loss = 0.02948456
Iteration 651, loss = 0.02933223
Iteration 652, loss = 0.02915663
Iteration 653, loss = 0.02924237
Iteration 654, loss = 0.02909984
Iteration 655, loss = 0.02900388
Iteration 656, loss = 0.02904448
Iteration 657, loss = 0.02902806
Iteration 658, loss = 0.02897169
Iteration 659, loss = 0.02879446
Iteration 660, loss = 0.02868862
Iteration 661, loss = 0.02847846
Iteration 662, loss = 0.02833539
Iteration 663, loss = 0.02825190
Iteration 664, loss = 0.02850483
Iteration 665, loss = 0.02830437
Iteration 666, loss = 0.02823351
Iteration 667, loss = 0.02824175
Iteration 668, loss = 0.02811214
Iteration 669, loss = 0.02803916
Iteration 670, loss = 0.02798056
Iteration 671, loss = 0.02793484
Iteration 672, loss = 0.02783873
Iteration 673, loss = 0.02775204
Iteration 674, loss = 0.02766771
Iteration 675, loss = 0.02767752
Iteration 676, loss = 0.02738486
Iteration 677, loss = 0.02746571
Iteration 678, loss = 0.02740252
Iteration 679, loss = 0.02744089
Iteration 680, loss = 0.02735751
Iteration 681, loss = 0.02723048
Iteration 682, loss = 0.02716228
Iteration 683, loss = 0.02711450
Iteration 684, loss = 0.02701154
Iteration 685, loss = 0.02690727
Iteration 686, loss = 0.02673517
Iteration 687, loss = 0.02663287
Iteration 688, loss = 0.02661766
Iteration 689, loss = 0.02675585
Iteration 690, loss = 0.02668675
Iteration 691, loss = 0.02650298
Iteration 692, loss = 0.02634501
Iteration 693, loss = 0.02629158
Iteration 694, loss = 0.02631406
Iteration 695, loss = 0.02648157
Iteration 696, loss = 0.02635469
Iteration 697, loss = 0.02615865
Iteration 698, loss = 0.02601671
Iteration 699, loss = 0.02585275
Iteration 700, loss = 0.02572672
Iteration 701, loss = 0.02569467
Iteration 702, loss = 0.02567299
Iteration 703, loss = 0.02565787
Iteration 704, loss = 0.02560121
Iteration 705, loss = 0.02549261
Iteration 706, loss = 0.02551577
Iteration 707, loss = 0.02545859
Iteration 708, loss = 0.02549699
Iteration 709, loss = 0.02546223
Iteration 710, loss = 0.02537185
Iteration 711, loss = 0.02527769
Iteration 712, loss = 0.02518281
Iteration 713, loss = 0.02512135
Iteration 714, loss = 0.02506968
Iteration 715, loss = 0.02506831
Iteration 716, loss = 0.02503060
Iteration 717, loss = 0.02501251
Iteration 718, loss = 0.02494107
Iteration 719, loss = 0.02489902
Iteration 720, loss = 0.02490345
Iteration 721, loss = 0.02486309
Iteration 722, loss = 0.02478881
Iteration 723, loss = 0.02471004
Iteration 724, loss = 0.02462547
Iteration 725, loss = 0.02440725
Iteration 726, loss = 0.02434630
Iteration 727, loss = 0.02420836
Iteration 728, loss = 0.02412124
Iteration 729, loss = 0.02402472
Iteration 730, loss = 0.02392511
Iteration 731, loss = 0.02385416
Iteration 732, loss = 0.02391504
Iteration 733, loss = 0.02383639
Iteration 734, loss = 0.02375016
Iteration 735, loss = 0.02367291
Iteration 736, loss = 0.02361565
Iteration 737, loss = 0.02366632
Iteration 738, loss = 0.02361216
Iteration 739, loss = 0.02359797
Iteration 740, loss = 0.02358749
Iteration 741, loss = 0.02365130
Iteration 742, loss = 0.02357601
Iteration 743, loss = 0.02341729
Iteration 744, loss = 0.02328242
Iteration 745, loss = 0.02319664
Iteration 746, loss = 0.02303609
Iteration 747, loss = 0.02291262
Iteration 748, loss = 0.02286099
Iteration 749, loss = 0.02283432
Iteration 750, loss = 0.02286172
Iteration 751, loss = 0.02285725
Iteration 752, loss = 0.02286087
Iteration 753, loss = 0.02277813
Iteration 754, loss = 0.02273720
Iteration 755, loss = 0.02267193
Iteration 756, loss = 0.02266756
Iteration 757, loss = 0.02269037
Iteration 758, loss = 0.02254564
Iteration 759, loss = 0.02241191
Iteration 760, loss = 0.02230925
Iteration 761, loss = 0.02221882
Iteration 762, loss = 0.02213959
Iteration 763, loss = 0.02214749
Iteration 764, loss = 0.02210482
Iteration 765, loss = 0.02203861
Iteration 766, loss = 0.02193055
Iteration 767, loss = 0.02188690
Iteration 768, loss = 0.02179354
Iteration 769, loss = 0.02177876
Iteration 770, loss = 0.02171478
Iteration 771, loss = 0.02164430
Iteration 772, loss = 0.02159038
Iteration 773, loss = 0.02151436
Iteration 774, loss = 0.02153496
Iteration 775, loss = 0.02145444
Iteration 776, loss = 0.02151479
Iteration 777, loss = 0.02154848
Iteration 778, loss = 0.02140009
Iteration 779, loss = 0.02129767
Iteration 780, loss = 0.02118449
Iteration 781, loss = 0.02110602
Iteration 782, loss = 0.02109153
Iteration 783, loss = 0.02107796
Iteration 784, loss = 0.02102239
Iteration 785, loss = 0.02093563
Iteration 786, loss = 0.02085609
Iteration 787, loss = 0.02081303
Iteration 788, loss = 0.02078491
Iteration 789, loss = 0.02071218
Iteration 790, loss = 0.02068742
Iteration 791, loss = 0.02069631
Iteration 792, loss = 0.02068666
Iteration 793, loss = 0.02049455
Iteration 794, loss = 0.02044222
Iteration 795, loss = 0.02055577
Iteration 796, loss = 0.02090209
Iteration 797, loss = 0.02124612
Iteration 798, loss = 0.02127320
Iteration 799, loss = 0.02100565
Iteration 800, loss = 0.02068415
Iteration 801, loss = 0.02033596
Iteration 802, loss = 0.02021252
Iteration 803, loss = 0.02005729
Iteration 804, loss = 0.02004692
Iteration 805, loss = 0.02008728
Iteration 806, loss = 0.01999221
Iteration 807, loss = 0.01993141
Iteration 808, loss = 0.01987409
Iteration 809, loss = 0.01982180
Iteration 810, loss = 0.01980265
Iteration 811, loss = 0.01978001
Iteration 812, loss = 0.01979062
Iteration 813, loss = 0.01969886
Iteration 814, loss = 0.01965302
Iteration 815, loss = 0.01966475
Iteration 816, loss = 0.01966195
Iteration 817, loss = 0.01963564
Iteration 818, loss = 0.01962070
Iteration 819, loss = 0.01957758
Iteration 820, loss = 0.01961073
Iteration 821, loss = 0.01935786
Iteration 822, loss = 0.01931314
Iteration 823, loss = 0.01941905
Iteration 824, loss = 0.01932447
Iteration 825, loss = 0.01925623
Iteration 826, loss = 0.01915294
Iteration 827, loss = 0.01906773
Iteration 828, loss = 0.01896178
Iteration 829, loss = 0.01896583
Iteration 830, loss = 0.01898140
Iteration 831, loss = 0.01894023
Iteration 832, loss = 0.01880885
Iteration 833, loss = 0.01889944
Iteration 834, loss = 0.01876659
Iteration 835, loss = 0.01870784
Iteration 836, loss = 0.01878764
Iteration 837, loss = 0.01861768
Iteration 838, loss = 0.01860597
Iteration 839, loss = 0.01854329
Iteration 840, loss = 0.01851689
Iteration 841, loss = 0.01850015
Iteration 842, loss = 0.01845079
Iteration 843, loss = 0.01840293
Iteration 844, loss = 0.01835511
Iteration 845, loss = 0.01829677
Iteration 846, loss = 0.01827631
Iteration 847, loss = 0.01828607
Iteration 848, loss = 0.01823586
Iteration 849, loss = 0.01817096
Iteration 850, loss = 0.01813517
Iteration 851, loss = 0.01809118
Iteration 852, loss = 0.01804963
Iteration 853, loss = 0.01805147
Iteration 854, loss = 0.01806426
Iteration 855, loss = 0.01801869
Iteration 856, loss = 0.01794419
Iteration 857, loss = 0.01784491
Iteration 858, loss = 0.01784905
Iteration 859, loss = 0.01774972
Iteration 860, loss = 0.01770265
Iteration 861, loss = 0.01766614
Iteration 862, loss = 0.01761520
Iteration 863, loss = 0.01758861
Iteration 864, loss = 0.01759817
Iteration 865, loss = 0.01755589
Iteration 866, loss = 0.01752120
Iteration 867, loss = 0.01757813
Iteration 868, loss = 0.01765225
Iteration 869, loss = 0.01773807
Iteration 870, loss = 0.01777422
Iteration 871, loss = 0.01769852
Iteration 872, loss = 0.01757009
Iteration 873, loss = 0.01744137
Iteration 874, loss = 0.01732069
Iteration 875, loss = 0.01723941
Iteration 876, loss = 0.01711858
Iteration 877, loss = 0.01707534
Iteration 878, loss = 0.01706585
Iteration 879, loss = 0.01701302
Iteration 880, loss = 0.01699514
Iteration 881, loss = 0.01701499
Iteration 882, loss = 0.01698441
Iteration 883, loss = 0.01694221
Iteration 884, loss = 0.01685564
Iteration 885, loss = 0.01682109
Iteration 886, loss = 0.01680010
Iteration 887, loss = 0.01677017
Iteration 888, loss = 0.01678552
Iteration 889, loss = 0.01671466
Iteration 890, loss = 0.01666968
Iteration 891, loss = 0.01662204
Iteration 892, loss = 0.01658130
Iteration 893, loss = 0.01654221
Iteration 894, loss = 0.01650677
Iteration 895, loss = 0.01647977
Iteration 896, loss = 0.01646662
Iteration 897, loss = 0.01646272
Iteration 898, loss = 0.01646092
Iteration 899, loss = 0.01646231
Iteration 900, loss = 0.01645089
Iteration 901, loss = 0.01642522
Iteration 902, loss = 0.01639877
Iteration 903, loss = 0.01627734
Iteration 904, loss = 0.01617618
Iteration 905, loss = 0.01613846
Iteration 906, loss = 0.01612413
Iteration 907, loss = 0.01614304
Iteration 908, loss = 0.01609830
Iteration 909, loss = 0.01606435
Iteration 910, loss = 0.01603018
Iteration 911, loss = 0.01597989
Iteration 912, loss = 0.01594017
Iteration 913, loss = 0.01585656
Iteration 914, loss = 0.01589406
Iteration 915, loss = 0.01584593
Iteration 916, loss = 0.01585583
Iteration 917, loss = 0.01585106
Iteration 918, loss = 0.01585591
Iteration 919, loss = 0.01587906
Iteration 920, loss = 0.01577075
Iteration 921, loss = 0.01565947
Iteration 922, loss = 0.01563560
Iteration 923, loss = 0.01555439
Iteration 924, loss = 0.01562427
Iteration 925, loss = 0.01561104
Iteration 926, loss = 0.01557275
Iteration 927, loss = 0.01551849
Iteration 928, loss = 0.01548541
Iteration 929, loss = 0.01544394
Iteration 930, loss = 0.01541240
Iteration 931, loss = 0.01539234
Iteration 932, loss = 0.01535851
Iteration 933, loss = 0.01528626
Iteration 934, loss = 0.01527499
Iteration 935, loss = 0.01521556
Iteration 936, loss = 0.01520305
Iteration 937, loss = 0.01525002
Iteration 938, loss = 0.01521765
Iteration 939, loss = 0.01517395
Iteration 940, loss = 0.01511534
Iteration 941, loss = 0.01503933
Iteration 942, loss = 0.01499810
Iteration 943, loss = 0.01496579
Iteration 944, loss = 0.01493433
Iteration 945, loss = 0.01490492
Iteration 946, loss = 0.01488075
Iteration 947, loss = 0.01487334
Iteration 948, loss = 0.01490296
Iteration 949, loss = 0.01492947
Iteration 950, loss = 0.01487613
Iteration 951, loss = 0.01483979
Iteration 952, loss = 0.01477572
Iteration 953, loss = 0.01473451
Iteration 954, loss = 0.01469157
Iteration 955, loss = 0.01467276
Iteration 956, loss = 0.01465766
Iteration 957, loss = 0.01465060
Iteration 958, loss = 0.01467509
Iteration 959, loss = 0.01468964
Iteration 960, loss = 0.01470826
Iteration 961, loss = 0.01468973
Iteration 962, loss = 0.01467008
Iteration 963, loss = 0.01456072
Iteration 964, loss = 0.01444956
Iteration 965, loss = 0.01439404
Iteration 966, loss = 0.01435310
Iteration 967, loss = 0.01437425
Iteration 968, loss = 0.01439507
Iteration 969, loss = 0.01447478
Iteration 970, loss = 0.01444626
Iteration 971, loss = 0.01442791
Iteration 972, loss = 0.01439851
Iteration 973, loss = 0.01433917
Iteration 974, loss = 0.01428088
Iteration 975, loss = 0.01421170
Iteration 976, loss = 0.01411167
Iteration 977, loss = 0.01409382
Iteration 978, loss = 0.01407584
Iteration 979, loss = 0.01411452
Iteration 980, loss = 0.01407088
Iteration 981, loss = 0.01403655
Iteration 982, loss = 0.01400661
Iteration 983, loss = 0.01397469
Iteration 984, loss = 0.01395887
Iteration 985, loss = 0.01392880
Iteration 986, loss = 0.01392010
Iteration 987, loss = 0.01394310
Iteration 988, loss = 0.01392510
Iteration 989, loss = 0.01391167
Iteration 990, loss = 0.01387854
Iteration 991, loss = 0.01384124
Iteration 992, loss = 0.01375015
Iteration 993, loss = 0.01369078
Iteration 994, loss = 0.01369900
Iteration 995, loss = 0.01377540
Iteration 996, loss = 0.01367851
Iteration 997, loss = 0.01361401
Iteration 998, loss = 0.01359022
Iteration 999, loss = 0.01356822
Iteration 1000, loss = 0.01358387
PARAMETROS DE INICIALIZACAO DA REDE
NUMERO DE NEURONIOS 
Camada de Entrada: 34
Camada Escondida: 10
Camada de Saida: 1

--- PARAMETROS DE CONFIGURACAO DA REDE ---
Numero de Epocas: 1000
Taxa de Aprendizado: adaptive
Taxa de Aprendizado Inicial: 0.1
METRICAS

RESULTADOS:

[0 0 1 0 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1 1 1 1 1 1 1
 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 1 1 1
 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 1 0]
ACURACIA: 0.9433962264150944

