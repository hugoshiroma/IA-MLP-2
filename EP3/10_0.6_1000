Pesos Camada de Entrada: 
[[-4.70557963e-03 -1.80521899e-01  1.05381054e-01 -2.13147949e-01
  -1.47901517e-01 -6.04767512e-02  2.41446859e-03  8.69637226e-02
  -9.09521188e-02 -4.67959929e-02]
 [ 8.28824433e-02 -1.50119327e-01 -7.44740252e-02 -9.04443762e-02
  -1.18463000e-02 -3.73844754e-02  4.63607004e-02 -2.06667454e-01
  -1.68662773e-01 -2.30292716e-02]
 [ 1.54733520e-01  1.90738526e-01  1.16102710e-01  1.94112678e-01
   4.68029029e-02  2.09232852e-01 -1.41351360e-02  1.78839148e-01
  -6.16655376e-03 -1.74561176e-02]
 [-2.11129445e-01 -1.50123328e-01 -2.05221474e-01 -1.45303491e-01
  -3.54783152e-03  1.11628918e-01 -9.53274268e-02 -2.08071312e-01
   7.84174682e-02  1.01434952e-01]
 [ 1.07832953e-01  1.39795172e-01  5.29034185e-02  1.88474414e-01
  -9.03078053e-02  1.20388396e-01 -7.98042190e-02 -7.18963533e-02
   1.99194878e-01  3.70737645e-02]
 [-9.60198957e-02  1.52776210e-01 -1.92714785e-01 -1.38010758e-01
  -1.13403793e-01  2.56523836e-02  1.35046758e-01  3.24340896e-02
  -1.20736814e-01  1.51921072e-01]
 [-6.50363552e-02  1.16169514e-02 -1.74965137e-01 -1.46793483e-01
  -7.98933790e-02  8.48183513e-02 -1.59123709e-01  1.99052247e-01
  -1.44153180e-01 -2.53705352e-04]
 [-9.57674108e-02  9.48307681e-02 -6.22652836e-02  7.61723987e-02
  -1.62443227e-02 -1.10140387e-01  1.66099100e-01 -1.06796908e-01
  -5.86115239e-02  1.59594862e-01]
 [ 9.04426278e-02  1.38075499e-02  1.41476170e-02  8.56972801e-02
   1.00820192e-01  9.26505854e-02  1.36320792e-01 -6.18565500e-02
   1.04462078e-01 -1.59783149e-01]
 [ 1.20405674e-01  1.28072402e-01  1.56038952e-01  1.82236708e-01
   1.51772294e-01 -1.76126579e-01 -1.71952407e-01  4.30744284e-02
  -1.10893493e-01 -9.19025195e-02]
 [ 1.77175280e-01  1.24748886e-01  1.45460799e-01 -9.28763032e-02
   1.56376519e-01 -1.13528202e-01  1.40618430e-01 -2.01122979e-01
  -3.85347582e-02 -7.89275803e-02]
 [ 6.65782430e-02 -1.21916931e-01 -1.95442769e-01  2.65947663e-02
  -2.85331068e-02 -2.32170520e-02  5.99552690e-02 -3.23729897e-02
  -7.11095996e-02 -3.12945550e-02]
 [ 5.09597325e-02  1.41991186e-01 -1.94893197e-01  1.26267306e-01
   7.75929731e-02  1.94141430e-01  5.03425980e-02  5.24801712e-03
  -2.05284617e-01 -1.45299986e-01]
 [-1.73909590e-02  1.85482650e-01 -2.60175320e-02  1.50067321e-01
   4.55541957e-03  1.74642191e-01 -9.97615331e-02  1.71564639e-01
   1.11853888e-01  1.24869302e-01]
 [ 1.75435643e-01  1.02267970e-01 -1.77764690e-01  1.26848119e-01
  -1.83142139e-01  1.93586641e-01 -9.02497181e-02 -6.32520945e-02
   4.71317046e-02  1.87862055e-01]
 [ 1.71703741e-01  4.78520512e-03 -5.65718063e-02 -1.12988214e-01
   8.99819499e-02  6.83104975e-02  1.90562007e-02  2.35249297e-02
   1.48690360e-01 -1.72244679e-01]
 [-1.33410338e-01 -2.35883410e-02  6.33180257e-02 -1.75559926e-01
   7.74161834e-02 -2.04741572e-01  1.98414797e-01  1.61245410e-01
   1.27988313e-01  8.05977165e-03]
 [-5.61683339e-02  8.55482312e-02  2.98334385e-03 -1.36811197e-02
  -6.73035796e-02 -1.18221212e-01 -7.60899317e-02  1.79248029e-01
  -7.62717232e-02  1.98515631e-01]
 [-1.53398433e-01  7.18640976e-02  1.75627011e-01  5.85901410e-02
  -1.71764835e-01  2.08367465e-01 -1.45845059e-01  1.10884853e-01
   1.16868840e-01  1.63921784e-01]
 [-7.69411855e-02 -1.66260116e-01 -3.20240865e-02 -1.67032551e-01
   1.66103569e-01  8.20389974e-02 -8.16108733e-02 -1.54471053e-01
   1.35862735e-01  1.09358909e-01]
 [-4.31641113e-02 -2.54753490e-05 -1.54536680e-01 -2.43046095e-02
   3.33119658e-02  1.62812728e-02  1.10154204e-01  1.26311853e-01
   6.53912175e-02  1.85239562e-01]
 [-1.20817127e-01 -4.77084334e-03 -1.38040354e-01  7.12109987e-02
  -4.90010146e-02  9.02745002e-02  1.90326022e-02  1.62246283e-01
  -2.57532976e-02 -1.94272470e-02]
 [ 1.02608678e-01  1.21837204e-01  1.87377217e-01 -1.32987941e-01
  -1.78131994e-01  1.97374339e-01 -1.49382105e-01  1.41403578e-01
   7.96496912e-02  5.54324592e-02]
 [-1.10816372e-02  1.33975225e-01  5.16120923e-02 -1.47363703e-01
  -1.13056081e-01  1.70437000e-02  1.19918422e-02  2.09324323e-01
   9.39403158e-02  1.97945362e-01]
 [ 1.03159652e-01 -1.97164618e-01  1.74418850e-01 -2.12701722e-01
  -8.10882735e-02 -2.03310014e-01  3.87439571e-02 -2.37330865e-02
   1.73816851e-01 -1.06938036e-01]
 [ 2.28771652e-03 -1.57096228e-01  1.00072337e-01 -1.84373680e-03
  -1.72975112e-01 -1.54142667e-01 -6.38558625e-02  1.29812615e-01
   1.24535909e-01  1.17478932e-01]
 [-7.15978367e-02 -1.98956700e-01  4.75798145e-02 -1.26407059e-02
  -1.76499001e-01 -9.48209848e-02 -9.50896588e-02  8.67126640e-02
   1.21428608e-01 -1.90191211e-01]
 [ 1.39646297e-01  1.53886355e-01 -1.79222796e-01 -1.47188102e-01
  -1.87734944e-01  5.77014617e-02 -1.98340219e-01 -7.37334676e-02
   6.39104200e-02  4.03147678e-02]
 [-7.59297320e-02 -1.44088528e-01 -2.06295210e-01 -6.17850256e-02
   1.31770294e-01 -1.88398052e-01 -1.37154882e-02  1.51448661e-01
   1.87505970e-01  3.41839537e-02]
 [ 1.74970726e-01 -5.96940872e-02  8.91463581e-02  8.75559732e-04
   4.45055775e-02  1.88397497e-01  1.90360469e-01 -1.96007059e-01
   1.08612909e-01 -8.16362530e-02]
 [-1.46938341e-01 -8.65129554e-02 -2.04090028e-01  1.34839561e-01
  -2.10046197e-01 -1.18316519e-01 -1.65522108e-02  1.04402737e-01
  -3.03588010e-02  8.98985990e-02]
 [-1.72443231e-01  7.49389411e-02  1.71757424e-02 -1.86371766e-01
   1.50392147e-01  1.60744956e-01  1.24468656e-01  1.12306336e-01
  -2.10001976e-01 -1.47961516e-01]
 [-1.44112426e-01  4.45293179e-02 -1.66472165e-01 -1.07441138e-02
   1.45435889e-01  1.45257459e-01 -1.61064220e-02  1.48813316e-01
   4.64542162e-02 -1.30306539e-01]
 [-4.64800706e-03 -5.46439924e-02 -1.66466577e-01 -1.11602114e-01
  -1.16184245e-01  5.61838860e-02 -9.64973376e-02 -1.40703171e-01
  -7.44847773e-02 -1.74164633e-01]]
Bias Camada de Entrada: 
[ 0.10293003  0.16108378  0.09615602 -0.20233189 -0.14571419  0.01344985
 -0.02639814 -0.02314149 -0.19285002 -0.10490029]
Pesos Camada Escondida: 
[[ 0.37103562]
 [-0.03155504]
 [ 0.08102314]
 [-0.14903963]
 [ 0.16004031]
 [ 0.05376376]
 [-0.36387166]
 [-0.13526731]
 [ 0.38160912]
 [-0.29453137]]
Bias Camada Escondida: 
[-0.4228486]
Iteration 1, loss = 0.73041644
Iteration 2, loss = 0.65984716
Iteration 3, loss = 0.61316723
Iteration 4, loss = 0.59025679
Iteration 5, loss = 0.55882721
Iteration 6, loss = 0.52637715
Iteration 7, loss = 0.48995504
Iteration 8, loss = 0.44517541
Iteration 9, loss = 0.41018096
Iteration 10, loss = 0.38281988
Iteration 11, loss = 0.36593515
Iteration 12, loss = 0.34227571
Iteration 13, loss = 0.32514803
Iteration 14, loss = 0.31231215
Iteration 15, loss = 0.30422419
Iteration 16, loss = 0.29466398
Iteration 17, loss = 0.28479192
Iteration 18, loss = 0.27577716
Iteration 19, loss = 0.26738051
Iteration 20, loss = 0.25970327
Iteration 21, loss = 0.25129659
Iteration 22, loss = 0.23894185
Iteration 23, loss = 0.22965224
Iteration 24, loss = 0.22006785
Iteration 25, loss = 0.21449284
Iteration 26, loss = 0.20387389
Iteration 27, loss = 0.19554779
Iteration 28, loss = 0.18965207
Iteration 29, loss = 0.18131782
Iteration 30, loss = 0.17589443
Iteration 31, loss = 0.16908163
Iteration 32, loss = 0.17756858
Iteration 33, loss = 0.17954478
Iteration 34, loss = 0.16436151
Iteration 35, loss = 0.14486848
Iteration 36, loss = 0.14051268
Iteration 37, loss = 0.13683880
Iteration 38, loss = 0.13400257
Iteration 39, loss = 0.12914344
Iteration 40, loss = 0.12225798
Iteration 41, loss = 0.12497336
Iteration 42, loss = 0.11823376
Iteration 43, loss = 0.11247440
Iteration 44, loss = 0.10808067
Iteration 45, loss = 0.10606883
Iteration 46, loss = 0.10679530
Iteration 47, loss = 0.10171002
Iteration 48, loss = 0.09840980
Iteration 49, loss = 0.09673523
Iteration 50, loss = 0.09487873
Iteration 51, loss = 0.09211463
Iteration 52, loss = 0.08893443
Iteration 53, loss = 0.08905852
Iteration 54, loss = 0.08763015
Iteration 55, loss = 0.08457360
Iteration 56, loss = 0.07906011
Iteration 57, loss = 0.08042388
Iteration 58, loss = 0.07731557
Iteration 59, loss = 0.07515226
Iteration 60, loss = 0.07397076
Iteration 61, loss = 0.07410471
Iteration 62, loss = 0.07172665
Iteration 63, loss = 0.07257908
Iteration 64, loss = 0.06919068
Iteration 65, loss = 0.06495351
Iteration 66, loss = 0.06390478
Iteration 67, loss = 0.06385085
Iteration 68, loss = 0.06080381
Iteration 69, loss = 0.06203940
Iteration 70, loss = 0.05874875
Iteration 71, loss = 0.05959777
Iteration 72, loss = 0.05677247
Iteration 73, loss = 0.05565116
Iteration 74, loss = 0.05478675
Iteration 75, loss = 0.05563965
Iteration 76, loss = 0.05248552
Iteration 77, loss = 0.05299357
Iteration 78, loss = 0.05075816
Iteration 79, loss = 0.05007217
Iteration 80, loss = 0.05063119
Iteration 81, loss = 0.04795082
Iteration 82, loss = 0.04950422
Iteration 83, loss = 0.04720677
Iteration 84, loss = 0.04548581
Iteration 85, loss = 0.04454988
Iteration 86, loss = 0.04340351
Iteration 87, loss = 0.04227703
Iteration 88, loss = 0.04182046
Iteration 89, loss = 0.04197141
Iteration 90, loss = 0.04262890
Iteration 91, loss = 0.03938777
Iteration 92, loss = 0.04157026
Iteration 93, loss = 0.03855044
Iteration 94, loss = 0.04005878
Iteration 95, loss = 0.03984047
Iteration 96, loss = 0.03850511
Iteration 97, loss = 0.03835192
Iteration 98, loss = 0.04091437
Iteration 99, loss = 0.03446158
Iteration 100, loss = 0.03597842
Iteration 101, loss = 0.03638062
Iteration 102, loss = 0.03343973
Iteration 103, loss = 0.03273562
Iteration 104, loss = 0.03165689
Iteration 105, loss = 0.03128371
Iteration 106, loss = 0.03057778
Iteration 107, loss = 0.03003782
Iteration 108, loss = 0.02962520
Iteration 109, loss = 0.02947692
Iteration 110, loss = 0.02771198
Iteration 111, loss = 0.02723272
Iteration 112, loss = 0.02701936
Iteration 113, loss = 0.02686772
Iteration 114, loss = 0.02654272
Iteration 115, loss = 0.02614234
Iteration 116, loss = 0.02502636
Iteration 117, loss = 0.02426570
Iteration 118, loss = 0.02453777
Iteration 119, loss = 0.02498564
Iteration 120, loss = 0.02392001
Iteration 121, loss = 0.02302630
Iteration 122, loss = 0.02379491
Iteration 123, loss = 0.02330339
Iteration 124, loss = 0.02217248
Iteration 125, loss = 0.02196113
Iteration 126, loss = 0.02177181
Iteration 127, loss = 0.02149131
Iteration 128, loss = 0.02052363
Iteration 129, loss = 0.02017919
Iteration 130, loss = 0.01991506
Iteration 131, loss = 0.01974117
Iteration 132, loss = 0.01950118
Iteration 133, loss = 0.01913391
Iteration 134, loss = 0.01902261
Iteration 135, loss = 0.01851993
Iteration 136, loss = 0.01844655
Iteration 137, loss = 0.01785150
Iteration 138, loss = 0.01811487
Iteration 139, loss = 0.01768403
Iteration 140, loss = 0.01743159
Iteration 141, loss = 0.01721862
Iteration 142, loss = 0.01691637
Iteration 143, loss = 0.01665622
Iteration 144, loss = 0.01663798
Iteration 145, loss = 0.01647008
Iteration 146, loss = 0.01614221
Iteration 147, loss = 0.01587184
Iteration 148, loss = 0.01614182
Iteration 149, loss = 0.01573157
Iteration 150, loss = 0.01535129
Iteration 151, loss = 0.01511464
Iteration 152, loss = 0.01482151
Iteration 153, loss = 0.01466919
Iteration 154, loss = 0.01468046
Iteration 155, loss = 0.01475890
Iteration 156, loss = 0.01479208
Iteration 157, loss = 0.01432906
Iteration 158, loss = 0.01402623
Iteration 159, loss = 0.01387331
Iteration 160, loss = 0.01370408
Iteration 161, loss = 0.01351242
Iteration 162, loss = 0.01351203
Iteration 163, loss = 0.01338906
Iteration 164, loss = 0.01325680
Iteration 165, loss = 0.01296646
Iteration 166, loss = 0.01274344
Iteration 167, loss = 0.01254164
Iteration 168, loss = 0.01242579
Iteration 169, loss = 0.01210399
Iteration 170, loss = 0.01189350
Iteration 171, loss = 0.01180781
Iteration 172, loss = 0.01183231
Iteration 173, loss = 0.01163407
Iteration 174, loss = 0.01140406
Iteration 175, loss = 0.01130713
Iteration 176, loss = 0.01127007
Iteration 177, loss = 0.01126774
Iteration 178, loss = 0.01121025
Iteration 179, loss = 0.01110169
Iteration 180, loss = 0.01092506
Iteration 181, loss = 0.01072509
Iteration 182, loss = 0.01078284
Iteration 183, loss = 0.01088096
Iteration 184, loss = 0.01095445
Iteration 185, loss = 0.01084464
Iteration 186, loss = 0.01033381
Iteration 187, loss = 0.01048985
Iteration 188, loss = 0.01019847
Iteration 189, loss = 0.01002709
Iteration 190, loss = 0.01034833
Iteration 191, loss = 0.00978727
Iteration 192, loss = 0.00992386
Iteration 193, loss = 0.01002447
Iteration 194, loss = 0.00982876
Iteration 195, loss = 0.00957709
Iteration 196, loss = 0.00939247
Iteration 197, loss = 0.00937197
Iteration 198, loss = 0.00913816
Iteration 199, loss = 0.00903846
Iteration 200, loss = 0.00898736
Iteration 201, loss = 0.00897615
Iteration 202, loss = 0.00888049
Iteration 203, loss = 0.00879358
Iteration 204, loss = 0.00873318
Iteration 205, loss = 0.00873458
Iteration 206, loss = 0.00874643
Iteration 207, loss = 0.00854780
Iteration 208, loss = 0.00842112
Iteration 209, loss = 0.00844422
Iteration 210, loss = 0.00869217
Iteration 211, loss = 0.00866336
Iteration 212, loss = 0.00845905
Iteration 213, loss = 0.00821389
Iteration 214, loss = 0.00809487
Iteration 215, loss = 0.00810670
Iteration 216, loss = 0.00791115
Iteration 217, loss = 0.00798576
Iteration 218, loss = 0.00788968
Iteration 219, loss = 0.00765139
Iteration 220, loss = 0.00772360
Iteration 221, loss = 0.00765118
Iteration 222, loss = 0.00748682
Iteration 223, loss = 0.00739229
Iteration 224, loss = 0.00738283
Iteration 225, loss = 0.00729625
Iteration 226, loss = 0.00722452
Iteration 227, loss = 0.00730317
Iteration 228, loss = 0.00728296
Iteration 229, loss = 0.00716251
Iteration 230, loss = 0.00708549
Iteration 231, loss = 0.00702748
Iteration 232, loss = 0.00698908
Iteration 233, loss = 0.00687669
Iteration 234, loss = 0.00681079
Iteration 235, loss = 0.00675966
Iteration 236, loss = 0.00678243
Iteration 237, loss = 0.00695161
Iteration 238, loss = 0.00691021
Iteration 239, loss = 0.00681007
Iteration 240, loss = 0.00668603
Iteration 241, loss = 0.00656444
Iteration 242, loss = 0.00648915
Iteration 243, loss = 0.00641881
Iteration 244, loss = 0.00635289
Iteration 245, loss = 0.00630174
Iteration 246, loss = 0.00624201
Iteration 247, loss = 0.00620433
Iteration 248, loss = 0.00619300
Iteration 249, loss = 0.00613234
Iteration 250, loss = 0.00613458
Iteration 251, loss = 0.00607325
Iteration 252, loss = 0.00609834
Iteration 253, loss = 0.00607775
Iteration 254, loss = 0.00601202
Iteration 255, loss = 0.00594082
Iteration 256, loss = 0.00589962
Iteration 257, loss = 0.00602526
Iteration 258, loss = 0.00609139
Iteration 259, loss = 0.00582650
Iteration 260, loss = 0.00570584
Iteration 261, loss = 0.00589243
Iteration 262, loss = 0.00588538
Iteration 263, loss = 0.00589080
Iteration 264, loss = 0.00573814
Iteration 265, loss = 0.00568312
Iteration 266, loss = 0.00564513
Iteration 267, loss = 0.00557712
Iteration 268, loss = 0.00565011
Iteration 269, loss = 0.00556813
Iteration 270, loss = 0.00561257
Iteration 271, loss = 0.00548346
Iteration 272, loss = 0.00529124
Iteration 273, loss = 0.00530266
Iteration 274, loss = 0.00544932
Iteration 275, loss = 0.00557623
Iteration 276, loss = 0.00539822
Iteration 277, loss = 0.00526143
Iteration 278, loss = 0.00517721
Iteration 279, loss = 0.00514142
Iteration 280, loss = 0.00511215
Iteration 281, loss = 0.00507154
Iteration 282, loss = 0.00512498
Iteration 283, loss = 0.00521815
Iteration 284, loss = 0.00525662
Iteration 285, loss = 0.00514819
Iteration 286, loss = 0.00497356
Iteration 287, loss = 0.00490818
Iteration 288, loss = 0.00491415
Iteration 289, loss = 0.00488684
Iteration 290, loss = 0.00484647
Iteration 291, loss = 0.00480156
Iteration 292, loss = 0.00477437
Iteration 293, loss = 0.00474139
Iteration 294, loss = 0.00471227
Iteration 295, loss = 0.00468174
Iteration 296, loss = 0.00465857
Iteration 297, loss = 0.00464596
Iteration 298, loss = 0.00461947
Iteration 299, loss = 0.00460043
Iteration 300, loss = 0.00457357
Iteration 301, loss = 0.00457753
Iteration 302, loss = 0.00451591
Iteration 303, loss = 0.00449526
Iteration 304, loss = 0.00447220
Iteration 305, loss = 0.00445255
Iteration 306, loss = 0.00442976
Iteration 307, loss = 0.00441624
Iteration 308, loss = 0.00440676
Iteration 309, loss = 0.00439999
Iteration 310, loss = 0.00439237
Iteration 311, loss = 0.00436887
Iteration 312, loss = 0.00435213
Iteration 313, loss = 0.00430133
Iteration 314, loss = 0.00428499
Iteration 315, loss = 0.00425033
Iteration 316, loss = 0.00423407
Iteration 317, loss = 0.00422937
Iteration 318, loss = 0.00418561
Iteration 319, loss = 0.00417436
Iteration 320, loss = 0.00416542
Iteration 321, loss = 0.00415179
Iteration 322, loss = 0.00412692
Iteration 323, loss = 0.00409791
Iteration 324, loss = 0.00410456
Iteration 325, loss = 0.00408029
Iteration 326, loss = 0.00406283
Iteration 327, loss = 0.00407140
Iteration 328, loss = 0.00409811
Iteration 329, loss = 0.00401123
Iteration 330, loss = 0.00396568
Iteration 331, loss = 0.00398486
Iteration 332, loss = 0.00394709
Iteration 333, loss = 0.00391813
Iteration 334, loss = 0.00391463
Iteration 335, loss = 0.00387622
Iteration 336, loss = 0.00384024
Iteration 337, loss = 0.00381675
Iteration 338, loss = 0.00381501
Iteration 339, loss = 0.00379890
Iteration 340, loss = 0.00380190
Iteration 341, loss = 0.00380140
Iteration 342, loss = 0.00379974
Iteration 343, loss = 0.00376322
Iteration 344, loss = 0.00374616
Iteration 345, loss = 0.00370716
Iteration 346, loss = 0.00369545
Iteration 347, loss = 0.00367912
Iteration 348, loss = 0.00365962
Iteration 349, loss = 0.00363914
Iteration 350, loss = 0.00363290
Iteration 351, loss = 0.00361344
Iteration 352, loss = 0.00359173
Iteration 353, loss = 0.00357210
Iteration 354, loss = 0.00355895
Iteration 355, loss = 0.00353550
Iteration 356, loss = 0.00351919
Iteration 357, loss = 0.00351015
Iteration 358, loss = 0.00349085
Iteration 359, loss = 0.00348871
Iteration 360, loss = 0.00346749
Iteration 361, loss = 0.00344239
Iteration 362, loss = 0.00342405
Iteration 363, loss = 0.00342106
Iteration 364, loss = 0.00342949
Iteration 365, loss = 0.00341677
Iteration 366, loss = 0.00339040
Iteration 367, loss = 0.00336746
Iteration 368, loss = 0.00336428
Iteration 369, loss = 0.00334817
Iteration 370, loss = 0.00333574
Iteration 371, loss = 0.00330960
Iteration 372, loss = 0.00330498
Iteration 373, loss = 0.00332224
Iteration 374, loss = 0.00328181
Iteration 375, loss = 0.00325877
Iteration 376, loss = 0.00326550
Iteration 377, loss = 0.00324156
Iteration 378, loss = 0.00323766
Iteration 379, loss = 0.00320927
Iteration 380, loss = 0.00318450
Iteration 381, loss = 0.00317521
Iteration 382, loss = 0.00318444
Iteration 383, loss = 0.00321450
Iteration 384, loss = 0.00323001
Iteration 385, loss = 0.00323336
Iteration 386, loss = 0.00322939
Iteration 387, loss = 0.00321774
Iteration 388, loss = 0.00320462
Iteration 389, loss = 0.00310859
Iteration 390, loss = 0.00307778
Iteration 391, loss = 0.00308458
Iteration 392, loss = 0.00309723
Iteration 393, loss = 0.00309533
Iteration 394, loss = 0.00307235
Iteration 395, loss = 0.00304681
Iteration 396, loss = 0.00303644
Iteration 397, loss = 0.00300639
Iteration 398, loss = 0.00297977
Iteration 399, loss = 0.00297772
Iteration 400, loss = 0.00303429
Iteration 401, loss = 0.00302606
Iteration 402, loss = 0.00302296
Iteration 403, loss = 0.00304268
Iteration 404, loss = 0.00292846
Iteration 405, loss = 0.00291640
Iteration 406, loss = 0.00291848
Iteration 407, loss = 0.00295501
Iteration 408, loss = 0.00294635
Iteration 409, loss = 0.00290339
Iteration 410, loss = 0.00286799
Iteration 411, loss = 0.00284845
Iteration 412, loss = 0.00284432
Iteration 413, loss = 0.00286199
Iteration 414, loss = 0.00283960
Iteration 415, loss = 0.00282940
Iteration 416, loss = 0.00281403
Iteration 417, loss = 0.00280150
Iteration 418, loss = 0.00279411
Iteration 419, loss = 0.00277342
Iteration 420, loss = 0.00276524
Iteration 421, loss = 0.00276354
Iteration 422, loss = 0.00274823
Iteration 423, loss = 0.00274705
Iteration 424, loss = 0.00273219
Iteration 425, loss = 0.00272342
Iteration 426, loss = 0.00270433
Iteration 427, loss = 0.00269620
Iteration 428, loss = 0.00269490
Iteration 429, loss = 0.00268778
Iteration 430, loss = 0.00267918
Iteration 431, loss = 0.00267235
Iteration 432, loss = 0.00265887
Iteration 433, loss = 0.00265227
Iteration 434, loss = 0.00264439
Iteration 435, loss = 0.00263469
Iteration 436, loss = 0.00263329
Iteration 437, loss = 0.00261838
Iteration 438, loss = 0.00261908
Iteration 439, loss = 0.00260776
Iteration 440, loss = 0.00259740
Iteration 441, loss = 0.00258637
Iteration 442, loss = 0.00257186
Iteration 443, loss = 0.00255782
Iteration 444, loss = 0.00255750
Iteration 445, loss = 0.00259032
Iteration 446, loss = 0.00260412
Iteration 447, loss = 0.00258849
Iteration 448, loss = 0.00255154
Iteration 449, loss = 0.00253019
Iteration 450, loss = 0.00250642
Iteration 451, loss = 0.00250326
Iteration 452, loss = 0.00250883
Iteration 453, loss = 0.00251963
Iteration 454, loss = 0.00249376
Iteration 455, loss = 0.00247579
Iteration 456, loss = 0.00246683
Iteration 457, loss = 0.00245601
Iteration 458, loss = 0.00244694
Iteration 459, loss = 0.00244266
Iteration 460, loss = 0.00243382
Iteration 461, loss = 0.00243182
Iteration 462, loss = 0.00242488
Iteration 463, loss = 0.00242256
Iteration 464, loss = 0.00241915
Iteration 465, loss = 0.00241976
Iteration 466, loss = 0.00241682
Iteration 467, loss = 0.00241429
Iteration 468, loss = 0.00240918
Iteration 469, loss = 0.00239863
Iteration 470, loss = 0.00239370
Iteration 471, loss = 0.00238290
Iteration 472, loss = 0.00237526
Iteration 473, loss = 0.00236795
Iteration 474, loss = 0.00236025
Iteration 475, loss = 0.00235278
Iteration 476, loss = 0.00235277
Iteration 477, loss = 0.00235635
Iteration 478, loss = 0.00235933
Iteration 479, loss = 0.00236076
Iteration 480, loss = 0.00235333
Iteration 481, loss = 0.00231312
Iteration 482, loss = 0.00228783
Iteration 483, loss = 0.00228141
Iteration 484, loss = 0.00228992
Iteration 485, loss = 0.00229152
Iteration 486, loss = 0.00229885
Iteration 487, loss = 0.00228468
Iteration 488, loss = 0.00227068
Iteration 489, loss = 0.00226775
Iteration 490, loss = 0.00225168
Iteration 491, loss = 0.00224043
Iteration 492, loss = 0.00223095
Iteration 493, loss = 0.00221888
Iteration 494, loss = 0.00221032
Iteration 495, loss = 0.00221115
Iteration 496, loss = 0.00220046
Iteration 497, loss = 0.00220117
Iteration 498, loss = 0.00219403
Iteration 499, loss = 0.00218965
Iteration 500, loss = 0.00219026
Iteration 501, loss = 0.00220367
Iteration 502, loss = 0.00218868
Iteration 503, loss = 0.00217371
Iteration 504, loss = 0.00215640
Iteration 505, loss = 0.00215318
Iteration 506, loss = 0.00214699
Iteration 507, loss = 0.00214488
Iteration 508, loss = 0.00213563
Iteration 509, loss = 0.00214265
Iteration 510, loss = 0.00214802
Iteration 511, loss = 0.00214528
Iteration 512, loss = 0.00214601
Iteration 513, loss = 0.00212118
Iteration 514, loss = 0.00210691
Iteration 515, loss = 0.00209215
Iteration 516, loss = 0.00208321
Iteration 517, loss = 0.00207716
Iteration 518, loss = 0.00208055
Iteration 519, loss = 0.00207519
Iteration 520, loss = 0.00207482
Iteration 521, loss = 0.00207270
Iteration 522, loss = 0.00205554
Iteration 523, loss = 0.00205701
Iteration 524, loss = 0.00204476
Iteration 525, loss = 0.00203825
Iteration 526, loss = 0.00204504
Iteration 527, loss = 0.00203162
Iteration 528, loss = 0.00203264
Iteration 529, loss = 0.00202648
Iteration 530, loss = 0.00202453
Iteration 531, loss = 0.00201054
Iteration 532, loss = 0.00200329
Iteration 533, loss = 0.00200921
Iteration 534, loss = 0.00201836
Iteration 535, loss = 0.00201273
Iteration 536, loss = 0.00200415
Iteration 537, loss = 0.00199861
Iteration 538, loss = 0.00198974
Iteration 539, loss = 0.00197141
Iteration 540, loss = 0.00195591
Iteration 541, loss = 0.00194536
Iteration 542, loss = 0.00195066
Iteration 543, loss = 0.00194753
Iteration 544, loss = 0.00195046
Iteration 545, loss = 0.00194384
Iteration 546, loss = 0.00192276
Iteration 547, loss = 0.00191745
Iteration 548, loss = 0.00191748
Iteration 549, loss = 0.00192890
Iteration 550, loss = 0.00194824
Iteration 551, loss = 0.00196303
Iteration 552, loss = 0.00195689
Iteration 553, loss = 0.00194349
Iteration 554, loss = 0.00192876
Iteration 555, loss = 0.00191448
Iteration 556, loss = 0.00189179
Iteration 557, loss = 0.00188161
Iteration 558, loss = 0.00186630
Iteration 559, loss = 0.00186580
Iteration 560, loss = 0.00188287
Iteration 561, loss = 0.00189036
Iteration 562, loss = 0.00187299
Iteration 563, loss = 0.00185631
Iteration 564, loss = 0.00184353
Iteration 565, loss = 0.00183357
Iteration 566, loss = 0.00183763
Iteration 567, loss = 0.00184803
Iteration 568, loss = 0.00184060
Iteration 569, loss = 0.00183298
Iteration 570, loss = 0.00183403
Iteration 571, loss = 0.00182987
Iteration 572, loss = 0.00182165
Iteration 573, loss = 0.00181101
Iteration 574, loss = 0.00179898
Iteration 575, loss = 0.00179427
Iteration 576, loss = 0.00179004
Iteration 577, loss = 0.00180148
Iteration 578, loss = 0.00179833
Iteration 579, loss = 0.00179894
Iteration 580, loss = 0.00180254
Iteration 581, loss = 0.00180772
Iteration 582, loss = 0.00179809
Iteration 583, loss = 0.00178509
Iteration 584, loss = 0.00176884
Iteration 585, loss = 0.00176145
Iteration 586, loss = 0.00175366
Iteration 587, loss = 0.00175225
Iteration 588, loss = 0.00174719
Iteration 589, loss = 0.00174742
Iteration 590, loss = 0.00173831
Iteration 591, loss = 0.00173054
Iteration 592, loss = 0.00172464
Iteration 593, loss = 0.00172195
Iteration 594, loss = 0.00171493
Iteration 595, loss = 0.00170958
Iteration 596, loss = 0.00170850
Iteration 597, loss = 0.00170876
Iteration 598, loss = 0.00170491
Iteration 599, loss = 0.00170420
Iteration 600, loss = 0.00169473
Iteration 601, loss = 0.00168917
Iteration 602, loss = 0.00168437
Iteration 603, loss = 0.00168285
Iteration 604, loss = 0.00167965
Iteration 605, loss = 0.00168089
Iteration 606, loss = 0.00169152
Iteration 607, loss = 0.00169956
Iteration 608, loss = 0.00168626
Iteration 609, loss = 0.00168693
Iteration 610, loss = 0.00167349
Iteration 611, loss = 0.00167084
Iteration 612, loss = 0.00166343
Iteration 613, loss = 0.00166027
Iteration 614, loss = 0.00165610
Iteration 615, loss = 0.00165187
Iteration 616, loss = 0.00164777
Iteration 617, loss = 0.00164700
Iteration 618, loss = 0.00164234
Iteration 619, loss = 0.00163332
Iteration 620, loss = 0.00163020
Iteration 621, loss = 0.00163479
Iteration 622, loss = 0.00163413
Iteration 623, loss = 0.00162880
Iteration 624, loss = 0.00162455
Iteration 625, loss = 0.00162193
Iteration 626, loss = 0.00161693
Iteration 627, loss = 0.00161221
Iteration 628, loss = 0.00160598
Iteration 629, loss = 0.00159995
Iteration 630, loss = 0.00159269
Iteration 631, loss = 0.00158851
Iteration 632, loss = 0.00158675
Iteration 633, loss = 0.00158210
Iteration 634, loss = 0.00158279
Iteration 635, loss = 0.00158007
Iteration 636, loss = 0.00157432
Iteration 637, loss = 0.00156797
Iteration 638, loss = 0.00156262
Iteration 639, loss = 0.00155981
Iteration 640, loss = 0.00155992
Iteration 641, loss = 0.00155719
Iteration 642, loss = 0.00155570
Iteration 643, loss = 0.00155501
Iteration 644, loss = 0.00155481
Iteration 645, loss = 0.00155231
Iteration 646, loss = 0.00154935
Iteration 647, loss = 0.00154531
Iteration 648, loss = 0.00154293
Iteration 649, loss = 0.00154363
Iteration 650, loss = 0.00152478
Iteration 651, loss = 0.00152320
Iteration 652, loss = 0.00152691
Iteration 653, loss = 0.00154043
Iteration 654, loss = 0.00154943
Iteration 655, loss = 0.00155455
Iteration 656, loss = 0.00155023
Iteration 657, loss = 0.00153078
Iteration 658, loss = 0.00151211
Iteration 659, loss = 0.00149729
Iteration 660, loss = 0.00149529
Iteration 661, loss = 0.00149931
Iteration 662, loss = 0.00149407
Iteration 663, loss = 0.00149086
Iteration 664, loss = 0.00148444
Iteration 665, loss = 0.00147977
Iteration 666, loss = 0.00147705
Iteration 667, loss = 0.00147759
Iteration 668, loss = 0.00147592
Iteration 669, loss = 0.00147521
Iteration 670, loss = 0.00147187
Iteration 671, loss = 0.00146895
Iteration 672, loss = 0.00146353
Iteration 673, loss = 0.00146310
Iteration 674, loss = 0.00145397
Iteration 675, loss = 0.00144918
Iteration 676, loss = 0.00144698
Iteration 677, loss = 0.00144638
Iteration 678, loss = 0.00145313
Iteration 679, loss = 0.00144727
Iteration 680, loss = 0.00144026
Iteration 681, loss = 0.00143746
Iteration 682, loss = 0.00143428
Iteration 683, loss = 0.00143174
Iteration 684, loss = 0.00142682
Iteration 685, loss = 0.00142238
Iteration 686, loss = 0.00142047
Iteration 687, loss = 0.00141784
Iteration 688, loss = 0.00141502
Iteration 689, loss = 0.00141221
Iteration 690, loss = 0.00141217
Iteration 691, loss = 0.00140661
Iteration 692, loss = 0.00140586
Iteration 693, loss = 0.00140338
Iteration 694, loss = 0.00140113
Iteration 695, loss = 0.00139600
Iteration 696, loss = 0.00139639
Iteration 697, loss = 0.00139664
Iteration 698, loss = 0.00140296
Iteration 699, loss = 0.00140892
Iteration 700, loss = 0.00141000
Iteration 701, loss = 0.00140826
Iteration 702, loss = 0.00139858
Iteration 703, loss = 0.00138694
Iteration 704, loss = 0.00137454
Iteration 705, loss = 0.00136935
Iteration 706, loss = 0.00137413
Iteration 707, loss = 0.00137631
Iteration 708, loss = 0.00138724
Iteration 709, loss = 0.00137690
Iteration 710, loss = 0.00136957
Iteration 711, loss = 0.00135978
Iteration 712, loss = 0.00135673
Iteration 713, loss = 0.00134983
Iteration 714, loss = 0.00134768
Iteration 715, loss = 0.00134562
Iteration 716, loss = 0.00134305
Iteration 717, loss = 0.00134274
Iteration 718, loss = 0.00133954
Iteration 719, loss = 0.00133799
Iteration 720, loss = 0.00133590
Iteration 721, loss = 0.00133355
Iteration 722, loss = 0.00133224
Iteration 723, loss = 0.00133173
Iteration 724, loss = 0.00132859
Iteration 725, loss = 0.00132921
Iteration 726, loss = 0.00133273
Iteration 727, loss = 0.00133129
Iteration 728, loss = 0.00132876
Iteration 729, loss = 0.00132448
Iteration 730, loss = 0.00132420
Iteration 731, loss = 0.00132106
Iteration 732, loss = 0.00131832
Iteration 733, loss = 0.00131577
Iteration 734, loss = 0.00131208
Iteration 735, loss = 0.00131171
Iteration 736, loss = 0.00130479
Iteration 737, loss = 0.00130228
Iteration 738, loss = 0.00129809
Iteration 739, loss = 0.00129457
Iteration 740, loss = 0.00129182
Iteration 741, loss = 0.00128912
Iteration 742, loss = 0.00128750
Iteration 743, loss = 0.00128731
Iteration 744, loss = 0.00128057
Iteration 745, loss = 0.00127855
Iteration 746, loss = 0.00127828
Iteration 747, loss = 0.00127541
Iteration 748, loss = 0.00127140
Iteration 749, loss = 0.00127266
Iteration 750, loss = 0.00127206
Iteration 751, loss = 0.00127040
Iteration 752, loss = 0.00126859
Iteration 753, loss = 0.00126621
Iteration 754, loss = 0.00126240
Iteration 755, loss = 0.00125624
Iteration 756, loss = 0.00125079
Iteration 757, loss = 0.00125122
Iteration 758, loss = 0.00125099
Iteration 759, loss = 0.00126022
Iteration 760, loss = 0.00125964
Iteration 761, loss = 0.00125808
Iteration 762, loss = 0.00125745
Iteration 763, loss = 0.00125610
Iteration 764, loss = 0.00125252
Iteration 765, loss = 0.00124679
Iteration 766, loss = 0.00123964
Iteration 767, loss = 0.00123772
Iteration 768, loss = 0.00122918
Iteration 769, loss = 0.00122902
Iteration 770, loss = 0.00122442
Iteration 771, loss = 0.00122335
Iteration 772, loss = 0.00122222
Iteration 773, loss = 0.00122073
Iteration 774, loss = 0.00122016
Iteration 775, loss = 0.00121848
Iteration 776, loss = 0.00121634
Iteration 777, loss = 0.00121519
Iteration 778, loss = 0.00121266
Iteration 779, loss = 0.00121116
Iteration 780, loss = 0.00120944
Iteration 781, loss = 0.00120759
Iteration 782, loss = 0.00120645
Iteration 783, loss = 0.00120781
Iteration 784, loss = 0.00120748
Iteration 785, loss = 0.00120400
Iteration 786, loss = 0.00120233
Iteration 787, loss = 0.00120052
Iteration 788, loss = 0.00119185
Iteration 789, loss = 0.00118954
Iteration 790, loss = 0.00118827
Iteration 791, loss = 0.00118897
Iteration 792, loss = 0.00118563
Iteration 793, loss = 0.00118481
Iteration 794, loss = 0.00118351
Iteration 795, loss = 0.00117753
Iteration 796, loss = 0.00117715
Iteration 797, loss = 0.00117351
Iteration 798, loss = 0.00117042
Iteration 799, loss = 0.00116814
Iteration 800, loss = 0.00116597
Iteration 801, loss = 0.00116437
Iteration 802, loss = 0.00116365
Iteration 803, loss = 0.00116822
Iteration 804, loss = 0.00116615
Iteration 805, loss = 0.00115589
Iteration 806, loss = 0.00115337
Iteration 807, loss = 0.00115561
Iteration 808, loss = 0.00115633
Iteration 809, loss = 0.00115782
Iteration 810, loss = 0.00115859
Iteration 811, loss = 0.00115681
Iteration 812, loss = 0.00115475
Iteration 813, loss = 0.00115247
Iteration 814, loss = 0.00115364
Iteration 815, loss = 0.00114944
Iteration 816, loss = 0.00114353
Iteration 817, loss = 0.00114012
Iteration 818, loss = 0.00113657
Iteration 819, loss = 0.00113340
Iteration 820, loss = 0.00113332
Iteration 821, loss = 0.00113300
Iteration 822, loss = 0.00113436
Iteration 823, loss = 0.00113315
Iteration 824, loss = 0.00113421
Iteration 825, loss = 0.00112979
Iteration 826, loss = 0.00113778
Iteration 827, loss = 0.00113155
Iteration 828, loss = 0.00112905
Iteration 829, loss = 0.00112009
Iteration 830, loss = 0.00111649
Iteration 831, loss = 0.00111270
Iteration 832, loss = 0.00111067
Iteration 833, loss = 0.00110846
Iteration 834, loss = 0.00111030
Iteration 835, loss = 0.00110493
Iteration 836, loss = 0.00110356
Iteration 837, loss = 0.00110123
Iteration 838, loss = 0.00110000
Iteration 839, loss = 0.00109782
Iteration 840, loss = 0.00109765
Iteration 841, loss = 0.00109746
Iteration 842, loss = 0.00109190
Iteration 843, loss = 0.00109106
Iteration 844, loss = 0.00108971
Iteration 845, loss = 0.00109207
Iteration 846, loss = 0.00110124
Iteration 847, loss = 0.00111623
Iteration 848, loss = 0.00110716
Iteration 849, loss = 0.00110055
Iteration 850, loss = 0.00109338
Iteration 851, loss = 0.00108531
Iteration 852, loss = 0.00107906
Iteration 853, loss = 0.00107456
Iteration 854, loss = 0.00107390
Iteration 855, loss = 0.00107103
Iteration 856, loss = 0.00106978
Iteration 857, loss = 0.00106940
Iteration 858, loss = 0.00106755
Iteration 859, loss = 0.00106712
Iteration 860, loss = 0.00106877
Iteration 861, loss = 0.00106814
Iteration 862, loss = 0.00107201
Iteration 863, loss = 0.00106020
Iteration 864, loss = 0.00105549
Iteration 865, loss = 0.00105668
Iteration 866, loss = 0.00105238
Iteration 867, loss = 0.00105216
Iteration 868, loss = 0.00105180
Iteration 869, loss = 0.00105234
Iteration 870, loss = 0.00105043
Iteration 871, loss = 0.00104806
Iteration 872, loss = 0.00104581
Iteration 873, loss = 0.00104425
Iteration 874, loss = 0.00104214
Iteration 875, loss = 0.00103975
Iteration 876, loss = 0.00103793
Iteration 877, loss = 0.00103566
Iteration 878, loss = 0.00103278
Iteration 879, loss = 0.00103140
Iteration 880, loss = 0.00102949
Iteration 881, loss = 0.00102810
Iteration 882, loss = 0.00102665
Iteration 883, loss = 0.00102526
Iteration 884, loss = 0.00102445
Iteration 885, loss = 0.00102201
Iteration 886, loss = 0.00102014
Iteration 887, loss = 0.00101973
Iteration 888, loss = 0.00101773
Iteration 889, loss = 0.00101725
Iteration 890, loss = 0.00101581
Iteration 891, loss = 0.00101412
Iteration 892, loss = 0.00101309
Iteration 893, loss = 0.00101527
Iteration 894, loss = 0.00101660
Iteration 895, loss = 0.00101696
Iteration 896, loss = 0.00101767
Iteration 897, loss = 0.00101999
Iteration 898, loss = 0.00101694
Iteration 899, loss = 0.00101256
Iteration 900, loss = 0.00100344
Iteration 901, loss = 0.00100451
Iteration 902, loss = 0.00099864
Iteration 903, loss = 0.00099748
Iteration 904, loss = 0.00099847
Iteration 905, loss = 0.00099868
Iteration 906, loss = 0.00099686
Iteration 907, loss = 0.00099386
Iteration 908, loss = 0.00099446
Iteration 909, loss = 0.00098943
Iteration 910, loss = 0.00098803
Iteration 911, loss = 0.00098673
Iteration 912, loss = 0.00098594
Iteration 913, loss = 0.00098437
Iteration 914, loss = 0.00098495
Iteration 915, loss = 0.00098422
Iteration 916, loss = 0.00098259
Iteration 917, loss = 0.00097959
Iteration 918, loss = 0.00097865
Iteration 919, loss = 0.00097840
Iteration 920, loss = 0.00097766
Iteration 921, loss = 0.00097728
Iteration 922, loss = 0.00097593
Iteration 923, loss = 0.00097534
Iteration 924, loss = 0.00097590
Iteration 925, loss = 0.00097164
Iteration 926, loss = 0.00097088
Iteration 927, loss = 0.00096771
Iteration 928, loss = 0.00096589
Iteration 929, loss = 0.00096486
Iteration 930, loss = 0.00096290
Iteration 931, loss = 0.00096133
Iteration 932, loss = 0.00096038
Iteration 933, loss = 0.00095878
Iteration 934, loss = 0.00095758
Iteration 935, loss = 0.00095671
Iteration 936, loss = 0.00095391
Iteration 937, loss = 0.00095820
Iteration 938, loss = 0.00095369
Iteration 939, loss = 0.00094921
Iteration 940, loss = 0.00095027
Iteration 941, loss = 0.00095380
Iteration 942, loss = 0.00095020
Iteration 943, loss = 0.00095095
Iteration 944, loss = 0.00094794
Iteration 945, loss = 0.00094723
Iteration 946, loss = 0.00094557
Iteration 947, loss = 0.00094175
Iteration 948, loss = 0.00094067
Iteration 949, loss = 0.00093911
Iteration 950, loss = 0.00093852
Iteration 951, loss = 0.00093780
Iteration 952, loss = 0.00093795
Iteration 953, loss = 0.00093611
Iteration 954, loss = 0.00093629
Iteration 955, loss = 0.00093150
Iteration 956, loss = 0.00092873
Iteration 957, loss = 0.00092773
Iteration 958, loss = 0.00092786
Iteration 959, loss = 0.00092836
Iteration 960, loss = 0.00093104
Iteration 961, loss = 0.00092808
Iteration 962, loss = 0.00092656
Iteration 963, loss = 0.00092064
Iteration 964, loss = 0.00091753
Iteration 965, loss = 0.00091935
Iteration 966, loss = 0.00092474
Iteration 967, loss = 0.00092354
Iteration 968, loss = 0.00092004
Iteration 969, loss = 0.00091800
Iteration 970, loss = 0.00091692
Iteration 971, loss = 0.00091379
Iteration 972, loss = 0.00091268
Iteration 973, loss = 0.00091165
Iteration 974, loss = 0.00090871
Iteration 975, loss = 0.00090752
Iteration 976, loss = 0.00090746
Iteration 977, loss = 0.00090475
Iteration 978, loss = 0.00090313
Iteration 979, loss = 0.00090355
Iteration 980, loss = 0.00090518
Iteration 981, loss = 0.00090634
Iteration 982, loss = 0.00090604
Iteration 983, loss = 0.00090578
Iteration 984, loss = 0.00090521
Iteration 985, loss = 0.00090510
Iteration 986, loss = 0.00090330
Iteration 987, loss = 0.00090048
Iteration 988, loss = 0.00089764
Iteration 989, loss = 0.00089543
Iteration 990, loss = 0.00089229
Iteration 991, loss = 0.00089119
Iteration 992, loss = 0.00088983
Iteration 993, loss = 0.00089148
Iteration 994, loss = 0.00088910
Iteration 995, loss = 0.00088881
Iteration 996, loss = 0.00089152
Iteration 997, loss = 0.00088621
Iteration 998, loss = 0.00088315
Iteration 999, loss = 0.00087972
Iteration 1000, loss = 0.00087847
PARAMETROS DE INICIALIZACAO DA REDE
NUMERO DE NEURONIOS 
Camada de Entrada: 34
Camada Escondida: 10
Camada de Saida: 1

--- PARAMETROS DE CONFIGURACAO DA REDE ---
Numero de Epocas: 1000
Taxa de Aprendizado: adaptive
Taxa de Aprendizado Inicial: 0.6
METRICAS

RESULTADOS:

[0 0 1 0 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 1 1 1 1 1
 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 1 1 1
 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0]
ACURACIA: 0.9150943396226415

